I0307 17:06:46.808054 28053 caffe.cpp:204] Using GPUs 0
I0307 17:06:46.888238 28053 caffe.cpp:209] GPU 0: GeForce GTX 1080 Ti
I0307 17:06:47.212455 28053 solver.cpp:45] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.1
display: 100
max_iter: 30000
lr_policy: "multistep"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0001
snapshot: 10000
snapshot_prefix: "examples/cifar10/cifar10_res20"
solver_mode: GPU
device_id: 0
net: "examples/cifar10/cifar10_resnet20_train_test.prototxt"
train_state {
  level: 0
  stage: ""
}
stepvalue: 20000
stepvalue: 25000
I0307 17:06:47.212599 28053 solver.cpp:102] Creating training net from net file: examples/cifar10/cifar10_resnet20_train_test.prototxt
I0307 17:06:47.213225 28053 upgrade_proto.cpp:79] Attempting to upgrade batch norm layers using deprecated params: examples/cifar10/cifar10_resnet20_train_test.prototxt
I0307 17:06:47.213238 28053 upgrade_proto.cpp:82] Successfully upgraded batch norm layers using deprecated params.
I0307 17:06:47.213372 28053 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer cifar
I0307 17:06:47.213413 28053 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer Accuracy1
I0307 17:06:47.213903 28053 net.cpp:51] Initializing net from parameters: 
name: "cifar10_resnet20"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 32
    mean_file: "examples/cifar10/mean.binaryproto"
  }
  data_param {
    source: "examples/cifar10/cifar10_train_lmdb"
    batch_size: 128
    backend: LMDB
  }
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "data"
  top: "Convolution1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm1"
  type: "BatchNorm"
  bottom: "Convolution1"
  top: "Convolution1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale1"
  type: "Scale"
  bottom: "Convolution1"
  top: "Convolution1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU1"
  type: "ReLU"
  bottom: "Convolution1"
  top: "Convolution1"
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "Convolution1"
  top: "Convolution2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm2"
  type: "BatchNorm"
  bottom: "Convolution2"
  top: "Convolution2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale2"
  type: "Scale"
  bottom: "Convolution2"
  top: "Convolution2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU2"
  type: "ReLU"
  bottom: "Convolution2"
  top: "Convolution2"
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "Convolution2"
  top: "Convolution3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm3"
  type: "BatchNorm"
  bottom: "Convolution3"
  top: "Convolution3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale3"
  type: "Scale"
  bottom: "Convolution3"
  top: "Convolution3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise1"
  type: "Eltwise"
  bottom: "Convolution1"
  bottom: "Convolution3"
  top: "Eltwise1"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU3"
  type: "ReLU"
  bottom: "Eltwise1"
  top: "Eltwise1"
}
layer {
  name: "Convolution4"
  type: "Convolution"
  bottom: "Eltwise1"
  top: "Convolution4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm4"
  type: "BatchNorm"
  bottom: "Convolution4"
  top: "Convolution4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale4"
  type: "Scale"
  bottom: "Convolution4"
  top: "Convolution4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU4"
  type: "ReLU"
  bottom: "Convolution4"
  top: "Convolution4"
}
layer {
  name: "Convolution5"
  type: "Convolution"
  bottom: "Convolution4"
  top: "Convolution5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm5"
  type: "BatchNorm"
  bottom: "Convolution5"
  top: "Convolution5"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale5"
  type: "Scale"
  bottom: "Convolution5"
  top: "Convolution5"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise2"
  type: "Eltwise"
  bottom: "Eltwise1"
  bottom: "Convolution5"
  top: "Eltwise2"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU5"
  type: "ReLU"
  bottom: "Eltwise2"
  top: "Eltwise2"
}
layer {
  name: "Convolution6"
  type: "Convolution"
  bottom: "Eltwise2"
  top: "Convolution6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm6"
  type: "BatchNorm"
  bottom: "Convolution6"
  top: "Convolution6"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale6"
  type: "Scale"
  bottom: "Convolution6"
  top: "Convolution6"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU6"
  type: "ReLU"
  bottom: "Convolution6"
  top: "Convolution6"
}
layer {
  name: "Convolution7"
  type: "Convolution"
  bottom: "Convolution6"
  top: "Convolution7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm7"
  type: "BatchNorm"
  bottom: "Convolution7"
  top: "Convolution7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale7"
  type: "Scale"
  bottom: "Convolution7"
  top: "Convolution7"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise3"
  type: "Eltwise"
  bottom: "Eltwise2"
  bottom: "Convolution7"
  top: "Eltwise3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU7"
  type: "ReLU"
  bottom: "Eltwise3"
  top: "Eltwise3"
}
layer {
  name: "Convolution8"
  type: "Convolution"
  bottom: "Eltwise3"
  top: "Convolution8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm8"
  type: "BatchNorm"
  bottom: "Convolution8"
  top: "Convolution8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale8"
  type: "Scale"
  bottom: "Convolution8"
  top: "Convolution8"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Convolution9"
  type: "Convolution"
  bottom: "Eltwise3"
  top: "Convolution9"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm9"
  type: "BatchNorm"
  bottom: "Convolution9"
  top: "Convolution9"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale9"
  type: "Scale"
  bottom: "Convolution9"
  top: "Convolution9"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU8"
  type: "ReLU"
  bottom: "Convolution9"
  top: "Convolution9"
}
layer {
  name: "Convolution10"
  type: "Convolution"
  bottom: "Convolution9"
  top: "Convolution10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm10"
  type: "BatchNorm"
  bottom: "Convolution10"
  top: "Convolution10"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale10"
  type: "Scale"
  bottom: "Convolution10"
  top: "Convolution10"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise4"
  type: "Eltwise"
  bottom: "Convolution8"
  bottom: "Convolution10"
  top: "Eltwise4"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU9"
  type: "ReLU"
  bottom: "Eltwise4"
  top: "Eltwise4"
}
layer {
  name: "Convolution11"
  type: "Convolution"
  bottom: "Eltwise4"
  top: "Convolution11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm11"
  type: "BatchNorm"
  bottom: "Convolution11"
  top: "Convolution11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale11"
  type: "Scale"
  bottom: "Convolution11"
  top: "Convolution11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU10"
  type: "ReLU"
  bottom: "Convolution11"
  top: "Convolution11"
}
layer {
  name: "Convolution12"
  type: "Convolution"
  bottom: "Convolution11"
  top: "Convolution12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm12"
  type: "BatchNorm"
  bottom: "Convolution12"
  top: "Convolution12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale12"
  type: "Scale"
  bottom: "Convolution12"
  top: "Convolution12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise5"
  type: "Eltwise"
  bottom: "Eltwise4"
  bottom: "Convolution12"
  top: "Eltwise5"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU11"
  type: "ReLU"
  bottom: "Eltwise5"
  top: "Eltwise5"
}
layer {
  name: "Convolution13"
  type: "Convolution"
  bottom: "Eltwise5"
  top: "Convolution13"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm13"
  type: "BatchNorm"
  bottom: "Convolution13"
  top: "Convolution13"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale13"
  type: "Scale"
  bottom: "Convolution13"
  top: "Convolution13"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU12"
  type: "ReLU"
  bottom: "Convolution13"
  top: "Convolution13"
}
layer {
  name: "Convolution14"
  type: "Convolution"
  bottom: "Convolution13"
  top: "Convolution14"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm14"
  type: "BatchNorm"
  bottom: "Convolution14"
  top: "Convolution14"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale14"
  type: "Scale"
  bottom: "Convolution14"
  top: "Convolution14"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise6"
  type: "Eltwise"
  bottom: "Eltwise5"
  bottom: "Convolution14"
  top: "Eltwise6"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU13"
  type: "ReLU"
  bottom: "Eltwise6"
  top: "Eltwise6"
}
layer {
  name: "Convolution15"
  type: "Convolution"
  bottom: "Eltwise6"
  top: "Convolution15"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm15"
  type: "BatchNorm"
  bottom: "Convolution15"
  top: "Convolution15"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale15"
  type: "Scale"
  bottom: "Convolution15"
  top: "Convolution15"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Convolution16"
  type: "Convolution"
  bottom: "Eltwise6"
  top: "Convolution16"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm16"
  type: "BatchNorm"
  bottom: "Convolution16"
  top: "Convolution16"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale16"
  type: "Scale"
  bottom: "Convolution16"
  top: "Convolution16"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU14"
  type: "ReLU"
  bottom: "Convolution16"
  top: "Convolution16"
}
layer {
  name: "Convolution17"
  type: "Convolution"
  bottom: "Convolution16"
  top: "Convolution17"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm17"
  type: "BatchNorm"
  bottom: "Convolution17"
  top: "Convolution17"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale17"
  type: "Scale"
  bottom: "Convolution17"
  top: "Convolution17"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise7"
  type: "Eltwise"
  bottom: "Convolution15"
  bottom: "Convolution17"
  top: "Eltwise7"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU15"
  type: "ReLU"
  bottom: "Eltwise7"
  top: "Eltwise7"
}
layer {
  name: "Convolution18"
  type: "Convolution"
  bottom: "Eltwise7"
  top: "Convolution18"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm18"
  type: "BatchNorm"
  bottom: "Convolution18"
  top: "Convolution18"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale18"
  type: "Scale"
  bottom: "Convolution18"
  top: "Convolution18"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU16"
  type: "ReLU"
  bottom: "Convolution18"
  top: "Convolution18"
}
layer {
  name: "Convolution19"
  type: "Convolution"
  bottom: "Convolution18"
  top: "Convolution19"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm19"
  type: "BatchNorm"
  bottom: "Convolution19"
  top: "Convolution19"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale19"
  type: "Scale"
  bottom: "Convolution19"
  top: "Convolution19"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise8"
  type: "Eltwise"
  bottom: "Eltwise7"
  bottom: "Convolution19"
  top: "Eltwise8"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU17"
  type: "ReLU"
  bottom: "Eltwise8"
  top: "Eltwise8"
}
layer {
  name: "Convolution20"
  type: "Convolution"
  bottom: "Eltwise8"
  top: "Convolution20"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm20"
  type: "BatchNorm"
  bottom: "Convolution20"
  top: "Convolution20"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale20"
  type: "Scale"
  bottom: "Convolution20"
  top: "Convolution20"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU18"
  type: "ReLU"
  bottom: "Convolution20"
  top: "Convolution20"
}
layer {
  name: "Convolution21"
  type: "Convolution"
  bottom: "Convolution20"
  top: "Convolution21"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm21"
  type: "BatchNorm"
  bottom: "Convolution21"
  top: "Convolution21"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale21"
  type: "Scale"
  bottom: "Convolution21"
  top: "Convolution21"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise9"
  type: "Eltwise"
  bottom: "Eltwise8"
  bottom: "Convolution21"
  top: "Eltwise9"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU19"
  type: "ReLU"
  bottom: "Eltwise9"
  top: "Eltwise9"
}
layer {
  name: "Pooling1"
  type: "Pooling"
  bottom: "Eltwise9"
  top: "Pooling1"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "InnerProduct1"
  type: "InnerProduct"
  bottom: "Pooling1"
  top: "InnerProduct1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 1
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "SoftmaxWithLoss1"
  type: "SoftmaxWithLoss"
  bottom: "InnerProduct1"
  bottom: "label"
  top: "SoftmaxWithLoss1"
}
I0307 17:06:47.214258 28053 layer_factory.hpp:77] Creating layer cifar
I0307 17:06:47.214370 28053 db_lmdb.cpp:35] Opened lmdb examples/cifar10/cifar10_train_lmdb
I0307 17:06:47.214397 28053 net.cpp:84] Creating Layer cifar
I0307 17:06:47.214407 28053 net.cpp:380] cifar -> data
I0307 17:06:47.214427 28053 net.cpp:380] cifar -> label
I0307 17:06:47.214442 28053 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto
I0307 17:06:47.216598 28053 data_layer.cpp:45] output data size: 128,3,32,32
I0307 17:06:47.224714 28053 net.cpp:122] Setting up cifar
I0307 17:06:47.224743 28053 net.cpp:129] Top shape: 128 3 32 32 (393216)
I0307 17:06:47.224753 28053 net.cpp:129] Top shape: 128 (128)
I0307 17:06:47.224758 28053 net.cpp:137] Memory required for data: 1573376
I0307 17:06:47.224771 28053 layer_factory.hpp:77] Creating layer Convolution1
I0307 17:06:47.224807 28053 net.cpp:84] Creating Layer Convolution1
I0307 17:06:47.224817 28053 net.cpp:406] Convolution1 <- data
I0307 17:06:47.224835 28053 net.cpp:380] Convolution1 -> Convolution1
I0307 17:06:47.902432 28053 net.cpp:122] Setting up Convolution1
I0307 17:06:47.902462 28053 net.cpp:129] Top shape: 128 16 32 32 (2097152)
I0307 17:06:47.902467 28053 net.cpp:137] Memory required for data: 9961984
I0307 17:06:47.902487 28053 layer_factory.hpp:77] Creating layer BatchNorm1
I0307 17:06:47.902498 28053 net.cpp:84] Creating Layer BatchNorm1
I0307 17:06:47.902503 28053 net.cpp:406] BatchNorm1 <- Convolution1
I0307 17:06:47.902509 28053 net.cpp:367] BatchNorm1 -> Convolution1 (in-place)
I0307 17:06:47.902716 28053 net.cpp:122] Setting up BatchNorm1
I0307 17:06:47.902724 28053 net.cpp:129] Top shape: 128 16 32 32 (2097152)
I0307 17:06:47.902727 28053 net.cpp:137] Memory required for data: 18350592
I0307 17:06:47.902737 28053 layer_factory.hpp:77] Creating layer Scale1
I0307 17:06:47.902745 28053 net.cpp:84] Creating Layer Scale1
I0307 17:06:47.902748 28053 net.cpp:406] Scale1 <- Convolution1
I0307 17:06:47.902753 28053 net.cpp:367] Scale1 -> Convolution1 (in-place)
I0307 17:06:47.902793 28053 layer_factory.hpp:77] Creating layer Scale1
I0307 17:06:47.902909 28053 net.cpp:122] Setting up Scale1
I0307 17:06:47.902916 28053 net.cpp:129] Top shape: 128 16 32 32 (2097152)
I0307 17:06:47.902920 28053 net.cpp:137] Memory required for data: 26739200
I0307 17:06:47.902925 28053 layer_factory.hpp:77] Creating layer ReLU1
I0307 17:06:47.902932 28053 net.cpp:84] Creating Layer ReLU1
I0307 17:06:47.902935 28053 net.cpp:406] ReLU1 <- Convolution1
I0307 17:06:47.902940 28053 net.cpp:367] ReLU1 -> Convolution1 (in-place)
I0307 17:06:47.903475 28053 net.cpp:122] Setting up ReLU1
I0307 17:06:47.903487 28053 net.cpp:129] Top shape: 128 16 32 32 (2097152)
I0307 17:06:47.903491 28053 net.cpp:137] Memory required for data: 35127808
I0307 17:06:47.903496 28053 layer_factory.hpp:77] Creating layer Convolution1_ReLU1_0_split
I0307 17:06:47.903503 28053 net.cpp:84] Creating Layer Convolution1_ReLU1_0_split
I0307 17:06:47.903507 28053 net.cpp:406] Convolution1_ReLU1_0_split <- Convolution1
I0307 17:06:47.903512 28053 net.cpp:380] Convolution1_ReLU1_0_split -> Convolution1_ReLU1_0_split_0
I0307 17:06:47.903520 28053 net.cpp:380] Convolution1_ReLU1_0_split -> Convolution1_ReLU1_0_split_1
I0307 17:06:47.903587 28053 net.cpp:122] Setting up Convolution1_ReLU1_0_split
I0307 17:06:47.903595 28053 net.cpp:129] Top shape: 128 16 32 32 (2097152)
I0307 17:06:47.903600 28053 net.cpp:129] Top shape: 128 16 32 32 (2097152)
I0307 17:06:47.903604 28053 net.cpp:137] Memory required for data: 51905024
I0307 17:06:47.903607 28053 layer_factory.hpp:77] Creating layer Convolution2
I0307 17:06:47.903622 28053 net.cpp:84] Creating Layer Convolution2
I0307 17:06:47.903627 28053 net.cpp:406] Convolution2 <- Convolution1_ReLU1_0_split_0
I0307 17:06:47.903635 28053 net.cpp:380] Convolution2 -> Convolution2
I0307 17:06:47.906483 28053 net.cpp:122] Setting up Convolution2
I0307 17:06:47.906499 28053 net.cpp:129] Top shape: 128 16 32 32 (2097152)
I0307 17:06:47.906505 28053 net.cpp:137] Memory required for data: 60293632
I0307 17:06:47.906514 28053 layer_factory.hpp:77] Creating layer BatchNorm2
I0307 17:06:47.906522 28053 net.cpp:84] Creating Layer BatchNorm2
I0307 17:06:47.906525 28053 net.cpp:406] BatchNorm2 <- Convolution2
I0307 17:06:47.906533 28053 net.cpp:367] BatchNorm2 -> Convolution2 (in-place)
I0307 17:06:47.906744 28053 net.cpp:122] Setting up BatchNorm2
I0307 17:06:47.906754 28053 net.cpp:129] Top shape: 128 16 32 32 (2097152)
I0307 17:06:47.906756 28053 net.cpp:137] Memory required for data: 68682240
I0307 17:06:47.906764 28053 layer_factory.hpp:77] Creating layer Scale2
I0307 17:06:47.906771 28053 net.cpp:84] Creating Layer Scale2
I0307 17:06:47.906775 28053 net.cpp:406] Scale2 <- Convolution2
I0307 17:06:47.906780 28053 net.cpp:367] Scale2 -> Convolution2 (in-place)
I0307 17:06:47.906816 28053 layer_factory.hpp:77] Creating layer Scale2
I0307 17:06:47.906929 28053 net.cpp:122] Setting up Scale2
I0307 17:06:47.906939 28053 net.cpp:129] Top shape: 128 16 32 32 (2097152)
I0307 17:06:47.906942 28053 net.cpp:137] Memory required for data: 77070848
I0307 17:06:47.906947 28053 layer_factory.hpp:77] Creating layer ReLU2
I0307 17:06:47.906953 28053 net.cpp:84] Creating Layer ReLU2
I0307 17:06:47.906956 28053 net.cpp:406] ReLU2 <- Convolution2
I0307 17:06:47.906960 28053 net.cpp:367] ReLU2 -> Convolution2 (in-place)
I0307 17:06:47.907536 28053 net.cpp:122] Setting up ReLU2
I0307 17:06:47.907548 28053 net.cpp:129] Top shape: 128 16 32 32 (2097152)
I0307 17:06:47.907552 28053 net.cpp:137] Memory required for data: 85459456
I0307 17:06:47.907554 28053 layer_factory.hpp:77] Creating layer Convolution3
I0307 17:06:47.907567 28053 net.cpp:84] Creating Layer Convolution3
I0307 17:06:47.907570 28053 net.cpp:406] Convolution3 <- Convolution2
I0307 17:06:47.907578 28053 net.cpp:380] Convolution3 -> Convolution3
I0307 17:06:47.910055 28053 net.cpp:122] Setting up Convolution3
I0307 17:06:47.910073 28053 net.cpp:129] Top shape: 128 16 32 32 (2097152)
I0307 17:06:47.910075 28053 net.cpp:137] Memory required for data: 93848064
I0307 17:06:47.910082 28053 layer_factory.hpp:77] Creating layer BatchNorm3
I0307 17:06:47.910090 28053 net.cpp:84] Creating Layer BatchNorm3
I0307 17:06:47.910094 28053 net.cpp:406] BatchNorm3 <- Convolution3
I0307 17:06:47.910099 28053 net.cpp:367] BatchNorm3 -> Convolution3 (in-place)
I0307 17:06:47.910308 28053 net.cpp:122] Setting up BatchNorm3
I0307 17:06:47.910317 28053 net.cpp:129] Top shape: 128 16 32 32 (2097152)
I0307 17:06:47.910320 28053 net.cpp:137] Memory required for data: 102236672
I0307 17:06:47.910331 28053 layer_factory.hpp:77] Creating layer Scale3
I0307 17:06:47.910336 28053 net.cpp:84] Creating Layer Scale3
I0307 17:06:47.910341 28053 net.cpp:406] Scale3 <- Convolution3
I0307 17:06:47.910346 28053 net.cpp:367] Scale3 -> Convolution3 (in-place)
I0307 17:06:47.910385 28053 layer_factory.hpp:77] Creating layer Scale3
I0307 17:06:47.910502 28053 net.cpp:122] Setting up Scale3
I0307 17:06:47.910511 28053 net.cpp:129] Top shape: 128 16 32 32 (2097152)
I0307 17:06:47.910513 28053 net.cpp:137] Memory required for data: 110625280
I0307 17:06:47.910519 28053 layer_factory.hpp:77] Creating layer Eltwise1
I0307 17:06:47.910527 28053 net.cpp:84] Creating Layer Eltwise1
I0307 17:06:47.910544 28053 net.cpp:406] Eltwise1 <- Convolution1_ReLU1_0_split_1
I0307 17:06:47.910549 28053 net.cpp:406] Eltwise1 <- Convolution3
I0307 17:06:47.910554 28053 net.cpp:380] Eltwise1 -> Eltwise1
I0307 17:06:47.910586 28053 net.cpp:122] Setting up Eltwise1
I0307 17:06:47.910594 28053 net.cpp:129] Top shape: 128 16 32 32 (2097152)
I0307 17:06:47.910598 28053 net.cpp:137] Memory required for data: 119013888
I0307 17:06:47.910601 28053 layer_factory.hpp:77] Creating layer ReLU3
I0307 17:06:47.910606 28053 net.cpp:84] Creating Layer ReLU3
I0307 17:06:47.910609 28053 net.cpp:406] ReLU3 <- Eltwise1
I0307 17:06:47.910615 28053 net.cpp:367] ReLU3 -> Eltwise1 (in-place)
I0307 17:06:47.911583 28053 net.cpp:122] Setting up ReLU3
I0307 17:06:47.911598 28053 net.cpp:129] Top shape: 128 16 32 32 (2097152)
I0307 17:06:47.911602 28053 net.cpp:137] Memory required for data: 127402496
I0307 17:06:47.911605 28053 layer_factory.hpp:77] Creating layer Eltwise1_ReLU3_0_split
I0307 17:06:47.911612 28053 net.cpp:84] Creating Layer Eltwise1_ReLU3_0_split
I0307 17:06:47.911615 28053 net.cpp:406] Eltwise1_ReLU3_0_split <- Eltwise1
I0307 17:06:47.911623 28053 net.cpp:380] Eltwise1_ReLU3_0_split -> Eltwise1_ReLU3_0_split_0
I0307 17:06:47.911633 28053 net.cpp:380] Eltwise1_ReLU3_0_split -> Eltwise1_ReLU3_0_split_1
I0307 17:06:47.911679 28053 net.cpp:122] Setting up Eltwise1_ReLU3_0_split
I0307 17:06:47.911689 28053 net.cpp:129] Top shape: 128 16 32 32 (2097152)
I0307 17:06:47.911692 28053 net.cpp:129] Top shape: 128 16 32 32 (2097152)
I0307 17:06:47.911695 28053 net.cpp:137] Memory required for data: 144179712
I0307 17:06:47.911698 28053 layer_factory.hpp:77] Creating layer Convolution4
I0307 17:06:47.911711 28053 net.cpp:84] Creating Layer Convolution4
I0307 17:06:47.911715 28053 net.cpp:406] Convolution4 <- Eltwise1_ReLU3_0_split_0
I0307 17:06:47.911723 28053 net.cpp:380] Convolution4 -> Convolution4
I0307 17:06:47.914194 28053 net.cpp:122] Setting up Convolution4
I0307 17:06:47.914211 28053 net.cpp:129] Top shape: 128 16 32 32 (2097152)
I0307 17:06:47.914213 28053 net.cpp:137] Memory required for data: 152568320
I0307 17:06:47.914221 28053 layer_factory.hpp:77] Creating layer BatchNorm4
I0307 17:06:47.914228 28053 net.cpp:84] Creating Layer BatchNorm4
I0307 17:06:47.914232 28053 net.cpp:406] BatchNorm4 <- Convolution4
I0307 17:06:47.914237 28053 net.cpp:367] BatchNorm4 -> Convolution4 (in-place)
I0307 17:06:47.914445 28053 net.cpp:122] Setting up BatchNorm4
I0307 17:06:47.914453 28053 net.cpp:129] Top shape: 128 16 32 32 (2097152)
I0307 17:06:47.914456 28053 net.cpp:137] Memory required for data: 160956928
I0307 17:06:47.914464 28053 layer_factory.hpp:77] Creating layer Scale4
I0307 17:06:47.914469 28053 net.cpp:84] Creating Layer Scale4
I0307 17:06:47.914472 28053 net.cpp:406] Scale4 <- Convolution4
I0307 17:06:47.914477 28053 net.cpp:367] Scale4 -> Convolution4 (in-place)
I0307 17:06:47.914516 28053 layer_factory.hpp:77] Creating layer Scale4
I0307 17:06:47.914638 28053 net.cpp:122] Setting up Scale4
I0307 17:06:47.914645 28053 net.cpp:129] Top shape: 128 16 32 32 (2097152)
I0307 17:06:47.914649 28053 net.cpp:137] Memory required for data: 169345536
I0307 17:06:47.914654 28053 layer_factory.hpp:77] Creating layer ReLU4
I0307 17:06:47.914659 28053 net.cpp:84] Creating Layer ReLU4
I0307 17:06:47.914664 28053 net.cpp:406] ReLU4 <- Convolution4
I0307 17:06:47.914669 28053 net.cpp:367] ReLU4 -> Convolution4 (in-place)
I0307 17:06:47.915633 28053 net.cpp:122] Setting up ReLU4
I0307 17:06:47.915647 28053 net.cpp:129] Top shape: 128 16 32 32 (2097152)
I0307 17:06:47.915652 28053 net.cpp:137] Memory required for data: 177734144
I0307 17:06:47.915655 28053 layer_factory.hpp:77] Creating layer Convolution5
I0307 17:06:47.915666 28053 net.cpp:84] Creating Layer Convolution5
I0307 17:06:47.915670 28053 net.cpp:406] Convolution5 <- Convolution4
I0307 17:06:47.915678 28053 net.cpp:380] Convolution5 -> Convolution5
I0307 17:06:47.918203 28053 net.cpp:122] Setting up Convolution5
I0307 17:06:47.918222 28053 net.cpp:129] Top shape: 128 16 32 32 (2097152)
I0307 17:06:47.918241 28053 net.cpp:137] Memory required for data: 186122752
I0307 17:06:47.918248 28053 layer_factory.hpp:77] Creating layer BatchNorm5
I0307 17:06:47.918256 28053 net.cpp:84] Creating Layer BatchNorm5
I0307 17:06:47.918260 28053 net.cpp:406] BatchNorm5 <- Convolution5
I0307 17:06:47.918267 28053 net.cpp:367] BatchNorm5 -> Convolution5 (in-place)
I0307 17:06:47.918488 28053 net.cpp:122] Setting up BatchNorm5
I0307 17:06:47.918498 28053 net.cpp:129] Top shape: 128 16 32 32 (2097152)
I0307 17:06:47.918500 28053 net.cpp:137] Memory required for data: 194511360
I0307 17:06:47.918511 28053 layer_factory.hpp:77] Creating layer Scale5
I0307 17:06:47.918519 28053 net.cpp:84] Creating Layer Scale5
I0307 17:06:47.918524 28053 net.cpp:406] Scale5 <- Convolution5
I0307 17:06:47.918529 28053 net.cpp:367] Scale5 -> Convolution5 (in-place)
I0307 17:06:47.918576 28053 layer_factory.hpp:77] Creating layer Scale5
I0307 17:06:47.918699 28053 net.cpp:122] Setting up Scale5
I0307 17:06:47.918709 28053 net.cpp:129] Top shape: 128 16 32 32 (2097152)
I0307 17:06:47.918711 28053 net.cpp:137] Memory required for data: 202899968
I0307 17:06:47.918717 28053 layer_factory.hpp:77] Creating layer Eltwise2
I0307 17:06:47.918723 28053 net.cpp:84] Creating Layer Eltwise2
I0307 17:06:47.918726 28053 net.cpp:406] Eltwise2 <- Eltwise1_ReLU3_0_split_1
I0307 17:06:47.918731 28053 net.cpp:406] Eltwise2 <- Convolution5
I0307 17:06:47.918738 28053 net.cpp:380] Eltwise2 -> Eltwise2
I0307 17:06:47.918763 28053 net.cpp:122] Setting up Eltwise2
I0307 17:06:47.918771 28053 net.cpp:129] Top shape: 128 16 32 32 (2097152)
I0307 17:06:47.918773 28053 net.cpp:137] Memory required for data: 211288576
I0307 17:06:47.918777 28053 layer_factory.hpp:77] Creating layer ReLU5
I0307 17:06:47.918782 28053 net.cpp:84] Creating Layer ReLU5
I0307 17:06:47.918786 28053 net.cpp:406] ReLU5 <- Eltwise2
I0307 17:06:47.918790 28053 net.cpp:367] ReLU5 -> Eltwise2 (in-place)
I0307 17:06:47.919407 28053 net.cpp:122] Setting up ReLU5
I0307 17:06:47.919420 28053 net.cpp:129] Top shape: 128 16 32 32 (2097152)
I0307 17:06:47.919425 28053 net.cpp:137] Memory required for data: 219677184
I0307 17:06:47.919427 28053 layer_factory.hpp:77] Creating layer Eltwise2_ReLU5_0_split
I0307 17:06:47.919436 28053 net.cpp:84] Creating Layer Eltwise2_ReLU5_0_split
I0307 17:06:47.919440 28053 net.cpp:406] Eltwise2_ReLU5_0_split <- Eltwise2
I0307 17:06:47.919445 28053 net.cpp:380] Eltwise2_ReLU5_0_split -> Eltwise2_ReLU5_0_split_0
I0307 17:06:47.919457 28053 net.cpp:380] Eltwise2_ReLU5_0_split -> Eltwise2_ReLU5_0_split_1
I0307 17:06:47.919510 28053 net.cpp:122] Setting up Eltwise2_ReLU5_0_split
I0307 17:06:47.919520 28053 net.cpp:129] Top shape: 128 16 32 32 (2097152)
I0307 17:06:47.919526 28053 net.cpp:129] Top shape: 128 16 32 32 (2097152)
I0307 17:06:47.919530 28053 net.cpp:137] Memory required for data: 236454400
I0307 17:06:47.919536 28053 layer_factory.hpp:77] Creating layer Convolution6
I0307 17:06:47.919551 28053 net.cpp:84] Creating Layer Convolution6
I0307 17:06:47.919556 28053 net.cpp:406] Convolution6 <- Eltwise2_ReLU5_0_split_0
I0307 17:06:47.919564 28053 net.cpp:380] Convolution6 -> Convolution6
I0307 17:06:47.922500 28053 net.cpp:122] Setting up Convolution6
I0307 17:06:47.922519 28053 net.cpp:129] Top shape: 128 16 32 32 (2097152)
I0307 17:06:47.922523 28053 net.cpp:137] Memory required for data: 244843008
I0307 17:06:47.922531 28053 layer_factory.hpp:77] Creating layer BatchNorm6
I0307 17:06:47.922544 28053 net.cpp:84] Creating Layer BatchNorm6
I0307 17:06:47.922549 28053 net.cpp:406] BatchNorm6 <- Convolution6
I0307 17:06:47.922556 28053 net.cpp:367] BatchNorm6 -> Convolution6 (in-place)
I0307 17:06:47.922781 28053 net.cpp:122] Setting up BatchNorm6
I0307 17:06:47.922791 28053 net.cpp:129] Top shape: 128 16 32 32 (2097152)
I0307 17:06:47.922794 28053 net.cpp:137] Memory required for data: 253231616
I0307 17:06:47.922803 28053 layer_factory.hpp:77] Creating layer Scale6
I0307 17:06:47.922811 28053 net.cpp:84] Creating Layer Scale6
I0307 17:06:47.922816 28053 net.cpp:406] Scale6 <- Convolution6
I0307 17:06:47.922839 28053 net.cpp:367] Scale6 -> Convolution6 (in-place)
I0307 17:06:47.922888 28053 layer_factory.hpp:77] Creating layer Scale6
I0307 17:06:47.923017 28053 net.cpp:122] Setting up Scale6
I0307 17:06:47.923027 28053 net.cpp:129] Top shape: 128 16 32 32 (2097152)
I0307 17:06:47.923029 28053 net.cpp:137] Memory required for data: 261620224
I0307 17:06:47.923036 28053 layer_factory.hpp:77] Creating layer ReLU6
I0307 17:06:47.923044 28053 net.cpp:84] Creating Layer ReLU6
I0307 17:06:47.923048 28053 net.cpp:406] ReLU6 <- Convolution6
I0307 17:06:47.923056 28053 net.cpp:367] ReLU6 -> Convolution6 (in-place)
I0307 17:06:47.923647 28053 net.cpp:122] Setting up ReLU6
I0307 17:06:47.923660 28053 net.cpp:129] Top shape: 128 16 32 32 (2097152)
I0307 17:06:47.923663 28053 net.cpp:137] Memory required for data: 270008832
I0307 17:06:47.923667 28053 layer_factory.hpp:77] Creating layer Convolution7
I0307 17:06:47.923682 28053 net.cpp:84] Creating Layer Convolution7
I0307 17:06:47.923688 28053 net.cpp:406] Convolution7 <- Convolution6
I0307 17:06:47.923698 28053 net.cpp:380] Convolution7 -> Convolution7
I0307 17:06:47.926234 28053 net.cpp:122] Setting up Convolution7
I0307 17:06:47.926250 28053 net.cpp:129] Top shape: 128 16 32 32 (2097152)
I0307 17:06:47.926254 28053 net.cpp:137] Memory required for data: 278397440
I0307 17:06:47.926264 28053 layer_factory.hpp:77] Creating layer BatchNorm7
I0307 17:06:47.926275 28053 net.cpp:84] Creating Layer BatchNorm7
I0307 17:06:47.926281 28053 net.cpp:406] BatchNorm7 <- Convolution7
I0307 17:06:47.926291 28053 net.cpp:367] BatchNorm7 -> Convolution7 (in-place)
I0307 17:06:47.926512 28053 net.cpp:122] Setting up BatchNorm7
I0307 17:06:47.926522 28053 net.cpp:129] Top shape: 128 16 32 32 (2097152)
I0307 17:06:47.926525 28053 net.cpp:137] Memory required for data: 286786048
I0307 17:06:47.926534 28053 layer_factory.hpp:77] Creating layer Scale7
I0307 17:06:47.926548 28053 net.cpp:84] Creating Layer Scale7
I0307 17:06:47.926555 28053 net.cpp:406] Scale7 <- Convolution7
I0307 17:06:47.926561 28053 net.cpp:367] Scale7 -> Convolution7 (in-place)
I0307 17:06:47.926607 28053 layer_factory.hpp:77] Creating layer Scale7
I0307 17:06:47.926738 28053 net.cpp:122] Setting up Scale7
I0307 17:06:47.926748 28053 net.cpp:129] Top shape: 128 16 32 32 (2097152)
I0307 17:06:47.926750 28053 net.cpp:137] Memory required for data: 295174656
I0307 17:06:47.926759 28053 layer_factory.hpp:77] Creating layer Eltwise3
I0307 17:06:47.926767 28053 net.cpp:84] Creating Layer Eltwise3
I0307 17:06:47.926772 28053 net.cpp:406] Eltwise3 <- Eltwise2_ReLU5_0_split_1
I0307 17:06:47.926779 28053 net.cpp:406] Eltwise3 <- Convolution7
I0307 17:06:47.926787 28053 net.cpp:380] Eltwise3 -> Eltwise3
I0307 17:06:47.926818 28053 net.cpp:122] Setting up Eltwise3
I0307 17:06:47.926827 28053 net.cpp:129] Top shape: 128 16 32 32 (2097152)
I0307 17:06:47.926831 28053 net.cpp:137] Memory required for data: 303563264
I0307 17:06:47.926834 28053 layer_factory.hpp:77] Creating layer ReLU7
I0307 17:06:47.926844 28053 net.cpp:84] Creating Layer ReLU7
I0307 17:06:47.926848 28053 net.cpp:406] ReLU7 <- Eltwise3
I0307 17:06:47.926854 28053 net.cpp:367] ReLU7 -> Eltwise3 (in-place)
I0307 17:06:47.927842 28053 net.cpp:122] Setting up ReLU7
I0307 17:06:47.927858 28053 net.cpp:129] Top shape: 128 16 32 32 (2097152)
I0307 17:06:47.927862 28053 net.cpp:137] Memory required for data: 311951872
I0307 17:06:47.927868 28053 layer_factory.hpp:77] Creating layer Eltwise3_ReLU7_0_split
I0307 17:06:47.927877 28053 net.cpp:84] Creating Layer Eltwise3_ReLU7_0_split
I0307 17:06:47.927882 28053 net.cpp:406] Eltwise3_ReLU7_0_split <- Eltwise3
I0307 17:06:47.927892 28053 net.cpp:380] Eltwise3_ReLU7_0_split -> Eltwise3_ReLU7_0_split_0
I0307 17:06:47.927903 28053 net.cpp:380] Eltwise3_ReLU7_0_split -> Eltwise3_ReLU7_0_split_1
I0307 17:06:47.927958 28053 net.cpp:122] Setting up Eltwise3_ReLU7_0_split
I0307 17:06:47.927966 28053 net.cpp:129] Top shape: 128 16 32 32 (2097152)
I0307 17:06:47.927971 28053 net.cpp:129] Top shape: 128 16 32 32 (2097152)
I0307 17:06:47.927990 28053 net.cpp:137] Memory required for data: 328729088
I0307 17:06:47.927994 28053 layer_factory.hpp:77] Creating layer Convolution8
I0307 17:06:47.928009 28053 net.cpp:84] Creating Layer Convolution8
I0307 17:06:47.928015 28053 net.cpp:406] Convolution8 <- Eltwise3_ReLU7_0_split_0
I0307 17:06:47.928025 28053 net.cpp:380] Convolution8 -> Convolution8
I0307 17:06:47.930583 28053 net.cpp:122] Setting up Convolution8
I0307 17:06:47.930599 28053 net.cpp:129] Top shape: 128 32 16 16 (1048576)
I0307 17:06:47.930603 28053 net.cpp:137] Memory required for data: 332923392
I0307 17:06:47.930610 28053 layer_factory.hpp:77] Creating layer BatchNorm8
I0307 17:06:47.930621 28053 net.cpp:84] Creating Layer BatchNorm8
I0307 17:06:47.930626 28053 net.cpp:406] BatchNorm8 <- Convolution8
I0307 17:06:47.930634 28053 net.cpp:367] BatchNorm8 -> Convolution8 (in-place)
I0307 17:06:47.930852 28053 net.cpp:122] Setting up BatchNorm8
I0307 17:06:47.930862 28053 net.cpp:129] Top shape: 128 32 16 16 (1048576)
I0307 17:06:47.930866 28053 net.cpp:137] Memory required for data: 337117696
I0307 17:06:47.930876 28053 layer_factory.hpp:77] Creating layer Scale8
I0307 17:06:47.930882 28053 net.cpp:84] Creating Layer Scale8
I0307 17:06:47.930886 28053 net.cpp:406] Scale8 <- Convolution8
I0307 17:06:47.930894 28053 net.cpp:367] Scale8 -> Convolution8 (in-place)
I0307 17:06:47.930939 28053 layer_factory.hpp:77] Creating layer Scale8
I0307 17:06:47.931071 28053 net.cpp:122] Setting up Scale8
I0307 17:06:47.931079 28053 net.cpp:129] Top shape: 128 32 16 16 (1048576)
I0307 17:06:47.931082 28053 net.cpp:137] Memory required for data: 341312000
I0307 17:06:47.931090 28053 layer_factory.hpp:77] Creating layer Convolution9
I0307 17:06:47.931103 28053 net.cpp:84] Creating Layer Convolution9
I0307 17:06:47.931110 28053 net.cpp:406] Convolution9 <- Eltwise3_ReLU7_0_split_1
I0307 17:06:47.931120 28053 net.cpp:380] Convolution9 -> Convolution9
I0307 17:06:47.934938 28053 net.cpp:122] Setting up Convolution9
I0307 17:06:47.934965 28053 net.cpp:129] Top shape: 128 32 16 16 (1048576)
I0307 17:06:47.934973 28053 net.cpp:137] Memory required for data: 345506304
I0307 17:06:47.934984 28053 layer_factory.hpp:77] Creating layer BatchNorm9
I0307 17:06:47.934998 28053 net.cpp:84] Creating Layer BatchNorm9
I0307 17:06:47.935007 28053 net.cpp:406] BatchNorm9 <- Convolution9
I0307 17:06:47.935017 28053 net.cpp:367] BatchNorm9 -> Convolution9 (in-place)
I0307 17:06:47.935293 28053 net.cpp:122] Setting up BatchNorm9
I0307 17:06:47.935308 28053 net.cpp:129] Top shape: 128 32 16 16 (1048576)
I0307 17:06:47.935313 28053 net.cpp:137] Memory required for data: 349700608
I0307 17:06:47.935324 28053 layer_factory.hpp:77] Creating layer Scale9
I0307 17:06:47.935333 28053 net.cpp:84] Creating Layer Scale9
I0307 17:06:47.935338 28053 net.cpp:406] Scale9 <- Convolution9
I0307 17:06:47.935343 28053 net.cpp:367] Scale9 -> Convolution9 (in-place)
I0307 17:06:47.935389 28053 layer_factory.hpp:77] Creating layer Scale9
I0307 17:06:47.935514 28053 net.cpp:122] Setting up Scale9
I0307 17:06:47.935523 28053 net.cpp:129] Top shape: 128 32 16 16 (1048576)
I0307 17:06:47.935526 28053 net.cpp:137] Memory required for data: 353894912
I0307 17:06:47.935533 28053 layer_factory.hpp:77] Creating layer ReLU8
I0307 17:06:47.935539 28053 net.cpp:84] Creating Layer ReLU8
I0307 17:06:47.935544 28053 net.cpp:406] ReLU8 <- Convolution9
I0307 17:06:47.935547 28053 net.cpp:367] ReLU8 -> Convolution9 (in-place)
I0307 17:06:47.936640 28053 net.cpp:122] Setting up ReLU8
I0307 17:06:47.936655 28053 net.cpp:129] Top shape: 128 32 16 16 (1048576)
I0307 17:06:47.936659 28053 net.cpp:137] Memory required for data: 358089216
I0307 17:06:47.936663 28053 layer_factory.hpp:77] Creating layer Convolution10
I0307 17:06:47.936676 28053 net.cpp:84] Creating Layer Convolution10
I0307 17:06:47.936681 28053 net.cpp:406] Convolution10 <- Convolution9
I0307 17:06:47.936688 28053 net.cpp:380] Convolution10 -> Convolution10
I0307 17:06:47.939419 28053 net.cpp:122] Setting up Convolution10
I0307 17:06:47.939435 28053 net.cpp:129] Top shape: 128 32 16 16 (1048576)
I0307 17:06:47.939453 28053 net.cpp:137] Memory required for data: 362283520
I0307 17:06:47.939471 28053 layer_factory.hpp:77] Creating layer BatchNorm10
I0307 17:06:47.939481 28053 net.cpp:84] Creating Layer BatchNorm10
I0307 17:06:47.939486 28053 net.cpp:406] BatchNorm10 <- Convolution10
I0307 17:06:47.939492 28053 net.cpp:367] BatchNorm10 -> Convolution10 (in-place)
I0307 17:06:47.939704 28053 net.cpp:122] Setting up BatchNorm10
I0307 17:06:47.939713 28053 net.cpp:129] Top shape: 128 32 16 16 (1048576)
I0307 17:06:47.939716 28053 net.cpp:137] Memory required for data: 366477824
I0307 17:06:47.939723 28053 layer_factory.hpp:77] Creating layer Scale10
I0307 17:06:47.939730 28053 net.cpp:84] Creating Layer Scale10
I0307 17:06:47.939734 28053 net.cpp:406] Scale10 <- Convolution10
I0307 17:06:47.939739 28053 net.cpp:367] Scale10 -> Convolution10 (in-place)
I0307 17:06:47.939779 28053 layer_factory.hpp:77] Creating layer Scale10
I0307 17:06:47.939918 28053 net.cpp:122] Setting up Scale10
I0307 17:06:47.939929 28053 net.cpp:129] Top shape: 128 32 16 16 (1048576)
I0307 17:06:47.939932 28053 net.cpp:137] Memory required for data: 370672128
I0307 17:06:47.939939 28053 layer_factory.hpp:77] Creating layer Eltwise4
I0307 17:06:47.939946 28053 net.cpp:84] Creating Layer Eltwise4
I0307 17:06:47.939950 28053 net.cpp:406] Eltwise4 <- Convolution8
I0307 17:06:47.939955 28053 net.cpp:406] Eltwise4 <- Convolution10
I0307 17:06:47.939960 28053 net.cpp:380] Eltwise4 -> Eltwise4
I0307 17:06:47.939985 28053 net.cpp:122] Setting up Eltwise4
I0307 17:06:47.939991 28053 net.cpp:129] Top shape: 128 32 16 16 (1048576)
I0307 17:06:47.939996 28053 net.cpp:137] Memory required for data: 374866432
I0307 17:06:47.940001 28053 layer_factory.hpp:77] Creating layer ReLU9
I0307 17:06:47.940009 28053 net.cpp:84] Creating Layer ReLU9
I0307 17:06:47.940014 28053 net.cpp:406] ReLU9 <- Eltwise4
I0307 17:06:47.940021 28053 net.cpp:367] ReLU9 -> Eltwise4 (in-place)
I0307 17:06:47.940624 28053 net.cpp:122] Setting up ReLU9
I0307 17:06:47.940637 28053 net.cpp:129] Top shape: 128 32 16 16 (1048576)
I0307 17:06:47.940640 28053 net.cpp:137] Memory required for data: 379060736
I0307 17:06:47.940644 28053 layer_factory.hpp:77] Creating layer Eltwise4_ReLU9_0_split
I0307 17:06:47.940654 28053 net.cpp:84] Creating Layer Eltwise4_ReLU9_0_split
I0307 17:06:47.940660 28053 net.cpp:406] Eltwise4_ReLU9_0_split <- Eltwise4
I0307 17:06:47.940675 28053 net.cpp:380] Eltwise4_ReLU9_0_split -> Eltwise4_ReLU9_0_split_0
I0307 17:06:47.940685 28053 net.cpp:380] Eltwise4_ReLU9_0_split -> Eltwise4_ReLU9_0_split_1
I0307 17:06:47.940743 28053 net.cpp:122] Setting up Eltwise4_ReLU9_0_split
I0307 17:06:47.940752 28053 net.cpp:129] Top shape: 128 32 16 16 (1048576)
I0307 17:06:47.940757 28053 net.cpp:129] Top shape: 128 32 16 16 (1048576)
I0307 17:06:47.940760 28053 net.cpp:137] Memory required for data: 387449344
I0307 17:06:47.940765 28053 layer_factory.hpp:77] Creating layer Convolution11
I0307 17:06:47.940788 28053 net.cpp:84] Creating Layer Convolution11
I0307 17:06:47.940794 28053 net.cpp:406] Convolution11 <- Eltwise4_ReLU9_0_split_0
I0307 17:06:47.940804 28053 net.cpp:380] Convolution11 -> Convolution11
I0307 17:06:47.944017 28053 net.cpp:122] Setting up Convolution11
I0307 17:06:47.944033 28053 net.cpp:129] Top shape: 128 32 16 16 (1048576)
I0307 17:06:47.944037 28053 net.cpp:137] Memory required for data: 391643648
I0307 17:06:47.944046 28053 layer_factory.hpp:77] Creating layer BatchNorm11
I0307 17:06:47.944056 28053 net.cpp:84] Creating Layer BatchNorm11
I0307 17:06:47.944062 28053 net.cpp:406] BatchNorm11 <- Convolution11
I0307 17:06:47.944067 28053 net.cpp:367] BatchNorm11 -> Convolution11 (in-place)
I0307 17:06:47.944283 28053 net.cpp:122] Setting up BatchNorm11
I0307 17:06:47.944293 28053 net.cpp:129] Top shape: 128 32 16 16 (1048576)
I0307 17:06:47.944295 28053 net.cpp:137] Memory required for data: 395837952
I0307 17:06:47.944303 28053 layer_factory.hpp:77] Creating layer Scale11
I0307 17:06:47.944310 28053 net.cpp:84] Creating Layer Scale11
I0307 17:06:47.944325 28053 net.cpp:406] Scale11 <- Convolution11
I0307 17:06:47.944331 28053 net.cpp:367] Scale11 -> Convolution11 (in-place)
I0307 17:06:47.944372 28053 layer_factory.hpp:77] Creating layer Scale11
I0307 17:06:47.944499 28053 net.cpp:122] Setting up Scale11
I0307 17:06:47.944507 28053 net.cpp:129] Top shape: 128 32 16 16 (1048576)
I0307 17:06:47.944510 28053 net.cpp:137] Memory required for data: 400032256
I0307 17:06:47.944516 28053 layer_factory.hpp:77] Creating layer ReLU10
I0307 17:06:47.944523 28053 net.cpp:84] Creating Layer ReLU10
I0307 17:06:47.944526 28053 net.cpp:406] ReLU10 <- Convolution11
I0307 17:06:47.944531 28053 net.cpp:367] ReLU10 -> Convolution11 (in-place)
I0307 17:06:47.945132 28053 net.cpp:122] Setting up ReLU10
I0307 17:06:47.945147 28053 net.cpp:129] Top shape: 128 32 16 16 (1048576)
I0307 17:06:47.945150 28053 net.cpp:137] Memory required for data: 404226560
I0307 17:06:47.945154 28053 layer_factory.hpp:77] Creating layer Convolution12
I0307 17:06:47.945165 28053 net.cpp:84] Creating Layer Convolution12
I0307 17:06:47.945170 28053 net.cpp:406] Convolution12 <- Convolution11
I0307 17:06:47.945178 28053 net.cpp:380] Convolution12 -> Convolution12
I0307 17:06:47.947933 28053 net.cpp:122] Setting up Convolution12
I0307 17:06:47.947952 28053 net.cpp:129] Top shape: 128 32 16 16 (1048576)
I0307 17:06:47.947954 28053 net.cpp:137] Memory required for data: 408420864
I0307 17:06:47.947962 28053 layer_factory.hpp:77] Creating layer BatchNorm12
I0307 17:06:47.947968 28053 net.cpp:84] Creating Layer BatchNorm12
I0307 17:06:47.947971 28053 net.cpp:406] BatchNorm12 <- Convolution12
I0307 17:06:47.947978 28053 net.cpp:367] BatchNorm12 -> Convolution12 (in-place)
I0307 17:06:47.948195 28053 net.cpp:122] Setting up BatchNorm12
I0307 17:06:47.948204 28053 net.cpp:129] Top shape: 128 32 16 16 (1048576)
I0307 17:06:47.948207 28053 net.cpp:137] Memory required for data: 412615168
I0307 17:06:47.948215 28053 layer_factory.hpp:77] Creating layer Scale12
I0307 17:06:47.948220 28053 net.cpp:84] Creating Layer Scale12
I0307 17:06:47.948223 28053 net.cpp:406] Scale12 <- Convolution12
I0307 17:06:47.948228 28053 net.cpp:367] Scale12 -> Convolution12 (in-place)
I0307 17:06:47.948267 28053 layer_factory.hpp:77] Creating layer Scale12
I0307 17:06:47.948393 28053 net.cpp:122] Setting up Scale12
I0307 17:06:47.948402 28053 net.cpp:129] Top shape: 128 32 16 16 (1048576)
I0307 17:06:47.948405 28053 net.cpp:137] Memory required for data: 416809472
I0307 17:06:47.948411 28053 layer_factory.hpp:77] Creating layer Eltwise5
I0307 17:06:47.948418 28053 net.cpp:84] Creating Layer Eltwise5
I0307 17:06:47.948423 28053 net.cpp:406] Eltwise5 <- Eltwise4_ReLU9_0_split_1
I0307 17:06:47.948428 28053 net.cpp:406] Eltwise5 <- Convolution12
I0307 17:06:47.948433 28053 net.cpp:380] Eltwise5 -> Eltwise5
I0307 17:06:47.948458 28053 net.cpp:122] Setting up Eltwise5
I0307 17:06:47.948465 28053 net.cpp:129] Top shape: 128 32 16 16 (1048576)
I0307 17:06:47.948468 28053 net.cpp:137] Memory required for data: 421003776
I0307 17:06:47.948472 28053 layer_factory.hpp:77] Creating layer ReLU11
I0307 17:06:47.948477 28053 net.cpp:84] Creating Layer ReLU11
I0307 17:06:47.948480 28053 net.cpp:406] ReLU11 <- Eltwise5
I0307 17:06:47.948484 28053 net.cpp:367] ReLU11 -> Eltwise5 (in-place)
I0307 17:06:47.949498 28053 net.cpp:122] Setting up ReLU11
I0307 17:06:47.949513 28053 net.cpp:129] Top shape: 128 32 16 16 (1048576)
I0307 17:06:47.949517 28053 net.cpp:137] Memory required for data: 425198080
I0307 17:06:47.949520 28053 layer_factory.hpp:77] Creating layer Eltwise5_ReLU11_0_split
I0307 17:06:47.949527 28053 net.cpp:84] Creating Layer Eltwise5_ReLU11_0_split
I0307 17:06:47.949530 28053 net.cpp:406] Eltwise5_ReLU11_0_split <- Eltwise5
I0307 17:06:47.949538 28053 net.cpp:380] Eltwise5_ReLU11_0_split -> Eltwise5_ReLU11_0_split_0
I0307 17:06:47.949548 28053 net.cpp:380] Eltwise5_ReLU11_0_split -> Eltwise5_ReLU11_0_split_1
I0307 17:06:47.949601 28053 net.cpp:122] Setting up Eltwise5_ReLU11_0_split
I0307 17:06:47.949610 28053 net.cpp:129] Top shape: 128 32 16 16 (1048576)
I0307 17:06:47.949626 28053 net.cpp:129] Top shape: 128 32 16 16 (1048576)
I0307 17:06:47.949630 28053 net.cpp:137] Memory required for data: 433586688
I0307 17:06:47.949633 28053 layer_factory.hpp:77] Creating layer Convolution13
I0307 17:06:47.949645 28053 net.cpp:84] Creating Layer Convolution13
I0307 17:06:47.949656 28053 net.cpp:406] Convolution13 <- Eltwise5_ReLU11_0_split_0
I0307 17:06:47.949663 28053 net.cpp:380] Convolution13 -> Convolution13
I0307 17:06:47.952370 28053 net.cpp:122] Setting up Convolution13
I0307 17:06:47.952386 28053 net.cpp:129] Top shape: 128 32 16 16 (1048576)
I0307 17:06:47.952390 28053 net.cpp:137] Memory required for data: 437780992
I0307 17:06:47.952397 28053 layer_factory.hpp:77] Creating layer BatchNorm13
I0307 17:06:47.952404 28053 net.cpp:84] Creating Layer BatchNorm13
I0307 17:06:47.952407 28053 net.cpp:406] BatchNorm13 <- Convolution13
I0307 17:06:47.952414 28053 net.cpp:367] BatchNorm13 -> Convolution13 (in-place)
I0307 17:06:47.952632 28053 net.cpp:122] Setting up BatchNorm13
I0307 17:06:47.952641 28053 net.cpp:129] Top shape: 128 32 16 16 (1048576)
I0307 17:06:47.952644 28053 net.cpp:137] Memory required for data: 441975296
I0307 17:06:47.952651 28053 layer_factory.hpp:77] Creating layer Scale13
I0307 17:06:47.952657 28053 net.cpp:84] Creating Layer Scale13
I0307 17:06:47.952661 28053 net.cpp:406] Scale13 <- Convolution13
I0307 17:06:47.952666 28053 net.cpp:367] Scale13 -> Convolution13 (in-place)
I0307 17:06:47.952704 28053 layer_factory.hpp:77] Creating layer Scale13
I0307 17:06:47.952848 28053 net.cpp:122] Setting up Scale13
I0307 17:06:47.952859 28053 net.cpp:129] Top shape: 128 32 16 16 (1048576)
I0307 17:06:47.952862 28053 net.cpp:137] Memory required for data: 446169600
I0307 17:06:47.952868 28053 layer_factory.hpp:77] Creating layer ReLU12
I0307 17:06:47.952877 28053 net.cpp:84] Creating Layer ReLU12
I0307 17:06:47.952880 28053 net.cpp:406] ReLU12 <- Convolution13
I0307 17:06:47.952885 28053 net.cpp:367] ReLU12 -> Convolution13 (in-place)
I0307 17:06:47.953478 28053 net.cpp:122] Setting up ReLU12
I0307 17:06:47.953490 28053 net.cpp:129] Top shape: 128 32 16 16 (1048576)
I0307 17:06:47.953495 28053 net.cpp:137] Memory required for data: 450363904
I0307 17:06:47.953497 28053 layer_factory.hpp:77] Creating layer Convolution14
I0307 17:06:47.953507 28053 net.cpp:84] Creating Layer Convolution14
I0307 17:06:47.953513 28053 net.cpp:406] Convolution14 <- Convolution13
I0307 17:06:47.953521 28053 net.cpp:380] Convolution14 -> Convolution14
I0307 17:06:47.956923 28053 net.cpp:122] Setting up Convolution14
I0307 17:06:47.956938 28053 net.cpp:129] Top shape: 128 32 16 16 (1048576)
I0307 17:06:47.956943 28053 net.cpp:137] Memory required for data: 454558208
I0307 17:06:47.956950 28053 layer_factory.hpp:77] Creating layer BatchNorm14
I0307 17:06:47.956964 28053 net.cpp:84] Creating Layer BatchNorm14
I0307 17:06:47.956969 28053 net.cpp:406] BatchNorm14 <- Convolution14
I0307 17:06:47.956975 28053 net.cpp:367] BatchNorm14 -> Convolution14 (in-place)
I0307 17:06:47.957202 28053 net.cpp:122] Setting up BatchNorm14
I0307 17:06:47.957211 28053 net.cpp:129] Top shape: 128 32 16 16 (1048576)
I0307 17:06:47.957214 28053 net.cpp:137] Memory required for data: 458752512
I0307 17:06:47.957221 28053 layer_factory.hpp:77] Creating layer Scale14
I0307 17:06:47.957228 28053 net.cpp:84] Creating Layer Scale14
I0307 17:06:47.957232 28053 net.cpp:406] Scale14 <- Convolution14
I0307 17:06:47.957238 28053 net.cpp:367] Scale14 -> Convolution14 (in-place)
I0307 17:06:47.957279 28053 layer_factory.hpp:77] Creating layer Scale14
I0307 17:06:47.957407 28053 net.cpp:122] Setting up Scale14
I0307 17:06:47.957415 28053 net.cpp:129] Top shape: 128 32 16 16 (1048576)
I0307 17:06:47.957418 28053 net.cpp:137] Memory required for data: 462946816
I0307 17:06:47.957424 28053 layer_factory.hpp:77] Creating layer Eltwise6
I0307 17:06:47.957432 28053 net.cpp:84] Creating Layer Eltwise6
I0307 17:06:47.957435 28053 net.cpp:406] Eltwise6 <- Eltwise5_ReLU11_0_split_1
I0307 17:06:47.957440 28053 net.cpp:406] Eltwise6 <- Convolution14
I0307 17:06:47.957458 28053 net.cpp:380] Eltwise6 -> Eltwise6
I0307 17:06:47.957486 28053 net.cpp:122] Setting up Eltwise6
I0307 17:06:47.957494 28053 net.cpp:129] Top shape: 128 32 16 16 (1048576)
I0307 17:06:47.957496 28053 net.cpp:137] Memory required for data: 467141120
I0307 17:06:47.957500 28053 layer_factory.hpp:77] Creating layer ReLU13
I0307 17:06:47.957505 28053 net.cpp:84] Creating Layer ReLU13
I0307 17:06:47.957509 28053 net.cpp:406] ReLU13 <- Eltwise6
I0307 17:06:47.957514 28053 net.cpp:367] ReLU13 -> Eltwise6 (in-place)
I0307 17:06:47.958112 28053 net.cpp:122] Setting up ReLU13
I0307 17:06:47.958124 28053 net.cpp:129] Top shape: 128 32 16 16 (1048576)
I0307 17:06:47.958127 28053 net.cpp:137] Memory required for data: 471335424
I0307 17:06:47.958132 28053 layer_factory.hpp:77] Creating layer Eltwise6_ReLU13_0_split
I0307 17:06:47.958137 28053 net.cpp:84] Creating Layer Eltwise6_ReLU13_0_split
I0307 17:06:47.958140 28053 net.cpp:406] Eltwise6_ReLU13_0_split <- Eltwise6
I0307 17:06:47.958148 28053 net.cpp:380] Eltwise6_ReLU13_0_split -> Eltwise6_ReLU13_0_split_0
I0307 17:06:47.958155 28053 net.cpp:380] Eltwise6_ReLU13_0_split -> Eltwise6_ReLU13_0_split_1
I0307 17:06:47.958206 28053 net.cpp:122] Setting up Eltwise6_ReLU13_0_split
I0307 17:06:47.958214 28053 net.cpp:129] Top shape: 128 32 16 16 (1048576)
I0307 17:06:47.958218 28053 net.cpp:129] Top shape: 128 32 16 16 (1048576)
I0307 17:06:47.958221 28053 net.cpp:137] Memory required for data: 479724032
I0307 17:06:47.958225 28053 layer_factory.hpp:77] Creating layer Convolution15
I0307 17:06:47.958235 28053 net.cpp:84] Creating Layer Convolution15
I0307 17:06:47.958238 28053 net.cpp:406] Convolution15 <- Eltwise6_ReLU13_0_split_0
I0307 17:06:47.958246 28053 net.cpp:380] Convolution15 -> Convolution15
I0307 17:06:47.960793 28053 net.cpp:122] Setting up Convolution15
I0307 17:06:47.960808 28053 net.cpp:129] Top shape: 128 64 8 8 (524288)
I0307 17:06:47.960811 28053 net.cpp:137] Memory required for data: 481821184
I0307 17:06:47.960819 28053 layer_factory.hpp:77] Creating layer BatchNorm15
I0307 17:06:47.960826 28053 net.cpp:84] Creating Layer BatchNorm15
I0307 17:06:47.960830 28053 net.cpp:406] BatchNorm15 <- Convolution15
I0307 17:06:47.960835 28053 net.cpp:367] BatchNorm15 -> Convolution15 (in-place)
I0307 17:06:47.961061 28053 net.cpp:122] Setting up BatchNorm15
I0307 17:06:47.961069 28053 net.cpp:129] Top shape: 128 64 8 8 (524288)
I0307 17:06:47.961073 28053 net.cpp:137] Memory required for data: 483918336
I0307 17:06:47.961081 28053 layer_factory.hpp:77] Creating layer Scale15
I0307 17:06:47.961087 28053 net.cpp:84] Creating Layer Scale15
I0307 17:06:47.961096 28053 net.cpp:406] Scale15 <- Convolution15
I0307 17:06:47.961100 28053 net.cpp:367] Scale15 -> Convolution15 (in-place)
I0307 17:06:47.961140 28053 layer_factory.hpp:77] Creating layer Scale15
I0307 17:06:47.961271 28053 net.cpp:122] Setting up Scale15
I0307 17:06:47.961278 28053 net.cpp:129] Top shape: 128 64 8 8 (524288)
I0307 17:06:47.961282 28053 net.cpp:137] Memory required for data: 486015488
I0307 17:06:47.961287 28053 layer_factory.hpp:77] Creating layer Convolution16
I0307 17:06:47.961298 28053 net.cpp:84] Creating Layer Convolution16
I0307 17:06:47.961303 28053 net.cpp:406] Convolution16 <- Eltwise6_ReLU13_0_split_1
I0307 17:06:47.961310 28053 net.cpp:380] Convolution16 -> Convolution16
I0307 17:06:47.966084 28053 net.cpp:122] Setting up Convolution16
I0307 17:06:47.966107 28053 net.cpp:129] Top shape: 128 64 8 8 (524288)
I0307 17:06:47.966111 28053 net.cpp:137] Memory required for data: 488112640
I0307 17:06:47.966120 28053 layer_factory.hpp:77] Creating layer BatchNorm16
I0307 17:06:47.966128 28053 net.cpp:84] Creating Layer BatchNorm16
I0307 17:06:47.966132 28053 net.cpp:406] BatchNorm16 <- Convolution16
I0307 17:06:47.966141 28053 net.cpp:367] BatchNorm16 -> Convolution16 (in-place)
I0307 17:06:47.966373 28053 net.cpp:122] Setting up BatchNorm16
I0307 17:06:47.966382 28053 net.cpp:129] Top shape: 128 64 8 8 (524288)
I0307 17:06:47.966385 28053 net.cpp:137] Memory required for data: 490209792
I0307 17:06:47.966408 28053 layer_factory.hpp:77] Creating layer Scale16
I0307 17:06:47.966415 28053 net.cpp:84] Creating Layer Scale16
I0307 17:06:47.966419 28053 net.cpp:406] Scale16 <- Convolution16
I0307 17:06:47.966424 28053 net.cpp:367] Scale16 -> Convolution16 (in-place)
I0307 17:06:47.966477 28053 layer_factory.hpp:77] Creating layer Scale16
I0307 17:06:47.966610 28053 net.cpp:122] Setting up Scale16
I0307 17:06:47.966619 28053 net.cpp:129] Top shape: 128 64 8 8 (524288)
I0307 17:06:47.966622 28053 net.cpp:137] Memory required for data: 492306944
I0307 17:06:47.966629 28053 layer_factory.hpp:77] Creating layer ReLU14
I0307 17:06:47.966637 28053 net.cpp:84] Creating Layer ReLU14
I0307 17:06:47.966642 28053 net.cpp:406] ReLU14 <- Convolution16
I0307 17:06:47.966647 28053 net.cpp:367] ReLU14 -> Convolution16 (in-place)
I0307 17:06:47.967279 28053 net.cpp:122] Setting up ReLU14
I0307 17:06:47.967293 28053 net.cpp:129] Top shape: 128 64 8 8 (524288)
I0307 17:06:47.967298 28053 net.cpp:137] Memory required for data: 494404096
I0307 17:06:47.967300 28053 layer_factory.hpp:77] Creating layer Convolution17
I0307 17:06:47.967314 28053 net.cpp:84] Creating Layer Convolution17
I0307 17:06:47.967319 28053 net.cpp:406] Convolution17 <- Convolution16
I0307 17:06:47.967326 28053 net.cpp:380] Convolution17 -> Convolution17
I0307 17:06:47.970706 28053 net.cpp:122] Setting up Convolution17
I0307 17:06:47.970723 28053 net.cpp:129] Top shape: 128 64 8 8 (524288)
I0307 17:06:47.970727 28053 net.cpp:137] Memory required for data: 496501248
I0307 17:06:47.970734 28053 layer_factory.hpp:77] Creating layer BatchNorm17
I0307 17:06:47.970742 28053 net.cpp:84] Creating Layer BatchNorm17
I0307 17:06:47.970746 28053 net.cpp:406] BatchNorm17 <- Convolution17
I0307 17:06:47.970752 28053 net.cpp:367] BatchNorm17 -> Convolution17 (in-place)
I0307 17:06:47.970978 28053 net.cpp:122] Setting up BatchNorm17
I0307 17:06:47.970986 28053 net.cpp:129] Top shape: 128 64 8 8 (524288)
I0307 17:06:47.970990 28053 net.cpp:137] Memory required for data: 498598400
I0307 17:06:47.970998 28053 layer_factory.hpp:77] Creating layer Scale17
I0307 17:06:47.971002 28053 net.cpp:84] Creating Layer Scale17
I0307 17:06:47.971006 28053 net.cpp:406] Scale17 <- Convolution17
I0307 17:06:47.971012 28053 net.cpp:367] Scale17 -> Convolution17 (in-place)
I0307 17:06:47.971053 28053 layer_factory.hpp:77] Creating layer Scale17
I0307 17:06:47.971187 28053 net.cpp:122] Setting up Scale17
I0307 17:06:47.971195 28053 net.cpp:129] Top shape: 128 64 8 8 (524288)
I0307 17:06:47.971199 28053 net.cpp:137] Memory required for data: 500695552
I0307 17:06:47.971204 28053 layer_factory.hpp:77] Creating layer Eltwise7
I0307 17:06:47.971211 28053 net.cpp:84] Creating Layer Eltwise7
I0307 17:06:47.971215 28053 net.cpp:406] Eltwise7 <- Convolution15
I0307 17:06:47.971220 28053 net.cpp:406] Eltwise7 <- Convolution17
I0307 17:06:47.971226 28053 net.cpp:380] Eltwise7 -> Eltwise7
I0307 17:06:47.971251 28053 net.cpp:122] Setting up Eltwise7
I0307 17:06:47.971259 28053 net.cpp:129] Top shape: 128 64 8 8 (524288)
I0307 17:06:47.971262 28053 net.cpp:137] Memory required for data: 502792704
I0307 17:06:47.971266 28053 layer_factory.hpp:77] Creating layer ReLU15
I0307 17:06:47.971271 28053 net.cpp:84] Creating Layer ReLU15
I0307 17:06:47.971276 28053 net.cpp:406] ReLU15 <- Eltwise7
I0307 17:06:47.971280 28053 net.cpp:367] ReLU15 -> Eltwise7 (in-place)
I0307 17:06:47.972354 28053 net.cpp:122] Setting up ReLU15
I0307 17:06:47.972370 28053 net.cpp:129] Top shape: 128 64 8 8 (524288)
I0307 17:06:47.972374 28053 net.cpp:137] Memory required for data: 504889856
I0307 17:06:47.972378 28053 layer_factory.hpp:77] Creating layer Eltwise7_ReLU15_0_split
I0307 17:06:47.972385 28053 net.cpp:84] Creating Layer Eltwise7_ReLU15_0_split
I0307 17:06:47.972389 28053 net.cpp:406] Eltwise7_ReLU15_0_split <- Eltwise7
I0307 17:06:47.972395 28053 net.cpp:380] Eltwise7_ReLU15_0_split -> Eltwise7_ReLU15_0_split_0
I0307 17:06:47.972404 28053 net.cpp:380] Eltwise7_ReLU15_0_split -> Eltwise7_ReLU15_0_split_1
I0307 17:06:47.972471 28053 net.cpp:122] Setting up Eltwise7_ReLU15_0_split
I0307 17:06:47.972481 28053 net.cpp:129] Top shape: 128 64 8 8 (524288)
I0307 17:06:47.972486 28053 net.cpp:129] Top shape: 128 64 8 8 (524288)
I0307 17:06:47.972488 28053 net.cpp:137] Memory required for data: 509084160
I0307 17:06:47.972491 28053 layer_factory.hpp:77] Creating layer Convolution18
I0307 17:06:47.972504 28053 net.cpp:84] Creating Layer Convolution18
I0307 17:06:47.972509 28053 net.cpp:406] Convolution18 <- Eltwise7_ReLU15_0_split_0
I0307 17:06:47.972517 28053 net.cpp:380] Convolution18 -> Convolution18
I0307 17:06:47.975845 28053 net.cpp:122] Setting up Convolution18
I0307 17:06:47.975862 28053 net.cpp:129] Top shape: 128 64 8 8 (524288)
I0307 17:06:47.975864 28053 net.cpp:137] Memory required for data: 511181312
I0307 17:06:47.975872 28053 layer_factory.hpp:77] Creating layer BatchNorm18
I0307 17:06:47.975880 28053 net.cpp:84] Creating Layer BatchNorm18
I0307 17:06:47.975884 28053 net.cpp:406] BatchNorm18 <- Convolution18
I0307 17:06:47.975890 28053 net.cpp:367] BatchNorm18 -> Convolution18 (in-place)
I0307 17:06:47.976125 28053 net.cpp:122] Setting up BatchNorm18
I0307 17:06:47.976133 28053 net.cpp:129] Top shape: 128 64 8 8 (524288)
I0307 17:06:47.976136 28053 net.cpp:137] Memory required for data: 513278464
I0307 17:06:47.976143 28053 layer_factory.hpp:77] Creating layer Scale18
I0307 17:06:47.976150 28053 net.cpp:84] Creating Layer Scale18
I0307 17:06:47.976153 28053 net.cpp:406] Scale18 <- Convolution18
I0307 17:06:47.976157 28053 net.cpp:367] Scale18 -> Convolution18 (in-place)
I0307 17:06:47.976198 28053 layer_factory.hpp:77] Creating layer Scale18
I0307 17:06:47.976331 28053 net.cpp:122] Setting up Scale18
I0307 17:06:47.976339 28053 net.cpp:129] Top shape: 128 64 8 8 (524288)
I0307 17:06:47.976342 28053 net.cpp:137] Memory required for data: 515375616
I0307 17:06:47.976348 28053 layer_factory.hpp:77] Creating layer ReLU16
I0307 17:06:47.976354 28053 net.cpp:84] Creating Layer ReLU16
I0307 17:06:47.976357 28053 net.cpp:406] ReLU16 <- Convolution18
I0307 17:06:47.976363 28053 net.cpp:367] ReLU16 -> Convolution18 (in-place)
I0307 17:06:47.977365 28053 net.cpp:122] Setting up ReLU16
I0307 17:06:47.977380 28053 net.cpp:129] Top shape: 128 64 8 8 (524288)
I0307 17:06:47.977383 28053 net.cpp:137] Memory required for data: 517472768
I0307 17:06:47.977387 28053 layer_factory.hpp:77] Creating layer Convolution19
I0307 17:06:47.977398 28053 net.cpp:84] Creating Layer Convolution19
I0307 17:06:47.977403 28053 net.cpp:406] Convolution19 <- Convolution18
I0307 17:06:47.977411 28053 net.cpp:380] Convolution19 -> Convolution19
I0307 17:06:47.980715 28053 net.cpp:122] Setting up Convolution19
I0307 17:06:47.980731 28053 net.cpp:129] Top shape: 128 64 8 8 (524288)
I0307 17:06:47.980734 28053 net.cpp:137] Memory required for data: 519569920
I0307 17:06:47.980741 28053 layer_factory.hpp:77] Creating layer BatchNorm19
I0307 17:06:47.980751 28053 net.cpp:84] Creating Layer BatchNorm19
I0307 17:06:47.980754 28053 net.cpp:406] BatchNorm19 <- Convolution19
I0307 17:06:47.980759 28053 net.cpp:367] BatchNorm19 -> Convolution19 (in-place)
I0307 17:06:47.981014 28053 net.cpp:122] Setting up BatchNorm19
I0307 17:06:47.981025 28053 net.cpp:129] Top shape: 128 64 8 8 (524288)
I0307 17:06:47.981029 28053 net.cpp:137] Memory required for data: 521667072
I0307 17:06:47.981051 28053 layer_factory.hpp:77] Creating layer Scale19
I0307 17:06:47.981060 28053 net.cpp:84] Creating Layer Scale19
I0307 17:06:47.981065 28053 net.cpp:406] Scale19 <- Convolution19
I0307 17:06:47.981070 28053 net.cpp:367] Scale19 -> Convolution19 (in-place)
I0307 17:06:47.981114 28053 layer_factory.hpp:77] Creating layer Scale19
I0307 17:06:47.981245 28053 net.cpp:122] Setting up Scale19
I0307 17:06:47.981254 28053 net.cpp:129] Top shape: 128 64 8 8 (524288)
I0307 17:06:47.981257 28053 net.cpp:137] Memory required for data: 523764224
I0307 17:06:47.981263 28053 layer_factory.hpp:77] Creating layer Eltwise8
I0307 17:06:47.981269 28053 net.cpp:84] Creating Layer Eltwise8
I0307 17:06:47.981287 28053 net.cpp:406] Eltwise8 <- Eltwise7_ReLU15_0_split_1
I0307 17:06:47.981292 28053 net.cpp:406] Eltwise8 <- Convolution19
I0307 17:06:47.981297 28053 net.cpp:380] Eltwise8 -> Eltwise8
I0307 17:06:47.981324 28053 net.cpp:122] Setting up Eltwise8
I0307 17:06:47.981333 28053 net.cpp:129] Top shape: 128 64 8 8 (524288)
I0307 17:06:47.981335 28053 net.cpp:137] Memory required for data: 525861376
I0307 17:06:47.981339 28053 layer_factory.hpp:77] Creating layer ReLU17
I0307 17:06:47.981344 28053 net.cpp:84] Creating Layer ReLU17
I0307 17:06:47.981348 28053 net.cpp:406] ReLU17 <- Eltwise8
I0307 17:06:47.981353 28053 net.cpp:367] ReLU17 -> Eltwise8 (in-place)
I0307 17:06:47.981958 28053 net.cpp:122] Setting up ReLU17
I0307 17:06:47.981971 28053 net.cpp:129] Top shape: 128 64 8 8 (524288)
I0307 17:06:47.981974 28053 net.cpp:137] Memory required for data: 527958528
I0307 17:06:47.981977 28053 layer_factory.hpp:77] Creating layer Eltwise8_ReLU17_0_split
I0307 17:06:47.981983 28053 net.cpp:84] Creating Layer Eltwise8_ReLU17_0_split
I0307 17:06:47.981987 28053 net.cpp:406] Eltwise8_ReLU17_0_split <- Eltwise8
I0307 17:06:47.981994 28053 net.cpp:380] Eltwise8_ReLU17_0_split -> Eltwise8_ReLU17_0_split_0
I0307 17:06:47.982002 28053 net.cpp:380] Eltwise8_ReLU17_0_split -> Eltwise8_ReLU17_0_split_1
I0307 17:06:47.982053 28053 net.cpp:122] Setting up Eltwise8_ReLU17_0_split
I0307 17:06:47.982060 28053 net.cpp:129] Top shape: 128 64 8 8 (524288)
I0307 17:06:47.982064 28053 net.cpp:129] Top shape: 128 64 8 8 (524288)
I0307 17:06:47.982067 28053 net.cpp:137] Memory required for data: 532152832
I0307 17:06:47.982070 28053 layer_factory.hpp:77] Creating layer Convolution20
I0307 17:06:47.982081 28053 net.cpp:84] Creating Layer Convolution20
I0307 17:06:47.982085 28053 net.cpp:406] Convolution20 <- Eltwise8_ReLU17_0_split_0
I0307 17:06:47.982093 28053 net.cpp:380] Convolution20 -> Convolution20
I0307 17:06:47.986279 28053 net.cpp:122] Setting up Convolution20
I0307 17:06:47.986299 28053 net.cpp:129] Top shape: 128 64 8 8 (524288)
I0307 17:06:47.986302 28053 net.cpp:137] Memory required for data: 534249984
I0307 17:06:47.986310 28053 layer_factory.hpp:77] Creating layer BatchNorm20
I0307 17:06:47.986320 28053 net.cpp:84] Creating Layer BatchNorm20
I0307 17:06:47.986323 28053 net.cpp:406] BatchNorm20 <- Convolution20
I0307 17:06:47.986328 28053 net.cpp:367] BatchNorm20 -> Convolution20 (in-place)
I0307 17:06:47.986567 28053 net.cpp:122] Setting up BatchNorm20
I0307 17:06:47.986575 28053 net.cpp:129] Top shape: 128 64 8 8 (524288)
I0307 17:06:47.986579 28053 net.cpp:137] Memory required for data: 536347136
I0307 17:06:47.986587 28053 layer_factory.hpp:77] Creating layer Scale20
I0307 17:06:47.986593 28053 net.cpp:84] Creating Layer Scale20
I0307 17:06:47.986595 28053 net.cpp:406] Scale20 <- Convolution20
I0307 17:06:47.986599 28053 net.cpp:367] Scale20 -> Convolution20 (in-place)
I0307 17:06:47.986644 28053 layer_factory.hpp:77] Creating layer Scale20
I0307 17:06:47.986779 28053 net.cpp:122] Setting up Scale20
I0307 17:06:47.986789 28053 net.cpp:129] Top shape: 128 64 8 8 (524288)
I0307 17:06:47.986793 28053 net.cpp:137] Memory required for data: 538444288
I0307 17:06:47.986799 28053 layer_factory.hpp:77] Creating layer ReLU18
I0307 17:06:47.986804 28053 net.cpp:84] Creating Layer ReLU18
I0307 17:06:47.986807 28053 net.cpp:406] ReLU18 <- Convolution20
I0307 17:06:47.986812 28053 net.cpp:367] ReLU18 -> Convolution20 (in-place)
I0307 17:06:47.987417 28053 net.cpp:122] Setting up ReLU18
I0307 17:06:47.987430 28053 net.cpp:129] Top shape: 128 64 8 8 (524288)
I0307 17:06:47.987435 28053 net.cpp:137] Memory required for data: 540541440
I0307 17:06:47.987438 28053 layer_factory.hpp:77] Creating layer Convolution21
I0307 17:06:47.987450 28053 net.cpp:84] Creating Layer Convolution21
I0307 17:06:47.987454 28053 net.cpp:406] Convolution21 <- Convolution20
I0307 17:06:47.987460 28053 net.cpp:380] Convolution21 -> Convolution21
I0307 17:06:47.990883 28053 net.cpp:122] Setting up Convolution21
I0307 17:06:47.990902 28053 net.cpp:129] Top shape: 128 64 8 8 (524288)
I0307 17:06:47.990921 28053 net.cpp:137] Memory required for data: 542638592
I0307 17:06:47.990928 28053 layer_factory.hpp:77] Creating layer BatchNorm21
I0307 17:06:47.990934 28053 net.cpp:84] Creating Layer BatchNorm21
I0307 17:06:47.990938 28053 net.cpp:406] BatchNorm21 <- Convolution21
I0307 17:06:47.990945 28053 net.cpp:367] BatchNorm21 -> Convolution21 (in-place)
I0307 17:06:47.991181 28053 net.cpp:122] Setting up BatchNorm21
I0307 17:06:47.991191 28053 net.cpp:129] Top shape: 128 64 8 8 (524288)
I0307 17:06:47.991194 28053 net.cpp:137] Memory required for data: 544735744
I0307 17:06:47.991201 28053 layer_factory.hpp:77] Creating layer Scale21
I0307 17:06:47.991206 28053 net.cpp:84] Creating Layer Scale21
I0307 17:06:47.991210 28053 net.cpp:406] Scale21 <- Convolution21
I0307 17:06:47.991216 28053 net.cpp:367] Scale21 -> Convolution21 (in-place)
I0307 17:06:47.991256 28053 layer_factory.hpp:77] Creating layer Scale21
I0307 17:06:47.991392 28053 net.cpp:122] Setting up Scale21
I0307 17:06:47.991402 28053 net.cpp:129] Top shape: 128 64 8 8 (524288)
I0307 17:06:47.991405 28053 net.cpp:137] Memory required for data: 546832896
I0307 17:06:47.991411 28053 layer_factory.hpp:77] Creating layer Eltwise9
I0307 17:06:47.991417 28053 net.cpp:84] Creating Layer Eltwise9
I0307 17:06:47.991422 28053 net.cpp:406] Eltwise9 <- Eltwise8_ReLU17_0_split_1
I0307 17:06:47.991426 28053 net.cpp:406] Eltwise9 <- Convolution21
I0307 17:06:47.991432 28053 net.cpp:380] Eltwise9 -> Eltwise9
I0307 17:06:47.991458 28053 net.cpp:122] Setting up Eltwise9
I0307 17:06:47.991468 28053 net.cpp:129] Top shape: 128 64 8 8 (524288)
I0307 17:06:47.991472 28053 net.cpp:137] Memory required for data: 548930048
I0307 17:06:47.991474 28053 layer_factory.hpp:77] Creating layer ReLU19
I0307 17:06:47.991480 28053 net.cpp:84] Creating Layer ReLU19
I0307 17:06:47.991483 28053 net.cpp:406] ReLU19 <- Eltwise9
I0307 17:06:47.991488 28053 net.cpp:367] ReLU19 -> Eltwise9 (in-place)
I0307 17:06:47.992483 28053 net.cpp:122] Setting up ReLU19
I0307 17:06:47.992501 28053 net.cpp:129] Top shape: 128 64 8 8 (524288)
I0307 17:06:47.992503 28053 net.cpp:137] Memory required for data: 551027200
I0307 17:06:47.992507 28053 layer_factory.hpp:77] Creating layer Pooling1
I0307 17:06:47.992514 28053 net.cpp:84] Creating Layer Pooling1
I0307 17:06:47.992518 28053 net.cpp:406] Pooling1 <- Eltwise9
I0307 17:06:47.992525 28053 net.cpp:380] Pooling1 -> Pooling1
I0307 17:06:47.993196 28053 net.cpp:122] Setting up Pooling1
I0307 17:06:47.993209 28053 net.cpp:129] Top shape: 128 64 1 1 (8192)
I0307 17:06:47.993212 28053 net.cpp:137] Memory required for data: 551059968
I0307 17:06:47.993216 28053 layer_factory.hpp:77] Creating layer InnerProduct1
I0307 17:06:47.993223 28053 net.cpp:84] Creating Layer InnerProduct1
I0307 17:06:47.993227 28053 net.cpp:406] InnerProduct1 <- Pooling1
I0307 17:06:47.993235 28053 net.cpp:380] InnerProduct1 -> InnerProduct1
I0307 17:06:47.993386 28053 net.cpp:122] Setting up InnerProduct1
I0307 17:06:47.993394 28053 net.cpp:129] Top shape: 128 10 (1280)
I0307 17:06:47.993397 28053 net.cpp:137] Memory required for data: 551065088
I0307 17:06:47.993403 28053 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I0307 17:06:47.993409 28053 net.cpp:84] Creating Layer SoftmaxWithLoss1
I0307 17:06:47.993413 28053 net.cpp:406] SoftmaxWithLoss1 <- InnerProduct1
I0307 17:06:47.993417 28053 net.cpp:406] SoftmaxWithLoss1 <- label
I0307 17:06:47.993425 28053 net.cpp:380] SoftmaxWithLoss1 -> SoftmaxWithLoss1
I0307 17:06:47.993434 28053 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I0307 17:06:47.994139 28053 net.cpp:122] Setting up SoftmaxWithLoss1
I0307 17:06:47.994150 28053 net.cpp:129] Top shape: (1)
I0307 17:06:47.994153 28053 net.cpp:132]     with loss weight 1
I0307 17:06:47.994168 28053 net.cpp:137] Memory required for data: 551065092
I0307 17:06:47.994172 28053 net.cpp:198] SoftmaxWithLoss1 needs backward computation.
I0307 17:06:47.994179 28053 net.cpp:198] InnerProduct1 needs backward computation.
I0307 17:06:47.994184 28053 net.cpp:198] Pooling1 needs backward computation.
I0307 17:06:47.994199 28053 net.cpp:198] ReLU19 needs backward computation.
I0307 17:06:47.994201 28053 net.cpp:198] Eltwise9 needs backward computation.
I0307 17:06:47.994205 28053 net.cpp:198] Scale21 needs backward computation.
I0307 17:06:47.994208 28053 net.cpp:198] BatchNorm21 needs backward computation.
I0307 17:06:47.994211 28053 net.cpp:198] Convolution21 needs backward computation.
I0307 17:06:47.994215 28053 net.cpp:198] ReLU18 needs backward computation.
I0307 17:06:47.994218 28053 net.cpp:198] Scale20 needs backward computation.
I0307 17:06:47.994221 28053 net.cpp:198] BatchNorm20 needs backward computation.
I0307 17:06:47.994225 28053 net.cpp:198] Convolution20 needs backward computation.
I0307 17:06:47.994228 28053 net.cpp:198] Eltwise8_ReLU17_0_split needs backward computation.
I0307 17:06:47.994232 28053 net.cpp:198] ReLU17 needs backward computation.
I0307 17:06:47.994235 28053 net.cpp:198] Eltwise8 needs backward computation.
I0307 17:06:47.994240 28053 net.cpp:198] Scale19 needs backward computation.
I0307 17:06:47.994243 28053 net.cpp:198] BatchNorm19 needs backward computation.
I0307 17:06:47.994246 28053 net.cpp:198] Convolution19 needs backward computation.
I0307 17:06:47.994251 28053 net.cpp:198] ReLU16 needs backward computation.
I0307 17:06:47.994253 28053 net.cpp:198] Scale18 needs backward computation.
I0307 17:06:47.994256 28053 net.cpp:198] BatchNorm18 needs backward computation.
I0307 17:06:47.994259 28053 net.cpp:198] Convolution18 needs backward computation.
I0307 17:06:47.994263 28053 net.cpp:198] Eltwise7_ReLU15_0_split needs backward computation.
I0307 17:06:47.994267 28053 net.cpp:198] ReLU15 needs backward computation.
I0307 17:06:47.994271 28053 net.cpp:198] Eltwise7 needs backward computation.
I0307 17:06:47.994274 28053 net.cpp:198] Scale17 needs backward computation.
I0307 17:06:47.994278 28053 net.cpp:198] BatchNorm17 needs backward computation.
I0307 17:06:47.994282 28053 net.cpp:198] Convolution17 needs backward computation.
I0307 17:06:47.994285 28053 net.cpp:198] ReLU14 needs backward computation.
I0307 17:06:47.994289 28053 net.cpp:198] Scale16 needs backward computation.
I0307 17:06:47.994292 28053 net.cpp:198] BatchNorm16 needs backward computation.
I0307 17:06:47.994295 28053 net.cpp:198] Convolution16 needs backward computation.
I0307 17:06:47.994299 28053 net.cpp:198] Scale15 needs backward computation.
I0307 17:06:47.994303 28053 net.cpp:198] BatchNorm15 needs backward computation.
I0307 17:06:47.994307 28053 net.cpp:198] Convolution15 needs backward computation.
I0307 17:06:47.994313 28053 net.cpp:198] Eltwise6_ReLU13_0_split needs backward computation.
I0307 17:06:47.994315 28053 net.cpp:198] ReLU13 needs backward computation.
I0307 17:06:47.994319 28053 net.cpp:198] Eltwise6 needs backward computation.
I0307 17:06:47.994323 28053 net.cpp:198] Scale14 needs backward computation.
I0307 17:06:47.994328 28053 net.cpp:198] BatchNorm14 needs backward computation.
I0307 17:06:47.994330 28053 net.cpp:198] Convolution14 needs backward computation.
I0307 17:06:47.994334 28053 net.cpp:198] ReLU12 needs backward computation.
I0307 17:06:47.994338 28053 net.cpp:198] Scale13 needs backward computation.
I0307 17:06:47.994340 28053 net.cpp:198] BatchNorm13 needs backward computation.
I0307 17:06:47.994343 28053 net.cpp:198] Convolution13 needs backward computation.
I0307 17:06:47.994347 28053 net.cpp:198] Eltwise5_ReLU11_0_split needs backward computation.
I0307 17:06:47.994351 28053 net.cpp:198] ReLU11 needs backward computation.
I0307 17:06:47.994354 28053 net.cpp:198] Eltwise5 needs backward computation.
I0307 17:06:47.994359 28053 net.cpp:198] Scale12 needs backward computation.
I0307 17:06:47.994362 28053 net.cpp:198] BatchNorm12 needs backward computation.
I0307 17:06:47.994366 28053 net.cpp:198] Convolution12 needs backward computation.
I0307 17:06:47.994369 28053 net.cpp:198] ReLU10 needs backward computation.
I0307 17:06:47.994374 28053 net.cpp:198] Scale11 needs backward computation.
I0307 17:06:47.994376 28053 net.cpp:198] BatchNorm11 needs backward computation.
I0307 17:06:47.994385 28053 net.cpp:198] Convolution11 needs backward computation.
I0307 17:06:47.994388 28053 net.cpp:198] Eltwise4_ReLU9_0_split needs backward computation.
I0307 17:06:47.994392 28053 net.cpp:198] ReLU9 needs backward computation.
I0307 17:06:47.994395 28053 net.cpp:198] Eltwise4 needs backward computation.
I0307 17:06:47.994400 28053 net.cpp:198] Scale10 needs backward computation.
I0307 17:06:47.994402 28053 net.cpp:198] BatchNorm10 needs backward computation.
I0307 17:06:47.994406 28053 net.cpp:198] Convolution10 needs backward computation.
I0307 17:06:47.994410 28053 net.cpp:198] ReLU8 needs backward computation.
I0307 17:06:47.994413 28053 net.cpp:198] Scale9 needs backward computation.
I0307 17:06:47.994417 28053 net.cpp:198] BatchNorm9 needs backward computation.
I0307 17:06:47.994421 28053 net.cpp:198] Convolution9 needs backward computation.
I0307 17:06:47.994424 28053 net.cpp:198] Scale8 needs backward computation.
I0307 17:06:47.994428 28053 net.cpp:198] BatchNorm8 needs backward computation.
I0307 17:06:47.994431 28053 net.cpp:198] Convolution8 needs backward computation.
I0307 17:06:47.994436 28053 net.cpp:198] Eltwise3_ReLU7_0_split needs backward computation.
I0307 17:06:47.994438 28053 net.cpp:198] ReLU7 needs backward computation.
I0307 17:06:47.994442 28053 net.cpp:198] Eltwise3 needs backward computation.
I0307 17:06:47.994446 28053 net.cpp:198] Scale7 needs backward computation.
I0307 17:06:47.994449 28053 net.cpp:198] BatchNorm7 needs backward computation.
I0307 17:06:47.994453 28053 net.cpp:198] Convolution7 needs backward computation.
I0307 17:06:47.994457 28053 net.cpp:198] ReLU6 needs backward computation.
I0307 17:06:47.994460 28053 net.cpp:198] Scale6 needs backward computation.
I0307 17:06:47.994463 28053 net.cpp:198] BatchNorm6 needs backward computation.
I0307 17:06:47.994467 28053 net.cpp:198] Convolution6 needs backward computation.
I0307 17:06:47.994470 28053 net.cpp:198] Eltwise2_ReLU5_0_split needs backward computation.
I0307 17:06:47.994474 28053 net.cpp:198] ReLU5 needs backward computation.
I0307 17:06:47.994477 28053 net.cpp:198] Eltwise2 needs backward computation.
I0307 17:06:47.994483 28053 net.cpp:198] Scale5 needs backward computation.
I0307 17:06:47.994488 28053 net.cpp:198] BatchNorm5 needs backward computation.
I0307 17:06:47.994490 28053 net.cpp:198] Convolution5 needs backward computation.
I0307 17:06:47.994493 28053 net.cpp:198] ReLU4 needs backward computation.
I0307 17:06:47.994498 28053 net.cpp:198] Scale4 needs backward computation.
I0307 17:06:47.994500 28053 net.cpp:198] BatchNorm4 needs backward computation.
I0307 17:06:47.994503 28053 net.cpp:198] Convolution4 needs backward computation.
I0307 17:06:47.994508 28053 net.cpp:198] Eltwise1_ReLU3_0_split needs backward computation.
I0307 17:06:47.994511 28053 net.cpp:198] ReLU3 needs backward computation.
I0307 17:06:47.994514 28053 net.cpp:198] Eltwise1 needs backward computation.
I0307 17:06:47.994518 28053 net.cpp:198] Scale3 needs backward computation.
I0307 17:06:47.994523 28053 net.cpp:198] BatchNorm3 needs backward computation.
I0307 17:06:47.994534 28053 net.cpp:198] Convolution3 needs backward computation.
I0307 17:06:47.994536 28053 net.cpp:198] ReLU2 needs backward computation.
I0307 17:06:47.994540 28053 net.cpp:198] Scale2 needs backward computation.
I0307 17:06:47.994544 28053 net.cpp:198] BatchNorm2 needs backward computation.
I0307 17:06:47.994546 28053 net.cpp:198] Convolution2 needs backward computation.
I0307 17:06:47.994550 28053 net.cpp:198] Convolution1_ReLU1_0_split needs backward computation.
I0307 17:06:47.994555 28053 net.cpp:198] ReLU1 needs backward computation.
I0307 17:06:47.994558 28053 net.cpp:198] Scale1 needs backward computation.
I0307 17:06:47.994561 28053 net.cpp:198] BatchNorm1 needs backward computation.
I0307 17:06:47.994565 28053 net.cpp:198] Convolution1 needs backward computation.
I0307 17:06:47.994570 28053 net.cpp:200] cifar does not need backward computation.
I0307 17:06:47.994573 28053 net.cpp:242] This network produces output SoftmaxWithLoss1
I0307 17:06:47.994638 28053 net.cpp:255] Network initialization done.
I0307 17:06:47.995407 28053 upgrade_proto.cpp:79] Attempting to upgrade batch norm layers using deprecated params: examples/cifar10/cifar10_resnet20_train_test.prototxt
I0307 17:06:47.995417 28053 upgrade_proto.cpp:82] Successfully upgraded batch norm layers using deprecated params.
I0307 17:06:47.995424 28053 solver.cpp:190] Creating test net (#0) specified by net file: examples/cifar10/cifar10_resnet20_train_test.prototxt
I0307 17:06:47.995498 28053 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar
I0307 17:06:47.995991 28053 net.cpp:51] Initializing net from parameters: 
name: "cifar10_resnet20"
state {
  phase: TEST
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mean_file: "examples/cifar10/mean.binaryproto"
  }
  data_param {
    source: "examples/cifar10/cifar10_train_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "data"
  top: "Convolution1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm1"
  type: "BatchNorm"
  bottom: "Convolution1"
  top: "Convolution1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale1"
  type: "Scale"
  bottom: "Convolution1"
  top: "Convolution1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU1"
  type: "ReLU"
  bottom: "Convolution1"
  top: "Convolution1"
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "Convolution1"
  top: "Convolution2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm2"
  type: "BatchNorm"
  bottom: "Convolution2"
  top: "Convolution2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale2"
  type: "Scale"
  bottom: "Convolution2"
  top: "Convolution2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU2"
  type: "ReLU"
  bottom: "Convolution2"
  top: "Convolution2"
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "Convolution2"
  top: "Convolution3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm3"
  type: "BatchNorm"
  bottom: "Convolution3"
  top: "Convolution3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale3"
  type: "Scale"
  bottom: "Convolution3"
  top: "Convolution3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise1"
  type: "Eltwise"
  bottom: "Convolution1"
  bottom: "Convolution3"
  top: "Eltwise1"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU3"
  type: "ReLU"
  bottom: "Eltwise1"
  top: "Eltwise1"
}
layer {
  name: "Convolution4"
  type: "Convolution"
  bottom: "Eltwise1"
  top: "Convolution4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm4"
  type: "BatchNorm"
  bottom: "Convolution4"
  top: "Convolution4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale4"
  type: "Scale"
  bottom: "Convolution4"
  top: "Convolution4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU4"
  type: "ReLU"
  bottom: "Convolution4"
  top: "Convolution4"
}
layer {
  name: "Convolution5"
  type: "Convolution"
  bottom: "Convolution4"
  top: "Convolution5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm5"
  type: "BatchNorm"
  bottom: "Convolution5"
  top: "Convolution5"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale5"
  type: "Scale"
  bottom: "Convolution5"
  top: "Convolution5"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise2"
  type: "Eltwise"
  bottom: "Eltwise1"
  bottom: "Convolution5"
  top: "Eltwise2"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU5"
  type: "ReLU"
  bottom: "Eltwise2"
  top: "Eltwise2"
}
layer {
  name: "Convolution6"
  type: "Convolution"
  bottom: "Eltwise2"
  top: "Convolution6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm6"
  type: "BatchNorm"
  bottom: "Convolution6"
  top: "Convolution6"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale6"
  type: "Scale"
  bottom: "Convolution6"
  top: "Convolution6"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU6"
  type: "ReLU"
  bottom: "Convolution6"
  top: "Convolution6"
}
layer {
  name: "Convolution7"
  type: "Convolution"
  bottom: "Convolution6"
  top: "Convolution7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm7"
  type: "BatchNorm"
  bottom: "Convolution7"
  top: "Convolution7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale7"
  type: "Scale"
  bottom: "Convolution7"
  top: "Convolution7"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise3"
  type: "Eltwise"
  bottom: "Eltwise2"
  bottom: "Convolution7"
  top: "Eltwise3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU7"
  type: "ReLU"
  bottom: "Eltwise3"
  top: "Eltwise3"
}
layer {
  name: "Convolution8"
  type: "Convolution"
  bottom: "Eltwise3"
  top: "Convolution8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm8"
  type: "BatchNorm"
  bottom: "Convolution8"
  top: "Convolution8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale8"
  type: "Scale"
  bottom: "Convolution8"
  top: "Convolution8"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Convolution9"
  type: "Convolution"
  bottom: "Eltwise3"
  top: "Convolution9"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm9"
  type: "BatchNorm"
  bottom: "Convolution9"
  top: "Convolution9"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale9"
  type: "Scale"
  bottom: "Convolution9"
  top: "Convolution9"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU8"
  type: "ReLU"
  bottom: "Convolution9"
  top: "Convolution9"
}
layer {
  name: "Convolution10"
  type: "Convolution"
  bottom: "Convolution9"
  top: "Convolution10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm10"
  type: "BatchNorm"
  bottom: "Convolution10"
  top: "Convolution10"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale10"
  type: "Scale"
  bottom: "Convolution10"
  top: "Convolution10"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise4"
  type: "Eltwise"
  bottom: "Convolution8"
  bottom: "Convolution10"
  top: "Eltwise4"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU9"
  type: "ReLU"
  bottom: "Eltwise4"
  top: "Eltwise4"
}
layer {
  name: "Convolution11"
  type: "Convolution"
  bottom: "Eltwise4"
  top: "Convolution11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm11"
  type: "BatchNorm"
  bottom: "Convolution11"
  top: "Convolution11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale11"
  type: "Scale"
  bottom: "Convolution11"
  top: "Convolution11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU10"
  type: "ReLU"
  bottom: "Convolution11"
  top: "Convolution11"
}
layer {
  name: "Convolution12"
  type: "Convolution"
  bottom: "Convolution11"
  top: "Convolution12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm12"
  type: "BatchNorm"
  bottom: "Convolution12"
  top: "Convolution12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale12"
  type: "Scale"
  bottom: "Convolution12"
  top: "Convolution12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise5"
  type: "Eltwise"
  bottom: "Eltwise4"
  bottom: "Convolution12"
  top: "Eltwise5"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU11"
  type: "ReLU"
  bottom: "Eltwise5"
  top: "Eltwise5"
}
layer {
  name: "Convolution13"
  type: "Convolution"
  bottom: "Eltwise5"
  top: "Convolution13"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm13"
  type: "BatchNorm"
  bottom: "Convolution13"
  top: "Convolution13"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale13"
  type: "Scale"
  bottom: "Convolution13"
  top: "Convolution13"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU12"
  type: "ReLU"
  bottom: "Convolution13"
  top: "Convolution13"
}
layer {
  name: "Convolution14"
  type: "Convolution"
  bottom: "Convolution13"
  top: "Convolution14"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm14"
  type: "BatchNorm"
  bottom: "Convolution14"
  top: "Convolution14"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale14"
  type: "Scale"
  bottom: "Convolution14"
  top: "Convolution14"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise6"
  type: "Eltwise"
  bottom: "Eltwise5"
  bottom: "Convolution14"
  top: "Eltwise6"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU13"
  type: "ReLU"
  bottom: "Eltwise6"
  top: "Eltwise6"
}
layer {
  name: "Convolution15"
  type: "Convolution"
  bottom: "Eltwise6"
  top: "Convolution15"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm15"
  type: "BatchNorm"
  bottom: "Convolution15"
  top: "Convolution15"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale15"
  type: "Scale"
  bottom: "Convolution15"
  top: "Convolution15"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Convolution16"
  type: "Convolution"
  bottom: "Eltwise6"
  top: "Convolution16"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm16"
  type: "BatchNorm"
  bottom: "Convolution16"
  top: "Convolution16"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale16"
  type: "Scale"
  bottom: "Convolution16"
  top: "Convolution16"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU14"
  type: "ReLU"
  bottom: "Convolution16"
  top: "Convolution16"
}
layer {
  name: "Convolution17"
  type: "Convolution"
  bottom: "Convolution16"
  top: "Convolution17"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm17"
  type: "BatchNorm"
  bottom: "Convolution17"
  top: "Convolution17"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale17"
  type: "Scale"
  bottom: "Convolution17"
  top: "Convolution17"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise7"
  type: "Eltwise"
  bottom: "Convolution15"
  bottom: "Convolution17"
  top: "Eltwise7"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU15"
  type: "ReLU"
  bottom: "Eltwise7"
  top: "Eltwise7"
}
layer {
  name: "Convolution18"
  type: "Convolution"
  bottom: "Eltwise7"
  top: "Convolution18"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm18"
  type: "BatchNorm"
  bottom: "Convolution18"
  top: "Convolution18"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale18"
  type: "Scale"
  bottom: "Convolution18"
  top: "Convolution18"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU16"
  type: "ReLU"
  bottom: "Convolution18"
  top: "Convolution18"
}
layer {
  name: "Convolution19"
  type: "Convolution"
  bottom: "Convolution18"
  top: "Convolution19"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm19"
  type: "BatchNorm"
  bottom: "Convolution19"
  top: "Convolution19"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale19"
  type: "Scale"
  bottom: "Convolution19"
  top: "Convolution19"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise8"
  type: "Eltwise"
  bottom: "Eltwise7"
  bottom: "Convolution19"
  top: "Eltwise8"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU17"
  type: "ReLU"
  bottom: "Eltwise8"
  top: "Eltwise8"
}
layer {
  name: "Convolution20"
  type: "Convolution"
  bottom: "Eltwise8"
  top: "Convolution20"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm20"
  type: "BatchNorm"
  bottom: "Convolution20"
  top: "Convolution20"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale20"
  type: "Scale"
  bottom: "Convolution20"
  top: "Convolution20"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU18"
  type: "ReLU"
  bottom: "Convolution20"
  top: "Convolution20"
}
layer {
  name: "Convolution21"
  type: "Convolution"
  bottom: "Convolution20"
  top: "Convolution21"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm21"
  type: "BatchNorm"
  bottom: "Convolution21"
  top: "Convolution21"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale21"
  type: "Scale"
  bottom: "Convolution21"
  top: "Convolution21"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise9"
  type: "Eltwise"
  bottom: "Eltwise8"
  bottom: "Convolution21"
  top: "Eltwise9"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU19"
  type: "ReLU"
  bottom: "Eltwise9"
  top: "Eltwise9"
}
layer {
  name: "Pooling1"
  type: "Pooling"
  bottom: "Eltwise9"
  top: "Pooling1"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "InnerProduct1"
  type: "InnerProduct"
  bottom: "Pooling1"
  top: "InnerProduct1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 1
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "SoftmaxWithLoss1"
  type: "SoftmaxWithLoss"
  bottom: "InnerProduct1"
  bottom: "label"
  top: "SoftmaxWithLoss1"
}
layer {
  name: "Accuracy1"
  type: "Accuracy"
  bottom: "InnerProduct1"
  bottom: "label"
  top: "Accuracy1"
  include {
    phase: TEST
  }
}
I0307 17:06:47.996302 28053 layer_factory.hpp:77] Creating layer cifar
I0307 17:06:47.996362 28053 db_lmdb.cpp:35] Opened lmdb examples/cifar10/cifar10_train_lmdb
I0307 17:06:47.996381 28053 net.cpp:84] Creating Layer cifar
I0307 17:06:47.996389 28053 net.cpp:380] cifar -> data
I0307 17:06:47.996398 28053 net.cpp:380] cifar -> label
I0307 17:06:47.996405 28053 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto
I0307 17:06:47.996587 28053 data_layer.cpp:45] output data size: 100,3,32,32
I0307 17:06:48.003432 28053 net.cpp:122] Setting up cifar
I0307 17:06:48.003458 28053 net.cpp:129] Top shape: 100 3 32 32 (307200)
I0307 17:06:48.003463 28053 net.cpp:129] Top shape: 100 (100)
I0307 17:06:48.003465 28053 net.cpp:137] Memory required for data: 1229200
I0307 17:06:48.003471 28053 layer_factory.hpp:77] Creating layer label_cifar_1_split
I0307 17:06:48.003484 28053 net.cpp:84] Creating Layer label_cifar_1_split
I0307 17:06:48.003487 28053 net.cpp:406] label_cifar_1_split <- label
I0307 17:06:48.003496 28053 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I0307 17:06:48.003506 28053 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I0307 17:06:48.003571 28053 net.cpp:122] Setting up label_cifar_1_split
I0307 17:06:48.003581 28053 net.cpp:129] Top shape: 100 (100)
I0307 17:06:48.003583 28053 net.cpp:129] Top shape: 100 (100)
I0307 17:06:48.003587 28053 net.cpp:137] Memory required for data: 1230000
I0307 17:06:48.003590 28053 layer_factory.hpp:77] Creating layer Convolution1
I0307 17:06:48.003603 28053 net.cpp:84] Creating Layer Convolution1
I0307 17:06:48.003607 28053 net.cpp:406] Convolution1 <- data
I0307 17:06:48.003615 28053 net.cpp:380] Convolution1 -> Convolution1
I0307 17:06:48.007203 28053 net.cpp:122] Setting up Convolution1
I0307 17:06:48.007223 28053 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0307 17:06:48.007227 28053 net.cpp:137] Memory required for data: 7783600
I0307 17:06:48.007239 28053 layer_factory.hpp:77] Creating layer BatchNorm1
I0307 17:06:48.007247 28053 net.cpp:84] Creating Layer BatchNorm1
I0307 17:06:48.007251 28053 net.cpp:406] BatchNorm1 <- Convolution1
I0307 17:06:48.007259 28053 net.cpp:367] BatchNorm1 -> Convolution1 (in-place)
I0307 17:06:48.007508 28053 net.cpp:122] Setting up BatchNorm1
I0307 17:06:48.007516 28053 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0307 17:06:48.007519 28053 net.cpp:137] Memory required for data: 14337200
I0307 17:06:48.007529 28053 layer_factory.hpp:77] Creating layer Scale1
I0307 17:06:48.007539 28053 net.cpp:84] Creating Layer Scale1
I0307 17:06:48.007542 28053 net.cpp:406] Scale1 <- Convolution1
I0307 17:06:48.007550 28053 net.cpp:367] Scale1 -> Convolution1 (in-place)
I0307 17:06:48.007601 28053 layer_factory.hpp:77] Creating layer Scale1
I0307 17:06:48.007742 28053 net.cpp:122] Setting up Scale1
I0307 17:06:48.007752 28053 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0307 17:06:48.007755 28053 net.cpp:137] Memory required for data: 20890800
I0307 17:06:48.007761 28053 layer_factory.hpp:77] Creating layer ReLU1
I0307 17:06:48.007767 28053 net.cpp:84] Creating Layer ReLU1
I0307 17:06:48.007772 28053 net.cpp:406] ReLU1 <- Convolution1
I0307 17:06:48.007778 28053 net.cpp:367] ReLU1 -> Convolution1 (in-place)
I0307 17:06:48.008392 28053 net.cpp:122] Setting up ReLU1
I0307 17:06:48.008405 28053 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0307 17:06:48.008409 28053 net.cpp:137] Memory required for data: 27444400
I0307 17:06:48.008412 28053 layer_factory.hpp:77] Creating layer Convolution1_ReLU1_0_split
I0307 17:06:48.008419 28053 net.cpp:84] Creating Layer Convolution1_ReLU1_0_split
I0307 17:06:48.008422 28053 net.cpp:406] Convolution1_ReLU1_0_split <- Convolution1
I0307 17:06:48.008445 28053 net.cpp:380] Convolution1_ReLU1_0_split -> Convolution1_ReLU1_0_split_0
I0307 17:06:48.008453 28053 net.cpp:380] Convolution1_ReLU1_0_split -> Convolution1_ReLU1_0_split_1
I0307 17:06:48.008509 28053 net.cpp:122] Setting up Convolution1_ReLU1_0_split
I0307 17:06:48.008517 28053 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0307 17:06:48.008522 28053 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0307 17:06:48.008524 28053 net.cpp:137] Memory required for data: 40551600
I0307 17:06:48.008528 28053 layer_factory.hpp:77] Creating layer Convolution2
I0307 17:06:48.008540 28053 net.cpp:84] Creating Layer Convolution2
I0307 17:06:48.008555 28053 net.cpp:406] Convolution2 <- Convolution1_ReLU1_0_split_0
I0307 17:06:48.008563 28053 net.cpp:380] Convolution2 -> Convolution2
I0307 17:06:48.011534 28053 net.cpp:122] Setting up Convolution2
I0307 17:06:48.011554 28053 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0307 17:06:48.011559 28053 net.cpp:137] Memory required for data: 47105200
I0307 17:06:48.011569 28053 layer_factory.hpp:77] Creating layer BatchNorm2
I0307 17:06:48.011580 28053 net.cpp:84] Creating Layer BatchNorm2
I0307 17:06:48.011584 28053 net.cpp:406] BatchNorm2 <- Convolution2
I0307 17:06:48.011593 28053 net.cpp:367] BatchNorm2 -> Convolution2 (in-place)
I0307 17:06:48.011849 28053 net.cpp:122] Setting up BatchNorm2
I0307 17:06:48.011858 28053 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0307 17:06:48.011862 28053 net.cpp:137] Memory required for data: 53658800
I0307 17:06:48.011869 28053 layer_factory.hpp:77] Creating layer Scale2
I0307 17:06:48.011878 28053 net.cpp:84] Creating Layer Scale2
I0307 17:06:48.011881 28053 net.cpp:406] Scale2 <- Convolution2
I0307 17:06:48.011886 28053 net.cpp:367] Scale2 -> Convolution2 (in-place)
I0307 17:06:48.011936 28053 layer_factory.hpp:77] Creating layer Scale2
I0307 17:06:48.012078 28053 net.cpp:122] Setting up Scale2
I0307 17:06:48.012086 28053 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0307 17:06:48.012089 28053 net.cpp:137] Memory required for data: 60212400
I0307 17:06:48.012095 28053 layer_factory.hpp:77] Creating layer ReLU2
I0307 17:06:48.012100 28053 net.cpp:84] Creating Layer ReLU2
I0307 17:06:48.012105 28053 net.cpp:406] ReLU2 <- Convolution2
I0307 17:06:48.012111 28053 net.cpp:367] ReLU2 -> Convolution2 (in-place)
I0307 17:06:48.013221 28053 net.cpp:122] Setting up ReLU2
I0307 17:06:48.013236 28053 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0307 17:06:48.013240 28053 net.cpp:137] Memory required for data: 66766000
I0307 17:06:48.013244 28053 layer_factory.hpp:77] Creating layer Convolution3
I0307 17:06:48.013257 28053 net.cpp:84] Creating Layer Convolution3
I0307 17:06:48.013262 28053 net.cpp:406] Convolution3 <- Convolution2
I0307 17:06:48.013269 28053 net.cpp:380] Convolution3 -> Convolution3
I0307 17:06:48.016541 28053 net.cpp:122] Setting up Convolution3
I0307 17:06:48.016561 28053 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0307 17:06:48.016566 28053 net.cpp:137] Memory required for data: 73319600
I0307 17:06:48.016573 28053 layer_factory.hpp:77] Creating layer BatchNorm3
I0307 17:06:48.016582 28053 net.cpp:84] Creating Layer BatchNorm3
I0307 17:06:48.016585 28053 net.cpp:406] BatchNorm3 <- Convolution3
I0307 17:06:48.016593 28053 net.cpp:367] BatchNorm3 -> Convolution3 (in-place)
I0307 17:06:48.016868 28053 net.cpp:122] Setting up BatchNorm3
I0307 17:06:48.016878 28053 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0307 17:06:48.016881 28053 net.cpp:137] Memory required for data: 79873200
I0307 17:06:48.016893 28053 layer_factory.hpp:77] Creating layer Scale3
I0307 17:06:48.016902 28053 net.cpp:84] Creating Layer Scale3
I0307 17:06:48.016906 28053 net.cpp:406] Scale3 <- Convolution3
I0307 17:06:48.016911 28053 net.cpp:367] Scale3 -> Convolution3 (in-place)
I0307 17:06:48.016963 28053 layer_factory.hpp:77] Creating layer Scale3
I0307 17:06:48.017104 28053 net.cpp:122] Setting up Scale3
I0307 17:06:48.017113 28053 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0307 17:06:48.017133 28053 net.cpp:137] Memory required for data: 86426800
I0307 17:06:48.017139 28053 layer_factory.hpp:77] Creating layer Eltwise1
I0307 17:06:48.017148 28053 net.cpp:84] Creating Layer Eltwise1
I0307 17:06:48.017153 28053 net.cpp:406] Eltwise1 <- Convolution1_ReLU1_0_split_1
I0307 17:06:48.017156 28053 net.cpp:406] Eltwise1 <- Convolution3
I0307 17:06:48.017163 28053 net.cpp:380] Eltwise1 -> Eltwise1
I0307 17:06:48.017204 28053 net.cpp:122] Setting up Eltwise1
I0307 17:06:48.017217 28053 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0307 17:06:48.017223 28053 net.cpp:137] Memory required for data: 92980400
I0307 17:06:48.017228 28053 layer_factory.hpp:77] Creating layer ReLU3
I0307 17:06:48.017237 28053 net.cpp:84] Creating Layer ReLU3
I0307 17:06:48.017242 28053 net.cpp:406] ReLU3 <- Eltwise1
I0307 17:06:48.017251 28053 net.cpp:367] ReLU3 -> Eltwise1 (in-place)
I0307 17:06:48.018322 28053 net.cpp:122] Setting up ReLU3
I0307 17:06:48.018338 28053 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0307 17:06:48.018342 28053 net.cpp:137] Memory required for data: 99534000
I0307 17:06:48.018345 28053 layer_factory.hpp:77] Creating layer Eltwise1_ReLU3_0_split
I0307 17:06:48.018357 28053 net.cpp:84] Creating Layer Eltwise1_ReLU3_0_split
I0307 17:06:48.018362 28053 net.cpp:406] Eltwise1_ReLU3_0_split <- Eltwise1
I0307 17:06:48.018368 28053 net.cpp:380] Eltwise1_ReLU3_0_split -> Eltwise1_ReLU3_0_split_0
I0307 17:06:48.018376 28053 net.cpp:380] Eltwise1_ReLU3_0_split -> Eltwise1_ReLU3_0_split_1
I0307 17:06:48.018446 28053 net.cpp:122] Setting up Eltwise1_ReLU3_0_split
I0307 17:06:48.018457 28053 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0307 17:06:48.018461 28053 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0307 17:06:48.018465 28053 net.cpp:137] Memory required for data: 112641200
I0307 17:06:48.018467 28053 layer_factory.hpp:77] Creating layer Convolution4
I0307 17:06:48.018481 28053 net.cpp:84] Creating Layer Convolution4
I0307 17:06:48.018486 28053 net.cpp:406] Convolution4 <- Eltwise1_ReLU3_0_split_0
I0307 17:06:48.018496 28053 net.cpp:380] Convolution4 -> Convolution4
I0307 17:06:48.021353 28053 net.cpp:122] Setting up Convolution4
I0307 17:06:48.021370 28053 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0307 17:06:48.021374 28053 net.cpp:137] Memory required for data: 119194800
I0307 17:06:48.021381 28053 layer_factory.hpp:77] Creating layer BatchNorm4
I0307 17:06:48.021391 28053 net.cpp:84] Creating Layer BatchNorm4
I0307 17:06:48.021397 28053 net.cpp:406] BatchNorm4 <- Convolution4
I0307 17:06:48.021404 28053 net.cpp:367] BatchNorm4 -> Convolution4 (in-place)
I0307 17:06:48.021651 28053 net.cpp:122] Setting up BatchNorm4
I0307 17:06:48.021659 28053 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0307 17:06:48.021663 28053 net.cpp:137] Memory required for data: 125748400
I0307 17:06:48.021670 28053 layer_factory.hpp:77] Creating layer Scale4
I0307 17:06:48.021677 28053 net.cpp:84] Creating Layer Scale4
I0307 17:06:48.021680 28053 net.cpp:406] Scale4 <- Convolution4
I0307 17:06:48.021687 28053 net.cpp:367] Scale4 -> Convolution4 (in-place)
I0307 17:06:48.021735 28053 layer_factory.hpp:77] Creating layer Scale4
I0307 17:06:48.021874 28053 net.cpp:122] Setting up Scale4
I0307 17:06:48.021883 28053 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0307 17:06:48.021888 28053 net.cpp:137] Memory required for data: 132302000
I0307 17:06:48.021893 28053 layer_factory.hpp:77] Creating layer ReLU4
I0307 17:06:48.021898 28053 net.cpp:84] Creating Layer ReLU4
I0307 17:06:48.021901 28053 net.cpp:406] ReLU4 <- Convolution4
I0307 17:06:48.021909 28053 net.cpp:367] ReLU4 -> Convolution4 (in-place)
I0307 17:06:48.022502 28053 net.cpp:122] Setting up ReLU4
I0307 17:06:48.022514 28053 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0307 17:06:48.022517 28053 net.cpp:137] Memory required for data: 138855600
I0307 17:06:48.022521 28053 layer_factory.hpp:77] Creating layer Convolution5
I0307 17:06:48.022532 28053 net.cpp:84] Creating Layer Convolution5
I0307 17:06:48.022537 28053 net.cpp:406] Convolution5 <- Convolution4
I0307 17:06:48.022559 28053 net.cpp:380] Convolution5 -> Convolution5
I0307 17:06:48.025591 28053 net.cpp:122] Setting up Convolution5
I0307 17:06:48.025609 28053 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0307 17:06:48.025611 28053 net.cpp:137] Memory required for data: 145409200
I0307 17:06:48.025619 28053 layer_factory.hpp:77] Creating layer BatchNorm5
I0307 17:06:48.025625 28053 net.cpp:84] Creating Layer BatchNorm5
I0307 17:06:48.025629 28053 net.cpp:406] BatchNorm5 <- Convolution5
I0307 17:06:48.025636 28053 net.cpp:367] BatchNorm5 -> Convolution5 (in-place)
I0307 17:06:48.025880 28053 net.cpp:122] Setting up BatchNorm5
I0307 17:06:48.025889 28053 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0307 17:06:48.025892 28053 net.cpp:137] Memory required for data: 151962800
I0307 17:06:48.025905 28053 layer_factory.hpp:77] Creating layer Scale5
I0307 17:06:48.025913 28053 net.cpp:84] Creating Layer Scale5
I0307 17:06:48.025916 28053 net.cpp:406] Scale5 <- Convolution5
I0307 17:06:48.025921 28053 net.cpp:367] Scale5 -> Convolution5 (in-place)
I0307 17:06:48.025972 28053 layer_factory.hpp:77] Creating layer Scale5
I0307 17:06:48.026114 28053 net.cpp:122] Setting up Scale5
I0307 17:06:48.026124 28053 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0307 17:06:48.026126 28053 net.cpp:137] Memory required for data: 158516400
I0307 17:06:48.026131 28053 layer_factory.hpp:77] Creating layer Eltwise2
I0307 17:06:48.026140 28053 net.cpp:84] Creating Layer Eltwise2
I0307 17:06:48.026144 28053 net.cpp:406] Eltwise2 <- Eltwise1_ReLU3_0_split_1
I0307 17:06:48.026149 28053 net.cpp:406] Eltwise2 <- Convolution5
I0307 17:06:48.026154 28053 net.cpp:380] Eltwise2 -> Eltwise2
I0307 17:06:48.026185 28053 net.cpp:122] Setting up Eltwise2
I0307 17:06:48.026192 28053 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0307 17:06:48.026196 28053 net.cpp:137] Memory required for data: 165070000
I0307 17:06:48.026198 28053 layer_factory.hpp:77] Creating layer ReLU5
I0307 17:06:48.026212 28053 net.cpp:84] Creating Layer ReLU5
I0307 17:06:48.026216 28053 net.cpp:406] ReLU5 <- Eltwise2
I0307 17:06:48.026221 28053 net.cpp:367] ReLU5 -> Eltwise2 (in-place)
I0307 17:06:48.026821 28053 net.cpp:122] Setting up ReLU5
I0307 17:06:48.026834 28053 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0307 17:06:48.026836 28053 net.cpp:137] Memory required for data: 171623600
I0307 17:06:48.026840 28053 layer_factory.hpp:77] Creating layer Eltwise2_ReLU5_0_split
I0307 17:06:48.026846 28053 net.cpp:84] Creating Layer Eltwise2_ReLU5_0_split
I0307 17:06:48.026850 28053 net.cpp:406] Eltwise2_ReLU5_0_split <- Eltwise2
I0307 17:06:48.026857 28053 net.cpp:380] Eltwise2_ReLU5_0_split -> Eltwise2_ReLU5_0_split_0
I0307 17:06:48.026865 28053 net.cpp:380] Eltwise2_ReLU5_0_split -> Eltwise2_ReLU5_0_split_1
I0307 17:06:48.026922 28053 net.cpp:122] Setting up Eltwise2_ReLU5_0_split
I0307 17:06:48.026932 28053 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0307 17:06:48.026937 28053 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0307 17:06:48.026939 28053 net.cpp:137] Memory required for data: 184730800
I0307 17:06:48.026942 28053 layer_factory.hpp:77] Creating layer Convolution6
I0307 17:06:48.026953 28053 net.cpp:84] Creating Layer Convolution6
I0307 17:06:48.026962 28053 net.cpp:406] Convolution6 <- Eltwise2_ReLU5_0_split_0
I0307 17:06:48.026968 28053 net.cpp:380] Convolution6 -> Convolution6
I0307 17:06:48.029856 28053 net.cpp:122] Setting up Convolution6
I0307 17:06:48.029875 28053 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0307 17:06:48.029878 28053 net.cpp:137] Memory required for data: 191284400
I0307 17:06:48.029886 28053 layer_factory.hpp:77] Creating layer BatchNorm6
I0307 17:06:48.029894 28053 net.cpp:84] Creating Layer BatchNorm6
I0307 17:06:48.029901 28053 net.cpp:406] BatchNorm6 <- Convolution6
I0307 17:06:48.029906 28053 net.cpp:367] BatchNorm6 -> Convolution6 (in-place)
I0307 17:06:48.030153 28053 net.cpp:122] Setting up BatchNorm6
I0307 17:06:48.030162 28053 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0307 17:06:48.030166 28053 net.cpp:137] Memory required for data: 197838000
I0307 17:06:48.030189 28053 layer_factory.hpp:77] Creating layer Scale6
I0307 17:06:48.030194 28053 net.cpp:84] Creating Layer Scale6
I0307 17:06:48.030198 28053 net.cpp:406] Scale6 <- Convolution6
I0307 17:06:48.030203 28053 net.cpp:367] Scale6 -> Convolution6 (in-place)
I0307 17:06:48.030261 28053 layer_factory.hpp:77] Creating layer Scale6
I0307 17:06:48.030400 28053 net.cpp:122] Setting up Scale6
I0307 17:06:48.030409 28053 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0307 17:06:48.030412 28053 net.cpp:137] Memory required for data: 204391600
I0307 17:06:48.030418 28053 layer_factory.hpp:77] Creating layer ReLU6
I0307 17:06:48.030424 28053 net.cpp:84] Creating Layer ReLU6
I0307 17:06:48.030427 28053 net.cpp:406] ReLU6 <- Convolution6
I0307 17:06:48.030433 28053 net.cpp:367] ReLU6 -> Convolution6 (in-place)
I0307 17:06:48.031466 28053 net.cpp:122] Setting up ReLU6
I0307 17:06:48.031481 28053 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0307 17:06:48.031484 28053 net.cpp:137] Memory required for data: 210945200
I0307 17:06:48.031488 28053 layer_factory.hpp:77] Creating layer Convolution7
I0307 17:06:48.031500 28053 net.cpp:84] Creating Layer Convolution7
I0307 17:06:48.031507 28053 net.cpp:406] Convolution7 <- Convolution6
I0307 17:06:48.031515 28053 net.cpp:380] Convolution7 -> Convolution7
I0307 17:06:48.034214 28053 net.cpp:122] Setting up Convolution7
I0307 17:06:48.034229 28053 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0307 17:06:48.034234 28053 net.cpp:137] Memory required for data: 217498800
I0307 17:06:48.034240 28053 layer_factory.hpp:77] Creating layer BatchNorm7
I0307 17:06:48.034253 28053 net.cpp:84] Creating Layer BatchNorm7
I0307 17:06:48.034257 28053 net.cpp:406] BatchNorm7 <- Convolution7
I0307 17:06:48.034263 28053 net.cpp:367] BatchNorm7 -> Convolution7 (in-place)
I0307 17:06:48.034514 28053 net.cpp:122] Setting up BatchNorm7
I0307 17:06:48.034523 28053 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0307 17:06:48.034526 28053 net.cpp:137] Memory required for data: 224052400
I0307 17:06:48.034533 28053 layer_factory.hpp:77] Creating layer Scale7
I0307 17:06:48.034539 28053 net.cpp:84] Creating Layer Scale7
I0307 17:06:48.034543 28053 net.cpp:406] Scale7 <- Convolution7
I0307 17:06:48.034549 28053 net.cpp:367] Scale7 -> Convolution7 (in-place)
I0307 17:06:48.034598 28053 layer_factory.hpp:77] Creating layer Scale7
I0307 17:06:48.034739 28053 net.cpp:122] Setting up Scale7
I0307 17:06:48.034749 28053 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0307 17:06:48.034751 28053 net.cpp:137] Memory required for data: 230606000
I0307 17:06:48.034757 28053 layer_factory.hpp:77] Creating layer Eltwise3
I0307 17:06:48.034765 28053 net.cpp:84] Creating Layer Eltwise3
I0307 17:06:48.034768 28053 net.cpp:406] Eltwise3 <- Eltwise2_ReLU5_0_split_1
I0307 17:06:48.034772 28053 net.cpp:406] Eltwise3 <- Convolution7
I0307 17:06:48.034780 28053 net.cpp:380] Eltwise3 -> Eltwise3
I0307 17:06:48.034809 28053 net.cpp:122] Setting up Eltwise3
I0307 17:06:48.034818 28053 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0307 17:06:48.034822 28053 net.cpp:137] Memory required for data: 237159600
I0307 17:06:48.034826 28053 layer_factory.hpp:77] Creating layer ReLU7
I0307 17:06:48.034831 28053 net.cpp:84] Creating Layer ReLU7
I0307 17:06:48.034833 28053 net.cpp:406] ReLU7 <- Eltwise3
I0307 17:06:48.034838 28053 net.cpp:367] ReLU7 -> Eltwise3 (in-place)
I0307 17:06:48.035449 28053 net.cpp:122] Setting up ReLU7
I0307 17:06:48.035464 28053 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0307 17:06:48.035466 28053 net.cpp:137] Memory required for data: 243713200
I0307 17:06:48.035470 28053 layer_factory.hpp:77] Creating layer Eltwise3_ReLU7_0_split
I0307 17:06:48.035476 28053 net.cpp:84] Creating Layer Eltwise3_ReLU7_0_split
I0307 17:06:48.035480 28053 net.cpp:406] Eltwise3_ReLU7_0_split <- Eltwise3
I0307 17:06:48.035486 28053 net.cpp:380] Eltwise3_ReLU7_0_split -> Eltwise3_ReLU7_0_split_0
I0307 17:06:48.035493 28053 net.cpp:380] Eltwise3_ReLU7_0_split -> Eltwise3_ReLU7_0_split_1
I0307 17:06:48.035547 28053 net.cpp:122] Setting up Eltwise3_ReLU7_0_split
I0307 17:06:48.035567 28053 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0307 17:06:48.035571 28053 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0307 17:06:48.035574 28053 net.cpp:137] Memory required for data: 256820400
I0307 17:06:48.035578 28053 layer_factory.hpp:77] Creating layer Convolution8
I0307 17:06:48.035591 28053 net.cpp:84] Creating Layer Convolution8
I0307 17:06:48.035595 28053 net.cpp:406] Convolution8 <- Eltwise3_ReLU7_0_split_0
I0307 17:06:48.035601 28053 net.cpp:380] Convolution8 -> Convolution8
I0307 17:06:48.038661 28053 net.cpp:122] Setting up Convolution8
I0307 17:06:48.038677 28053 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0307 17:06:48.038681 28053 net.cpp:137] Memory required for data: 260097200
I0307 17:06:48.038688 28053 layer_factory.hpp:77] Creating layer BatchNorm8
I0307 17:06:48.038695 28053 net.cpp:84] Creating Layer BatchNorm8
I0307 17:06:48.038699 28053 net.cpp:406] BatchNorm8 <- Convolution8
I0307 17:06:48.038705 28053 net.cpp:367] BatchNorm8 -> Convolution8 (in-place)
I0307 17:06:48.038950 28053 net.cpp:122] Setting up BatchNorm8
I0307 17:06:48.038960 28053 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0307 17:06:48.038964 28053 net.cpp:137] Memory required for data: 263374000
I0307 17:06:48.038970 28053 layer_factory.hpp:77] Creating layer Scale8
I0307 17:06:48.038977 28053 net.cpp:84] Creating Layer Scale8
I0307 17:06:48.038980 28053 net.cpp:406] Scale8 <- Convolution8
I0307 17:06:48.038985 28053 net.cpp:367] Scale8 -> Convolution8 (in-place)
I0307 17:06:48.039036 28053 layer_factory.hpp:77] Creating layer Scale8
I0307 17:06:48.039177 28053 net.cpp:122] Setting up Scale8
I0307 17:06:48.039186 28053 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0307 17:06:48.039189 28053 net.cpp:137] Memory required for data: 266650800
I0307 17:06:48.039196 28053 layer_factory.hpp:77] Creating layer Convolution9
I0307 17:06:48.039208 28053 net.cpp:84] Creating Layer Convolution9
I0307 17:06:48.039213 28053 net.cpp:406] Convolution9 <- Eltwise3_ReLU7_0_split_1
I0307 17:06:48.039219 28053 net.cpp:380] Convolution9 -> Convolution9
I0307 17:06:48.041857 28053 net.cpp:122] Setting up Convolution9
I0307 17:06:48.041874 28053 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0307 17:06:48.041878 28053 net.cpp:137] Memory required for data: 269927600
I0307 17:06:48.041885 28053 layer_factory.hpp:77] Creating layer BatchNorm9
I0307 17:06:48.041893 28053 net.cpp:84] Creating Layer BatchNorm9
I0307 17:06:48.041896 28053 net.cpp:406] BatchNorm9 <- Convolution9
I0307 17:06:48.041903 28053 net.cpp:367] BatchNorm9 -> Convolution9 (in-place)
I0307 17:06:48.042146 28053 net.cpp:122] Setting up BatchNorm9
I0307 17:06:48.042155 28053 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0307 17:06:48.042158 28053 net.cpp:137] Memory required for data: 273204400
I0307 17:06:48.042165 28053 layer_factory.hpp:77] Creating layer Scale9
I0307 17:06:48.042171 28053 net.cpp:84] Creating Layer Scale9
I0307 17:06:48.042174 28053 net.cpp:406] Scale9 <- Convolution9
I0307 17:06:48.042179 28053 net.cpp:367] Scale9 -> Convolution9 (in-place)
I0307 17:06:48.042229 28053 layer_factory.hpp:77] Creating layer Scale9
I0307 17:06:48.042371 28053 net.cpp:122] Setting up Scale9
I0307 17:06:48.042381 28053 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0307 17:06:48.042384 28053 net.cpp:137] Memory required for data: 276481200
I0307 17:06:48.042389 28053 layer_factory.hpp:77] Creating layer ReLU8
I0307 17:06:48.042394 28053 net.cpp:84] Creating Layer ReLU8
I0307 17:06:48.042398 28053 net.cpp:406] ReLU8 <- Convolution9
I0307 17:06:48.042402 28053 net.cpp:367] ReLU8 -> Convolution9 (in-place)
I0307 17:06:48.043000 28053 net.cpp:122] Setting up ReLU8
I0307 17:06:48.043012 28053 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0307 17:06:48.043016 28053 net.cpp:137] Memory required for data: 279758000
I0307 17:06:48.043020 28053 layer_factory.hpp:77] Creating layer Convolution10
I0307 17:06:48.043030 28053 net.cpp:84] Creating Layer Convolution10
I0307 17:06:48.043035 28053 net.cpp:406] Convolution10 <- Convolution9
I0307 17:06:48.043053 28053 net.cpp:380] Convolution10 -> Convolution10
I0307 17:06:48.046439 28053 net.cpp:122] Setting up Convolution10
I0307 17:06:48.046458 28053 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0307 17:06:48.046461 28053 net.cpp:137] Memory required for data: 283034800
I0307 17:06:48.046479 28053 layer_factory.hpp:77] Creating layer BatchNorm10
I0307 17:06:48.046489 28053 net.cpp:84] Creating Layer BatchNorm10
I0307 17:06:48.046494 28053 net.cpp:406] BatchNorm10 <- Convolution10
I0307 17:06:48.046499 28053 net.cpp:367] BatchNorm10 -> Convolution10 (in-place)
I0307 17:06:48.046757 28053 net.cpp:122] Setting up BatchNorm10
I0307 17:06:48.046767 28053 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0307 17:06:48.046772 28053 net.cpp:137] Memory required for data: 286311600
I0307 17:06:48.046779 28053 layer_factory.hpp:77] Creating layer Scale10
I0307 17:06:48.046785 28053 net.cpp:84] Creating Layer Scale10
I0307 17:06:48.046788 28053 net.cpp:406] Scale10 <- Convolution10
I0307 17:06:48.046793 28053 net.cpp:367] Scale10 -> Convolution10 (in-place)
I0307 17:06:48.046845 28053 layer_factory.hpp:77] Creating layer Scale10
I0307 17:06:48.046991 28053 net.cpp:122] Setting up Scale10
I0307 17:06:48.047000 28053 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0307 17:06:48.047003 28053 net.cpp:137] Memory required for data: 289588400
I0307 17:06:48.047009 28053 layer_factory.hpp:77] Creating layer Eltwise4
I0307 17:06:48.047015 28053 net.cpp:84] Creating Layer Eltwise4
I0307 17:06:48.047019 28053 net.cpp:406] Eltwise4 <- Convolution8
I0307 17:06:48.047024 28053 net.cpp:406] Eltwise4 <- Convolution10
I0307 17:06:48.047029 28053 net.cpp:380] Eltwise4 -> Eltwise4
I0307 17:06:48.047055 28053 net.cpp:122] Setting up Eltwise4
I0307 17:06:48.047062 28053 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0307 17:06:48.047066 28053 net.cpp:137] Memory required for data: 292865200
I0307 17:06:48.047070 28053 layer_factory.hpp:77] Creating layer ReLU9
I0307 17:06:48.047075 28053 net.cpp:84] Creating Layer ReLU9
I0307 17:06:48.047077 28053 net.cpp:406] ReLU9 <- Eltwise4
I0307 17:06:48.047082 28053 net.cpp:367] ReLU9 -> Eltwise4 (in-place)
I0307 17:06:48.047695 28053 net.cpp:122] Setting up ReLU9
I0307 17:06:48.047708 28053 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0307 17:06:48.047711 28053 net.cpp:137] Memory required for data: 296142000
I0307 17:06:48.047714 28053 layer_factory.hpp:77] Creating layer Eltwise4_ReLU9_0_split
I0307 17:06:48.047722 28053 net.cpp:84] Creating Layer Eltwise4_ReLU9_0_split
I0307 17:06:48.047725 28053 net.cpp:406] Eltwise4_ReLU9_0_split <- Eltwise4
I0307 17:06:48.047731 28053 net.cpp:380] Eltwise4_ReLU9_0_split -> Eltwise4_ReLU9_0_split_0
I0307 17:06:48.047739 28053 net.cpp:380] Eltwise4_ReLU9_0_split -> Eltwise4_ReLU9_0_split_1
I0307 17:06:48.047793 28053 net.cpp:122] Setting up Eltwise4_ReLU9_0_split
I0307 17:06:48.047801 28053 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0307 17:06:48.047806 28053 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0307 17:06:48.047808 28053 net.cpp:137] Memory required for data: 302695600
I0307 17:06:48.047812 28053 layer_factory.hpp:77] Creating layer Convolution11
I0307 17:06:48.047823 28053 net.cpp:84] Creating Layer Convolution11
I0307 17:06:48.047828 28053 net.cpp:406] Convolution11 <- Eltwise4_ReLU9_0_split_0
I0307 17:06:48.047835 28053 net.cpp:380] Convolution11 -> Convolution11
I0307 17:06:48.051126 28053 net.cpp:122] Setting up Convolution11
I0307 17:06:48.051143 28053 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0307 17:06:48.051146 28053 net.cpp:137] Memory required for data: 305972400
I0307 17:06:48.051153 28053 layer_factory.hpp:77] Creating layer BatchNorm11
I0307 17:06:48.051162 28053 net.cpp:84] Creating Layer BatchNorm11
I0307 17:06:48.051165 28053 net.cpp:406] BatchNorm11 <- Convolution11
I0307 17:06:48.051174 28053 net.cpp:367] BatchNorm11 -> Convolution11 (in-place)
I0307 17:06:48.051425 28053 net.cpp:122] Setting up BatchNorm11
I0307 17:06:48.051434 28053 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0307 17:06:48.051451 28053 net.cpp:137] Memory required for data: 309249200
I0307 17:06:48.051458 28053 layer_factory.hpp:77] Creating layer Scale11
I0307 17:06:48.051465 28053 net.cpp:84] Creating Layer Scale11
I0307 17:06:48.051468 28053 net.cpp:406] Scale11 <- Convolution11
I0307 17:06:48.051476 28053 net.cpp:367] Scale11 -> Convolution11 (in-place)
I0307 17:06:48.051529 28053 layer_factory.hpp:77] Creating layer Scale11
I0307 17:06:48.051677 28053 net.cpp:122] Setting up Scale11
I0307 17:06:48.051687 28053 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0307 17:06:48.051689 28053 net.cpp:137] Memory required for data: 312526000
I0307 17:06:48.051695 28053 layer_factory.hpp:77] Creating layer ReLU10
I0307 17:06:48.051702 28053 net.cpp:84] Creating Layer ReLU10
I0307 17:06:48.051704 28053 net.cpp:406] ReLU10 <- Convolution11
I0307 17:06:48.051710 28053 net.cpp:367] ReLU10 -> Convolution11 (in-place)
I0307 17:06:48.052734 28053 net.cpp:122] Setting up ReLU10
I0307 17:06:48.052749 28053 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0307 17:06:48.052753 28053 net.cpp:137] Memory required for data: 315802800
I0307 17:06:48.052757 28053 layer_factory.hpp:77] Creating layer Convolution12
I0307 17:06:48.052768 28053 net.cpp:84] Creating Layer Convolution12
I0307 17:06:48.052772 28053 net.cpp:406] Convolution12 <- Convolution11
I0307 17:06:48.052788 28053 net.cpp:380] Convolution12 -> Convolution12
I0307 17:06:48.055533 28053 net.cpp:122] Setting up Convolution12
I0307 17:06:48.055549 28053 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0307 17:06:48.055553 28053 net.cpp:137] Memory required for data: 319079600
I0307 17:06:48.055560 28053 layer_factory.hpp:77] Creating layer BatchNorm12
I0307 17:06:48.055567 28053 net.cpp:84] Creating Layer BatchNorm12
I0307 17:06:48.055572 28053 net.cpp:406] BatchNorm12 <- Convolution12
I0307 17:06:48.055577 28053 net.cpp:367] BatchNorm12 -> Convolution12 (in-place)
I0307 17:06:48.055827 28053 net.cpp:122] Setting up BatchNorm12
I0307 17:06:48.055835 28053 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0307 17:06:48.055838 28053 net.cpp:137] Memory required for data: 322356400
I0307 17:06:48.055845 28053 layer_factory.hpp:77] Creating layer Scale12
I0307 17:06:48.055851 28053 net.cpp:84] Creating Layer Scale12
I0307 17:06:48.055855 28053 net.cpp:406] Scale12 <- Convolution12
I0307 17:06:48.055861 28053 net.cpp:367] Scale12 -> Convolution12 (in-place)
I0307 17:06:48.055912 28053 layer_factory.hpp:77] Creating layer Scale12
I0307 17:06:48.056062 28053 net.cpp:122] Setting up Scale12
I0307 17:06:48.056071 28053 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0307 17:06:48.056074 28053 net.cpp:137] Memory required for data: 325633200
I0307 17:06:48.056080 28053 layer_factory.hpp:77] Creating layer Eltwise5
I0307 17:06:48.056087 28053 net.cpp:84] Creating Layer Eltwise5
I0307 17:06:48.056090 28053 net.cpp:406] Eltwise5 <- Eltwise4_ReLU9_0_split_1
I0307 17:06:48.056095 28053 net.cpp:406] Eltwise5 <- Convolution12
I0307 17:06:48.056102 28053 net.cpp:380] Eltwise5 -> Eltwise5
I0307 17:06:48.056129 28053 net.cpp:122] Setting up Eltwise5
I0307 17:06:48.056138 28053 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0307 17:06:48.056141 28053 net.cpp:137] Memory required for data: 328910000
I0307 17:06:48.056145 28053 layer_factory.hpp:77] Creating layer ReLU11
I0307 17:06:48.056150 28053 net.cpp:84] Creating Layer ReLU11
I0307 17:06:48.056154 28053 net.cpp:406] ReLU11 <- Eltwise5
I0307 17:06:48.056159 28053 net.cpp:367] ReLU11 -> Eltwise5 (in-place)
I0307 17:06:48.056762 28053 net.cpp:122] Setting up ReLU11
I0307 17:06:48.056782 28053 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0307 17:06:48.056787 28053 net.cpp:137] Memory required for data: 332186800
I0307 17:06:48.056790 28053 layer_factory.hpp:77] Creating layer Eltwise5_ReLU11_0_split
I0307 17:06:48.056795 28053 net.cpp:84] Creating Layer Eltwise5_ReLU11_0_split
I0307 17:06:48.056799 28053 net.cpp:406] Eltwise5_ReLU11_0_split <- Eltwise5
I0307 17:06:48.056804 28053 net.cpp:380] Eltwise5_ReLU11_0_split -> Eltwise5_ReLU11_0_split_0
I0307 17:06:48.056813 28053 net.cpp:380] Eltwise5_ReLU11_0_split -> Eltwise5_ReLU11_0_split_1
I0307 17:06:48.056887 28053 net.cpp:122] Setting up Eltwise5_ReLU11_0_split
I0307 17:06:48.056900 28053 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0307 17:06:48.056905 28053 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0307 17:06:48.056907 28053 net.cpp:137] Memory required for data: 338740400
I0307 17:06:48.056911 28053 layer_factory.hpp:77] Creating layer Convolution13
I0307 17:06:48.056921 28053 net.cpp:84] Creating Layer Convolution13
I0307 17:06:48.056926 28053 net.cpp:406] Convolution13 <- Eltwise5_ReLU11_0_split_0
I0307 17:06:48.056932 28053 net.cpp:380] Convolution13 -> Convolution13
I0307 17:06:48.060061 28053 net.cpp:122] Setting up Convolution13
I0307 17:06:48.060079 28053 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0307 17:06:48.060082 28053 net.cpp:137] Memory required for data: 342017200
I0307 17:06:48.060089 28053 layer_factory.hpp:77] Creating layer BatchNorm13
I0307 17:06:48.060096 28053 net.cpp:84] Creating Layer BatchNorm13
I0307 17:06:48.060101 28053 net.cpp:406] BatchNorm13 <- Convolution13
I0307 17:06:48.060107 28053 net.cpp:367] BatchNorm13 -> Convolution13 (in-place)
I0307 17:06:48.060359 28053 net.cpp:122] Setting up BatchNorm13
I0307 17:06:48.060370 28053 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0307 17:06:48.060374 28053 net.cpp:137] Memory required for data: 345294000
I0307 17:06:48.060380 28053 layer_factory.hpp:77] Creating layer Scale13
I0307 17:06:48.060385 28053 net.cpp:84] Creating Layer Scale13
I0307 17:06:48.060389 28053 net.cpp:406] Scale13 <- Convolution13
I0307 17:06:48.060395 28053 net.cpp:367] Scale13 -> Convolution13 (in-place)
I0307 17:06:48.060447 28053 layer_factory.hpp:77] Creating layer Scale13
I0307 17:06:48.060595 28053 net.cpp:122] Setting up Scale13
I0307 17:06:48.060606 28053 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0307 17:06:48.060609 28053 net.cpp:137] Memory required for data: 348570800
I0307 17:06:48.060616 28053 layer_factory.hpp:77] Creating layer ReLU12
I0307 17:06:48.060621 28053 net.cpp:84] Creating Layer ReLU12
I0307 17:06:48.060623 28053 net.cpp:406] ReLU12 <- Convolution13
I0307 17:06:48.060628 28053 net.cpp:367] ReLU12 -> Convolution13 (in-place)
I0307 17:06:48.061247 28053 net.cpp:122] Setting up ReLU12
I0307 17:06:48.061260 28053 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0307 17:06:48.061264 28053 net.cpp:137] Memory required for data: 351847600
I0307 17:06:48.061267 28053 layer_factory.hpp:77] Creating layer Convolution14
I0307 17:06:48.061295 28053 net.cpp:84] Creating Layer Convolution14
I0307 17:06:48.061303 28053 net.cpp:406] Convolution14 <- Convolution13
I0307 17:06:48.061311 28053 net.cpp:380] Convolution14 -> Convolution14
I0307 17:06:48.064296 28053 net.cpp:122] Setting up Convolution14
I0307 17:06:48.064313 28053 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0307 17:06:48.064317 28053 net.cpp:137] Memory required for data: 355124400
I0307 17:06:48.064326 28053 layer_factory.hpp:77] Creating layer BatchNorm14
I0307 17:06:48.064332 28053 net.cpp:84] Creating Layer BatchNorm14
I0307 17:06:48.064337 28053 net.cpp:406] BatchNorm14 <- Convolution14
I0307 17:06:48.064343 28053 net.cpp:367] BatchNorm14 -> Convolution14 (in-place)
I0307 17:06:48.064599 28053 net.cpp:122] Setting up BatchNorm14
I0307 17:06:48.064607 28053 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0307 17:06:48.064610 28053 net.cpp:137] Memory required for data: 358401200
I0307 17:06:48.064617 28053 layer_factory.hpp:77] Creating layer Scale14
I0307 17:06:48.064623 28053 net.cpp:84] Creating Layer Scale14
I0307 17:06:48.064627 28053 net.cpp:406] Scale14 <- Convolution14
I0307 17:06:48.064632 28053 net.cpp:367] Scale14 -> Convolution14 (in-place)
I0307 17:06:48.064685 28053 layer_factory.hpp:77] Creating layer Scale14
I0307 17:06:48.064843 28053 net.cpp:122] Setting up Scale14
I0307 17:06:48.064853 28053 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0307 17:06:48.064857 28053 net.cpp:137] Memory required for data: 361678000
I0307 17:06:48.064862 28053 layer_factory.hpp:77] Creating layer Eltwise6
I0307 17:06:48.064883 28053 net.cpp:84] Creating Layer Eltwise6
I0307 17:06:48.064889 28053 net.cpp:406] Eltwise6 <- Eltwise5_ReLU11_0_split_1
I0307 17:06:48.064894 28053 net.cpp:406] Eltwise6 <- Convolution14
I0307 17:06:48.064899 28053 net.cpp:380] Eltwise6 -> Eltwise6
I0307 17:06:48.064930 28053 net.cpp:122] Setting up Eltwise6
I0307 17:06:48.064939 28053 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0307 17:06:48.064941 28053 net.cpp:137] Memory required for data: 364954800
I0307 17:06:48.064945 28053 layer_factory.hpp:77] Creating layer ReLU13
I0307 17:06:48.064951 28053 net.cpp:84] Creating Layer ReLU13
I0307 17:06:48.064955 28053 net.cpp:406] ReLU13 <- Eltwise6
I0307 17:06:48.064961 28053 net.cpp:367] ReLU13 -> Eltwise6 (in-place)
I0307 17:06:48.065984 28053 net.cpp:122] Setting up ReLU13
I0307 17:06:48.065999 28053 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0307 17:06:48.066002 28053 net.cpp:137] Memory required for data: 368231600
I0307 17:06:48.066006 28053 layer_factory.hpp:77] Creating layer Eltwise6_ReLU13_0_split
I0307 17:06:48.066015 28053 net.cpp:84] Creating Layer Eltwise6_ReLU13_0_split
I0307 17:06:48.066017 28053 net.cpp:406] Eltwise6_ReLU13_0_split <- Eltwise6
I0307 17:06:48.066023 28053 net.cpp:380] Eltwise6_ReLU13_0_split -> Eltwise6_ReLU13_0_split_0
I0307 17:06:48.066033 28053 net.cpp:380] Eltwise6_ReLU13_0_split -> Eltwise6_ReLU13_0_split_1
I0307 17:06:48.066098 28053 net.cpp:122] Setting up Eltwise6_ReLU13_0_split
I0307 17:06:48.066108 28053 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0307 17:06:48.066112 28053 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0307 17:06:48.066115 28053 net.cpp:137] Memory required for data: 374785200
I0307 17:06:48.066118 28053 layer_factory.hpp:77] Creating layer Convolution15
I0307 17:06:48.066129 28053 net.cpp:84] Creating Layer Convolution15
I0307 17:06:48.066134 28053 net.cpp:406] Convolution15 <- Eltwise6_ReLU13_0_split_0
I0307 17:06:48.066143 28053 net.cpp:380] Convolution15 -> Convolution15
I0307 17:06:48.069049 28053 net.cpp:122] Setting up Convolution15
I0307 17:06:48.069069 28053 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0307 17:06:48.069073 28053 net.cpp:137] Memory required for data: 376423600
I0307 17:06:48.069082 28053 layer_factory.hpp:77] Creating layer BatchNorm15
I0307 17:06:48.069087 28053 net.cpp:84] Creating Layer BatchNorm15
I0307 17:06:48.069092 28053 net.cpp:406] BatchNorm15 <- Convolution15
I0307 17:06:48.069098 28053 net.cpp:367] BatchNorm15 -> Convolution15 (in-place)
I0307 17:06:48.069356 28053 net.cpp:122] Setting up BatchNorm15
I0307 17:06:48.069365 28053 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0307 17:06:48.069370 28053 net.cpp:137] Memory required for data: 378062000
I0307 17:06:48.069376 28053 layer_factory.hpp:77] Creating layer Scale15
I0307 17:06:48.069382 28053 net.cpp:84] Creating Layer Scale15
I0307 17:06:48.069386 28053 net.cpp:406] Scale15 <- Convolution15
I0307 17:06:48.069391 28053 net.cpp:367] Scale15 -> Convolution15 (in-place)
I0307 17:06:48.069447 28053 layer_factory.hpp:77] Creating layer Scale15
I0307 17:06:48.069595 28053 net.cpp:122] Setting up Scale15
I0307 17:06:48.069607 28053 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0307 17:06:48.069609 28053 net.cpp:137] Memory required for data: 379700400
I0307 17:06:48.069615 28053 layer_factory.hpp:77] Creating layer Convolution16
I0307 17:06:48.069625 28053 net.cpp:84] Creating Layer Convolution16
I0307 17:06:48.069629 28053 net.cpp:406] Convolution16 <- Eltwise6_ReLU13_0_split_1
I0307 17:06:48.069635 28053 net.cpp:380] Convolution16 -> Convolution16
I0307 17:06:48.072499 28053 net.cpp:122] Setting up Convolution16
I0307 17:06:48.072515 28053 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0307 17:06:48.072520 28053 net.cpp:137] Memory required for data: 381338800
I0307 17:06:48.072526 28053 layer_factory.hpp:77] Creating layer BatchNorm16
I0307 17:06:48.072532 28053 net.cpp:84] Creating Layer BatchNorm16
I0307 17:06:48.072536 28053 net.cpp:406] BatchNorm16 <- Convolution16
I0307 17:06:48.072543 28053 net.cpp:367] BatchNorm16 -> Convolution16 (in-place)
I0307 17:06:48.072826 28053 net.cpp:122] Setting up BatchNorm16
I0307 17:06:48.072837 28053 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0307 17:06:48.072841 28053 net.cpp:137] Memory required for data: 382977200
I0307 17:06:48.072849 28053 layer_factory.hpp:77] Creating layer Scale16
I0307 17:06:48.072854 28053 net.cpp:84] Creating Layer Scale16
I0307 17:06:48.072859 28053 net.cpp:406] Scale16 <- Convolution16
I0307 17:06:48.072863 28053 net.cpp:367] Scale16 -> Convolution16 (in-place)
I0307 17:06:48.072918 28053 layer_factory.hpp:77] Creating layer Scale16
I0307 17:06:48.073068 28053 net.cpp:122] Setting up Scale16
I0307 17:06:48.073078 28053 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0307 17:06:48.073081 28053 net.cpp:137] Memory required for data: 384615600
I0307 17:06:48.073087 28053 layer_factory.hpp:77] Creating layer ReLU14
I0307 17:06:48.073093 28053 net.cpp:84] Creating Layer ReLU14
I0307 17:06:48.073096 28053 net.cpp:406] ReLU14 <- Convolution16
I0307 17:06:48.073101 28053 net.cpp:367] ReLU14 -> Convolution16 (in-place)
I0307 17:06:48.074146 28053 net.cpp:122] Setting up ReLU14
I0307 17:06:48.074162 28053 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0307 17:06:48.074165 28053 net.cpp:137] Memory required for data: 386254000
I0307 17:06:48.074169 28053 layer_factory.hpp:77] Creating layer Convolution17
I0307 17:06:48.074182 28053 net.cpp:84] Creating Layer Convolution17
I0307 17:06:48.074187 28053 net.cpp:406] Convolution17 <- Convolution16
I0307 17:06:48.074194 28053 net.cpp:380] Convolution17 -> Convolution17
I0307 17:06:48.078006 28053 net.cpp:122] Setting up Convolution17
I0307 17:06:48.078024 28053 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0307 17:06:48.078028 28053 net.cpp:137] Memory required for data: 387892400
I0307 17:06:48.078035 28053 layer_factory.hpp:77] Creating layer BatchNorm17
I0307 17:06:48.078042 28053 net.cpp:84] Creating Layer BatchNorm17
I0307 17:06:48.078045 28053 net.cpp:406] BatchNorm17 <- Convolution17
I0307 17:06:48.078052 28053 net.cpp:367] BatchNorm17 -> Convolution17 (in-place)
I0307 17:06:48.078320 28053 net.cpp:122] Setting up BatchNorm17
I0307 17:06:48.078330 28053 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0307 17:06:48.078333 28053 net.cpp:137] Memory required for data: 389530800
I0307 17:06:48.078341 28053 layer_factory.hpp:77] Creating layer Scale17
I0307 17:06:48.078346 28053 net.cpp:84] Creating Layer Scale17
I0307 17:06:48.078351 28053 net.cpp:406] Scale17 <- Convolution17
I0307 17:06:48.078356 28053 net.cpp:367] Scale17 -> Convolution17 (in-place)
I0307 17:06:48.078406 28053 layer_factory.hpp:77] Creating layer Scale17
I0307 17:06:48.078558 28053 net.cpp:122] Setting up Scale17
I0307 17:06:48.078567 28053 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0307 17:06:48.078570 28053 net.cpp:137] Memory required for data: 391169200
I0307 17:06:48.078577 28053 layer_factory.hpp:77] Creating layer Eltwise7
I0307 17:06:48.078583 28053 net.cpp:84] Creating Layer Eltwise7
I0307 17:06:48.078588 28053 net.cpp:406] Eltwise7 <- Convolution15
I0307 17:06:48.078591 28053 net.cpp:406] Eltwise7 <- Convolution17
I0307 17:06:48.078596 28053 net.cpp:380] Eltwise7 -> Eltwise7
I0307 17:06:48.078630 28053 net.cpp:122] Setting up Eltwise7
I0307 17:06:48.078639 28053 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0307 17:06:48.078641 28053 net.cpp:137] Memory required for data: 392807600
I0307 17:06:48.078645 28053 layer_factory.hpp:77] Creating layer ReLU15
I0307 17:06:48.078651 28053 net.cpp:84] Creating Layer ReLU15
I0307 17:06:48.078655 28053 net.cpp:406] ReLU15 <- Eltwise7
I0307 17:06:48.078662 28053 net.cpp:367] ReLU15 -> Eltwise7 (in-place)
I0307 17:06:48.079694 28053 net.cpp:122] Setting up ReLU15
I0307 17:06:48.079708 28053 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0307 17:06:48.079712 28053 net.cpp:137] Memory required for data: 394446000
I0307 17:06:48.079716 28053 layer_factory.hpp:77] Creating layer Eltwise7_ReLU15_0_split
I0307 17:06:48.079725 28053 net.cpp:84] Creating Layer Eltwise7_ReLU15_0_split
I0307 17:06:48.079728 28053 net.cpp:406] Eltwise7_ReLU15_0_split <- Eltwise7
I0307 17:06:48.079746 28053 net.cpp:380] Eltwise7_ReLU15_0_split -> Eltwise7_ReLU15_0_split_0
I0307 17:06:48.079753 28053 net.cpp:380] Eltwise7_ReLU15_0_split -> Eltwise7_ReLU15_0_split_1
I0307 17:06:48.079813 28053 net.cpp:122] Setting up Eltwise7_ReLU15_0_split
I0307 17:06:48.079821 28053 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0307 17:06:48.079825 28053 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0307 17:06:48.079828 28053 net.cpp:137] Memory required for data: 397722800
I0307 17:06:48.079833 28053 layer_factory.hpp:77] Creating layer Convolution18
I0307 17:06:48.079843 28053 net.cpp:84] Creating Layer Convolution18
I0307 17:06:48.079846 28053 net.cpp:406] Convolution18 <- Eltwise7_ReLU15_0_split_0
I0307 17:06:48.079854 28053 net.cpp:380] Convolution18 -> Convolution18
I0307 17:06:48.083319 28053 net.cpp:122] Setting up Convolution18
I0307 17:06:48.083335 28053 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0307 17:06:48.083339 28053 net.cpp:137] Memory required for data: 399361200
I0307 17:06:48.083348 28053 layer_factory.hpp:77] Creating layer BatchNorm18
I0307 17:06:48.083354 28053 net.cpp:84] Creating Layer BatchNorm18
I0307 17:06:48.083359 28053 net.cpp:406] BatchNorm18 <- Convolution18
I0307 17:06:48.083365 28053 net.cpp:367] BatchNorm18 -> Convolution18 (in-place)
I0307 17:06:48.083636 28053 net.cpp:122] Setting up BatchNorm18
I0307 17:06:48.083644 28053 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0307 17:06:48.083648 28053 net.cpp:137] Memory required for data: 400999600
I0307 17:06:48.083655 28053 layer_factory.hpp:77] Creating layer Scale18
I0307 17:06:48.083662 28053 net.cpp:84] Creating Layer Scale18
I0307 17:06:48.083665 28053 net.cpp:406] Scale18 <- Convolution18
I0307 17:06:48.083670 28053 net.cpp:367] Scale18 -> Convolution18 (in-place)
I0307 17:06:48.083724 28053 layer_factory.hpp:77] Creating layer Scale18
I0307 17:06:48.083879 28053 net.cpp:122] Setting up Scale18
I0307 17:06:48.083890 28053 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0307 17:06:48.083894 28053 net.cpp:137] Memory required for data: 402638000
I0307 17:06:48.083900 28053 layer_factory.hpp:77] Creating layer ReLU16
I0307 17:06:48.083905 28053 net.cpp:84] Creating Layer ReLU16
I0307 17:06:48.083909 28053 net.cpp:406] ReLU16 <- Convolution18
I0307 17:06:48.083915 28053 net.cpp:367] ReLU16 -> Convolution18 (in-place)
I0307 17:06:48.084540 28053 net.cpp:122] Setting up ReLU16
I0307 17:06:48.084553 28053 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0307 17:06:48.084556 28053 net.cpp:137] Memory required for data: 404276400
I0307 17:06:48.084560 28053 layer_factory.hpp:77] Creating layer Convolution19
I0307 17:06:48.084571 28053 net.cpp:84] Creating Layer Convolution19
I0307 17:06:48.084575 28053 net.cpp:406] Convolution19 <- Convolution18
I0307 17:06:48.084583 28053 net.cpp:380] Convolution19 -> Convolution19
I0307 17:06:48.088599 28053 net.cpp:122] Setting up Convolution19
I0307 17:06:48.088615 28053 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0307 17:06:48.088619 28053 net.cpp:137] Memory required for data: 405914800
I0307 17:06:48.088626 28053 layer_factory.hpp:77] Creating layer BatchNorm19
I0307 17:06:48.088634 28053 net.cpp:84] Creating Layer BatchNorm19
I0307 17:06:48.088639 28053 net.cpp:406] BatchNorm19 <- Convolution19
I0307 17:06:48.088644 28053 net.cpp:367] BatchNorm19 -> Convolution19 (in-place)
I0307 17:06:48.088928 28053 net.cpp:122] Setting up BatchNorm19
I0307 17:06:48.088939 28053 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0307 17:06:48.088943 28053 net.cpp:137] Memory required for data: 407553200
I0307 17:06:48.088966 28053 layer_factory.hpp:77] Creating layer Scale19
I0307 17:06:48.088974 28053 net.cpp:84] Creating Layer Scale19
I0307 17:06:48.088979 28053 net.cpp:406] Scale19 <- Convolution19
I0307 17:06:48.088985 28053 net.cpp:367] Scale19 -> Convolution19 (in-place)
I0307 17:06:48.089040 28053 layer_factory.hpp:77] Creating layer Scale19
I0307 17:06:48.089191 28053 net.cpp:122] Setting up Scale19
I0307 17:06:48.089200 28053 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0307 17:06:48.089205 28053 net.cpp:137] Memory required for data: 409191600
I0307 17:06:48.089223 28053 layer_factory.hpp:77] Creating layer Eltwise8
I0307 17:06:48.089231 28053 net.cpp:84] Creating Layer Eltwise8
I0307 17:06:48.089236 28053 net.cpp:406] Eltwise8 <- Eltwise7_ReLU15_0_split_1
I0307 17:06:48.089239 28053 net.cpp:406] Eltwise8 <- Convolution19
I0307 17:06:48.089246 28053 net.cpp:380] Eltwise8 -> Eltwise8
I0307 17:06:48.089283 28053 net.cpp:122] Setting up Eltwise8
I0307 17:06:48.089291 28053 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0307 17:06:48.089294 28053 net.cpp:137] Memory required for data: 410830000
I0307 17:06:48.089298 28053 layer_factory.hpp:77] Creating layer ReLU17
I0307 17:06:48.089303 28053 net.cpp:84] Creating Layer ReLU17
I0307 17:06:48.089306 28053 net.cpp:406] ReLU17 <- Eltwise8
I0307 17:06:48.089310 28053 net.cpp:367] ReLU17 -> Eltwise8 (in-place)
I0307 17:06:48.089932 28053 net.cpp:122] Setting up ReLU17
I0307 17:06:48.089946 28053 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0307 17:06:48.089948 28053 net.cpp:137] Memory required for data: 412468400
I0307 17:06:48.089952 28053 layer_factory.hpp:77] Creating layer Eltwise8_ReLU17_0_split
I0307 17:06:48.089958 28053 net.cpp:84] Creating Layer Eltwise8_ReLU17_0_split
I0307 17:06:48.089962 28053 net.cpp:406] Eltwise8_ReLU17_0_split <- Eltwise8
I0307 17:06:48.089969 28053 net.cpp:380] Eltwise8_ReLU17_0_split -> Eltwise8_ReLU17_0_split_0
I0307 17:06:48.089977 28053 net.cpp:380] Eltwise8_ReLU17_0_split -> Eltwise8_ReLU17_0_split_1
I0307 17:06:48.090042 28053 net.cpp:122] Setting up Eltwise8_ReLU17_0_split
I0307 17:06:48.090052 28053 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0307 17:06:48.090056 28053 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0307 17:06:48.090059 28053 net.cpp:137] Memory required for data: 415745200
I0307 17:06:48.090062 28053 layer_factory.hpp:77] Creating layer Convolution20
I0307 17:06:48.090075 28053 net.cpp:84] Creating Layer Convolution20
I0307 17:06:48.090078 28053 net.cpp:406] Convolution20 <- Eltwise8_ReLU17_0_split_0
I0307 17:06:48.090085 28053 net.cpp:380] Convolution20 -> Convolution20
I0307 17:06:48.093744 28053 net.cpp:122] Setting up Convolution20
I0307 17:06:48.093760 28053 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0307 17:06:48.093765 28053 net.cpp:137] Memory required for data: 417383600
I0307 17:06:48.093771 28053 layer_factory.hpp:77] Creating layer BatchNorm20
I0307 17:06:48.093780 28053 net.cpp:84] Creating Layer BatchNorm20
I0307 17:06:48.093783 28053 net.cpp:406] BatchNorm20 <- Convolution20
I0307 17:06:48.093788 28053 net.cpp:367] BatchNorm20 -> Convolution20 (in-place)
I0307 17:06:48.094056 28053 net.cpp:122] Setting up BatchNorm20
I0307 17:06:48.094066 28053 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0307 17:06:48.094069 28053 net.cpp:137] Memory required for data: 419022000
I0307 17:06:48.094076 28053 layer_factory.hpp:77] Creating layer Scale20
I0307 17:06:48.094082 28053 net.cpp:84] Creating Layer Scale20
I0307 17:06:48.094085 28053 net.cpp:406] Scale20 <- Convolution20
I0307 17:06:48.094090 28053 net.cpp:367] Scale20 -> Convolution20 (in-place)
I0307 17:06:48.094146 28053 layer_factory.hpp:77] Creating layer Scale20
I0307 17:06:48.094316 28053 net.cpp:122] Setting up Scale20
I0307 17:06:48.094326 28053 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0307 17:06:48.094328 28053 net.cpp:137] Memory required for data: 420660400
I0307 17:06:48.094336 28053 layer_factory.hpp:77] Creating layer ReLU18
I0307 17:06:48.094341 28053 net.cpp:84] Creating Layer ReLU18
I0307 17:06:48.094344 28053 net.cpp:406] ReLU18 <- Convolution20
I0307 17:06:48.094349 28053 net.cpp:367] ReLU18 -> Convolution20 (in-place)
I0307 17:06:48.095438 28053 net.cpp:122] Setting up ReLU18
I0307 17:06:48.095456 28053 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0307 17:06:48.095460 28053 net.cpp:137] Memory required for data: 422298800
I0307 17:06:48.095464 28053 layer_factory.hpp:77] Creating layer Convolution21
I0307 17:06:48.095476 28053 net.cpp:84] Creating Layer Convolution21
I0307 17:06:48.095480 28053 net.cpp:406] Convolution21 <- Convolution20
I0307 17:06:48.095504 28053 net.cpp:380] Convolution21 -> Convolution21
I0307 17:06:48.099164 28053 net.cpp:122] Setting up Convolution21
I0307 17:06:48.099181 28053 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0307 17:06:48.099185 28053 net.cpp:137] Memory required for data: 423937200
I0307 17:06:48.099193 28053 layer_factory.hpp:77] Creating layer BatchNorm21
I0307 17:06:48.099205 28053 net.cpp:84] Creating Layer BatchNorm21
I0307 17:06:48.099211 28053 net.cpp:406] BatchNorm21 <- Convolution21
I0307 17:06:48.099220 28053 net.cpp:367] BatchNorm21 -> Convolution21 (in-place)
I0307 17:06:48.099522 28053 net.cpp:122] Setting up BatchNorm21
I0307 17:06:48.099534 28053 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0307 17:06:48.099537 28053 net.cpp:137] Memory required for data: 425575600
I0307 17:06:48.099545 28053 layer_factory.hpp:77] Creating layer Scale21
I0307 17:06:48.099550 28053 net.cpp:84] Creating Layer Scale21
I0307 17:06:48.099555 28053 net.cpp:406] Scale21 <- Convolution21
I0307 17:06:48.099558 28053 net.cpp:367] Scale21 -> Convolution21 (in-place)
I0307 17:06:48.099617 28053 layer_factory.hpp:77] Creating layer Scale21
I0307 17:06:48.099781 28053 net.cpp:122] Setting up Scale21
I0307 17:06:48.099792 28053 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0307 17:06:48.099794 28053 net.cpp:137] Memory required for data: 427214000
I0307 17:06:48.099802 28053 layer_factory.hpp:77] Creating layer Eltwise9
I0307 17:06:48.099808 28053 net.cpp:84] Creating Layer Eltwise9
I0307 17:06:48.099812 28053 net.cpp:406] Eltwise9 <- Eltwise8_ReLU17_0_split_1
I0307 17:06:48.099817 28053 net.cpp:406] Eltwise9 <- Convolution21
I0307 17:06:48.099822 28053 net.cpp:380] Eltwise9 -> Eltwise9
I0307 17:06:48.099856 28053 net.cpp:122] Setting up Eltwise9
I0307 17:06:48.099867 28053 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0307 17:06:48.099870 28053 net.cpp:137] Memory required for data: 428852400
I0307 17:06:48.099874 28053 layer_factory.hpp:77] Creating layer ReLU19
I0307 17:06:48.099879 28053 net.cpp:84] Creating Layer ReLU19
I0307 17:06:48.099882 28053 net.cpp:406] ReLU19 <- Eltwise9
I0307 17:06:48.099887 28053 net.cpp:367] ReLU19 -> Eltwise9 (in-place)
I0307 17:06:48.100510 28053 net.cpp:122] Setting up ReLU19
I0307 17:06:48.100524 28053 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0307 17:06:48.100528 28053 net.cpp:137] Memory required for data: 430490800
I0307 17:06:48.100533 28053 layer_factory.hpp:77] Creating layer Pooling1
I0307 17:06:48.100539 28053 net.cpp:84] Creating Layer Pooling1
I0307 17:06:48.100543 28053 net.cpp:406] Pooling1 <- Eltwise9
I0307 17:06:48.100550 28053 net.cpp:380] Pooling1 -> Pooling1
I0307 17:06:48.101755 28053 net.cpp:122] Setting up Pooling1
I0307 17:06:48.101770 28053 net.cpp:129] Top shape: 100 64 1 1 (6400)
I0307 17:06:48.101775 28053 net.cpp:137] Memory required for data: 430516400
I0307 17:06:48.101780 28053 layer_factory.hpp:77] Creating layer InnerProduct1
I0307 17:06:48.101789 28053 net.cpp:84] Creating Layer InnerProduct1
I0307 17:06:48.101794 28053 net.cpp:406] InnerProduct1 <- Pooling1
I0307 17:06:48.101801 28053 net.cpp:380] InnerProduct1 -> InnerProduct1
I0307 17:06:48.102010 28053 net.cpp:122] Setting up InnerProduct1
I0307 17:06:48.102018 28053 net.cpp:129] Top shape: 100 10 (1000)
I0307 17:06:48.102022 28053 net.cpp:137] Memory required for data: 430520400
I0307 17:06:48.102031 28053 layer_factory.hpp:77] Creating layer InnerProduct1_InnerProduct1_0_split
I0307 17:06:48.102038 28053 net.cpp:84] Creating Layer InnerProduct1_InnerProduct1_0_split
I0307 17:06:48.102041 28053 net.cpp:406] InnerProduct1_InnerProduct1_0_split <- InnerProduct1
I0307 17:06:48.102051 28053 net.cpp:380] InnerProduct1_InnerProduct1_0_split -> InnerProduct1_InnerProduct1_0_split_0
I0307 17:06:48.102057 28053 net.cpp:380] InnerProduct1_InnerProduct1_0_split -> InnerProduct1_InnerProduct1_0_split_1
I0307 17:06:48.102113 28053 net.cpp:122] Setting up InnerProduct1_InnerProduct1_0_split
I0307 17:06:48.102120 28053 net.cpp:129] Top shape: 100 10 (1000)
I0307 17:06:48.102124 28053 net.cpp:129] Top shape: 100 10 (1000)
I0307 17:06:48.102139 28053 net.cpp:137] Memory required for data: 430528400
I0307 17:06:48.102144 28053 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I0307 17:06:48.102149 28053 net.cpp:84] Creating Layer SoftmaxWithLoss1
I0307 17:06:48.102152 28053 net.cpp:406] SoftmaxWithLoss1 <- InnerProduct1_InnerProduct1_0_split_0
I0307 17:06:48.102157 28053 net.cpp:406] SoftmaxWithLoss1 <- label_cifar_1_split_0
I0307 17:06:48.102164 28053 net.cpp:380] SoftmaxWithLoss1 -> SoftmaxWithLoss1
I0307 17:06:48.102172 28053 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I0307 17:06:48.102910 28053 net.cpp:122] Setting up SoftmaxWithLoss1
I0307 17:06:48.102923 28053 net.cpp:129] Top shape: (1)
I0307 17:06:48.102926 28053 net.cpp:132]     with loss weight 1
I0307 17:06:48.102934 28053 net.cpp:137] Memory required for data: 430528404
I0307 17:06:48.102938 28053 layer_factory.hpp:77] Creating layer Accuracy1
I0307 17:06:48.102946 28053 net.cpp:84] Creating Layer Accuracy1
I0307 17:06:48.102949 28053 net.cpp:406] Accuracy1 <- InnerProduct1_InnerProduct1_0_split_1
I0307 17:06:48.102957 28053 net.cpp:406] Accuracy1 <- label_cifar_1_split_1
I0307 17:06:48.102962 28053 net.cpp:380] Accuracy1 -> Accuracy1
I0307 17:06:48.102970 28053 net.cpp:122] Setting up Accuracy1
I0307 17:06:48.102974 28053 net.cpp:129] Top shape: (1)
I0307 17:06:48.102977 28053 net.cpp:137] Memory required for data: 430528408
I0307 17:06:48.102982 28053 net.cpp:200] Accuracy1 does not need backward computation.
I0307 17:06:48.102985 28053 net.cpp:198] SoftmaxWithLoss1 needs backward computation.
I0307 17:06:48.102990 28053 net.cpp:198] InnerProduct1_InnerProduct1_0_split needs backward computation.
I0307 17:06:48.102993 28053 net.cpp:198] InnerProduct1 needs backward computation.
I0307 17:06:48.102998 28053 net.cpp:198] Pooling1 needs backward computation.
I0307 17:06:48.103000 28053 net.cpp:198] ReLU19 needs backward computation.
I0307 17:06:48.103003 28053 net.cpp:198] Eltwise9 needs backward computation.
I0307 17:06:48.103008 28053 net.cpp:198] Scale21 needs backward computation.
I0307 17:06:48.103010 28053 net.cpp:198] BatchNorm21 needs backward computation.
I0307 17:06:48.103013 28053 net.cpp:198] Convolution21 needs backward computation.
I0307 17:06:48.103018 28053 net.cpp:198] ReLU18 needs backward computation.
I0307 17:06:48.103020 28053 net.cpp:198] Scale20 needs backward computation.
I0307 17:06:48.103024 28053 net.cpp:198] BatchNorm20 needs backward computation.
I0307 17:06:48.103026 28053 net.cpp:198] Convolution20 needs backward computation.
I0307 17:06:48.103030 28053 net.cpp:198] Eltwise8_ReLU17_0_split needs backward computation.
I0307 17:06:48.103034 28053 net.cpp:198] ReLU17 needs backward computation.
I0307 17:06:48.103039 28053 net.cpp:198] Eltwise8 needs backward computation.
I0307 17:06:48.103044 28053 net.cpp:198] Scale19 needs backward computation.
I0307 17:06:48.103046 28053 net.cpp:198] BatchNorm19 needs backward computation.
I0307 17:06:48.103049 28053 net.cpp:198] Convolution19 needs backward computation.
I0307 17:06:48.103052 28053 net.cpp:198] ReLU16 needs backward computation.
I0307 17:06:48.103055 28053 net.cpp:198] Scale18 needs backward computation.
I0307 17:06:48.103060 28053 net.cpp:198] BatchNorm18 needs backward computation.
I0307 17:06:48.103062 28053 net.cpp:198] Convolution18 needs backward computation.
I0307 17:06:48.103066 28053 net.cpp:198] Eltwise7_ReLU15_0_split needs backward computation.
I0307 17:06:48.103070 28053 net.cpp:198] ReLU15 needs backward computation.
I0307 17:06:48.103072 28053 net.cpp:198] Eltwise7 needs backward computation.
I0307 17:06:48.103076 28053 net.cpp:198] Scale17 needs backward computation.
I0307 17:06:48.103080 28053 net.cpp:198] BatchNorm17 needs backward computation.
I0307 17:06:48.103083 28053 net.cpp:198] Convolution17 needs backward computation.
I0307 17:06:48.103086 28053 net.cpp:198] ReLU14 needs backward computation.
I0307 17:06:48.103091 28053 net.cpp:198] Scale16 needs backward computation.
I0307 17:06:48.103093 28053 net.cpp:198] BatchNorm16 needs backward computation.
I0307 17:06:48.103096 28053 net.cpp:198] Convolution16 needs backward computation.
I0307 17:06:48.103111 28053 net.cpp:198] Scale15 needs backward computation.
I0307 17:06:48.103114 28053 net.cpp:198] BatchNorm15 needs backward computation.
I0307 17:06:48.103117 28053 net.cpp:198] Convolution15 needs backward computation.
I0307 17:06:48.103121 28053 net.cpp:198] Eltwise6_ReLU13_0_split needs backward computation.
I0307 17:06:48.103124 28053 net.cpp:198] ReLU13 needs backward computation.
I0307 17:06:48.103128 28053 net.cpp:198] Eltwise6 needs backward computation.
I0307 17:06:48.103132 28053 net.cpp:198] Scale14 needs backward computation.
I0307 17:06:48.103135 28053 net.cpp:198] BatchNorm14 needs backward computation.
I0307 17:06:48.103138 28053 net.cpp:198] Convolution14 needs backward computation.
I0307 17:06:48.103142 28053 net.cpp:198] ReLU12 needs backward computation.
I0307 17:06:48.103145 28053 net.cpp:198] Scale13 needs backward computation.
I0307 17:06:48.103148 28053 net.cpp:198] BatchNorm13 needs backward computation.
I0307 17:06:48.103152 28053 net.cpp:198] Convolution13 needs backward computation.
I0307 17:06:48.103155 28053 net.cpp:198] Eltwise5_ReLU11_0_split needs backward computation.
I0307 17:06:48.103159 28053 net.cpp:198] ReLU11 needs backward computation.
I0307 17:06:48.103163 28053 net.cpp:198] Eltwise5 needs backward computation.
I0307 17:06:48.103166 28053 net.cpp:198] Scale12 needs backward computation.
I0307 17:06:48.103169 28053 net.cpp:198] BatchNorm12 needs backward computation.
I0307 17:06:48.103173 28053 net.cpp:198] Convolution12 needs backward computation.
I0307 17:06:48.103175 28053 net.cpp:198] ReLU10 needs backward computation.
I0307 17:06:48.103178 28053 net.cpp:198] Scale11 needs backward computation.
I0307 17:06:48.103181 28053 net.cpp:198] BatchNorm11 needs backward computation.
I0307 17:06:48.103184 28053 net.cpp:198] Convolution11 needs backward computation.
I0307 17:06:48.103188 28053 net.cpp:198] Eltwise4_ReLU9_0_split needs backward computation.
I0307 17:06:48.103193 28053 net.cpp:198] ReLU9 needs backward computation.
I0307 17:06:48.103195 28053 net.cpp:198] Eltwise4 needs backward computation.
I0307 17:06:48.103200 28053 net.cpp:198] Scale10 needs backward computation.
I0307 17:06:48.103204 28053 net.cpp:198] BatchNorm10 needs backward computation.
I0307 17:06:48.103206 28053 net.cpp:198] Convolution10 needs backward computation.
I0307 17:06:48.103210 28053 net.cpp:198] ReLU8 needs backward computation.
I0307 17:06:48.103214 28053 net.cpp:198] Scale9 needs backward computation.
I0307 17:06:48.103217 28053 net.cpp:198] BatchNorm9 needs backward computation.
I0307 17:06:48.103220 28053 net.cpp:198] Convolution9 needs backward computation.
I0307 17:06:48.103225 28053 net.cpp:198] Scale8 needs backward computation.
I0307 17:06:48.103229 28053 net.cpp:198] BatchNorm8 needs backward computation.
I0307 17:06:48.103232 28053 net.cpp:198] Convolution8 needs backward computation.
I0307 17:06:48.103236 28053 net.cpp:198] Eltwise3_ReLU7_0_split needs backward computation.
I0307 17:06:48.103240 28053 net.cpp:198] ReLU7 needs backward computation.
I0307 17:06:48.103243 28053 net.cpp:198] Eltwise3 needs backward computation.
I0307 17:06:48.103247 28053 net.cpp:198] Scale7 needs backward computation.
I0307 17:06:48.103250 28053 net.cpp:198] BatchNorm7 needs backward computation.
I0307 17:06:48.103253 28053 net.cpp:198] Convolution7 needs backward computation.
I0307 17:06:48.103257 28053 net.cpp:198] ReLU6 needs backward computation.
I0307 17:06:48.103260 28053 net.cpp:198] Scale6 needs backward computation.
I0307 17:06:48.103263 28053 net.cpp:198] BatchNorm6 needs backward computation.
I0307 17:06:48.103266 28053 net.cpp:198] Convolution6 needs backward computation.
I0307 17:06:48.103271 28053 net.cpp:198] Eltwise2_ReLU5_0_split needs backward computation.
I0307 17:06:48.103273 28053 net.cpp:198] ReLU5 needs backward computation.
I0307 17:06:48.103277 28053 net.cpp:198] Eltwise2 needs backward computation.
I0307 17:06:48.103281 28053 net.cpp:198] Scale5 needs backward computation.
I0307 17:06:48.103284 28053 net.cpp:198] BatchNorm5 needs backward computation.
I0307 17:06:48.103293 28053 net.cpp:198] Convolution5 needs backward computation.
I0307 17:06:48.103296 28053 net.cpp:198] ReLU4 needs backward computation.
I0307 17:06:48.103299 28053 net.cpp:198] Scale4 needs backward computation.
I0307 17:06:48.103303 28053 net.cpp:198] BatchNorm4 needs backward computation.
I0307 17:06:48.103307 28053 net.cpp:198] Convolution4 needs backward computation.
I0307 17:06:48.103310 28053 net.cpp:198] Eltwise1_ReLU3_0_split needs backward computation.
I0307 17:06:48.103313 28053 net.cpp:198] ReLU3 needs backward computation.
I0307 17:06:48.103317 28053 net.cpp:198] Eltwise1 needs backward computation.
I0307 17:06:48.103322 28053 net.cpp:198] Scale3 needs backward computation.
I0307 17:06:48.103325 28053 net.cpp:198] BatchNorm3 needs backward computation.
I0307 17:06:48.103328 28053 net.cpp:198] Convolution3 needs backward computation.
I0307 17:06:48.103332 28053 net.cpp:198] ReLU2 needs backward computation.
I0307 17:06:48.103335 28053 net.cpp:198] Scale2 needs backward computation.
I0307 17:06:48.103338 28053 net.cpp:198] BatchNorm2 needs backward computation.
I0307 17:06:48.103341 28053 net.cpp:198] Convolution2 needs backward computation.
I0307 17:06:48.103345 28053 net.cpp:198] Convolution1_ReLU1_0_split needs backward computation.
I0307 17:06:48.103348 28053 net.cpp:198] ReLU1 needs backward computation.
I0307 17:06:48.103353 28053 net.cpp:198] Scale1 needs backward computation.
I0307 17:06:48.103356 28053 net.cpp:198] BatchNorm1 needs backward computation.
I0307 17:06:48.103360 28053 net.cpp:198] Convolution1 needs backward computation.
I0307 17:06:48.103363 28053 net.cpp:200] label_cifar_1_split does not need backward computation.
I0307 17:06:48.103368 28053 net.cpp:200] cifar does not need backward computation.
I0307 17:06:48.103371 28053 net.cpp:242] This network produces output Accuracy1
I0307 17:06:48.103375 28053 net.cpp:242] This network produces output SoftmaxWithLoss1
I0307 17:06:48.103431 28053 net.cpp:255] Network initialization done.
I0307 17:06:48.103684 28053 solver.cpp:57] Solver scaffolding done.
I0307 17:06:48.110683 28053 caffe.cpp:239] Starting Optimization
I0307 17:06:48.110695 28053 solver.cpp:289] Solving cifar10_resnet20
I0307 17:06:48.110698 28053 solver.cpp:290] Learning Rate Policy: multistep
I0307 17:06:48.113929 28053 solver.cpp:347] Iteration 0, Testing net (#0)
I0307 17:06:49.311519 28053 solver.cpp:414]     Test net output #0: Accuracy1 = 1
I0307 17:06:49.311552 28053 solver.cpp:414]     Test net output #1: SoftmaxWithLoss1 = 87.3365 (* 1 = 87.3365 loss)
I0307 17:06:49.415422 28053 solver.cpp:239] Iteration 0 (1.69146e-34 iter/s, 1.30461s/100 iters), loss = 3.72914
I0307 17:06:49.415459 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 3.72914 (* 1 = 3.72914 loss)
I0307 17:06:49.415477 28053 sgd_solver.cpp:112] Iteration 0, lr = 0.1
I0307 17:06:57.535553 28053 solver.cpp:239] Iteration 100 (12.3155 iter/s, 8.11984s/100 iters), loss = 1.74731
I0307 17:06:57.535590 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 1.74731 (* 1 = 1.74731 loss)
I0307 17:06:57.535598 28053 sgd_solver.cpp:112] Iteration 100, lr = 0.1
I0307 17:07:05.635843 28053 solver.cpp:239] Iteration 200 (12.3457 iter/s, 8.1s/100 iters), loss = 1.55182
I0307 17:07:05.635884 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 1.55182 (* 1 = 1.55182 loss)
I0307 17:07:05.635890 28053 sgd_solver.cpp:112] Iteration 200, lr = 0.1
I0307 17:07:13.747715 28053 solver.cpp:239] Iteration 300 (12.3281 iter/s, 8.11158s/100 iters), loss = 1.31692
I0307 17:07:13.747754 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 1.31692 (* 1 = 1.31692 loss)
I0307 17:07:13.747761 28053 sgd_solver.cpp:112] Iteration 300, lr = 0.1
I0307 17:07:20.731365 28062 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:07:21.865469 28053 solver.cpp:239] Iteration 400 (12.3191 iter/s, 8.11746s/100 iters), loss = 1.21894
I0307 17:07:21.865517 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 1.21894 (* 1 = 1.21894 loss)
I0307 17:07:21.865527 28053 sgd_solver.cpp:112] Iteration 400, lr = 0.1
I0307 17:07:29.901490 28053 solver.cpp:347] Iteration 500, Testing net (#0)
I0307 17:07:31.013568 28053 solver.cpp:414]     Test net output #0: Accuracy1 = 0.3347
I0307 17:07:31.013598 28053 solver.cpp:414]     Test net output #1: SoftmaxWithLoss1 = 2.12805 (* 1 = 2.12805 loss)
I0307 17:07:31.094240 28053 solver.cpp:239] Iteration 500 (10.8361 iter/s, 9.22844s/100 iters), loss = 1.0362
I0307 17:07:31.094275 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 1.0362 (* 1 = 1.0362 loss)
I0307 17:07:31.094283 28053 sgd_solver.cpp:112] Iteration 500, lr = 0.1
I0307 17:07:39.210703 28053 solver.cpp:239] Iteration 600 (12.3211 iter/s, 8.11617s/100 iters), loss = 1.03624
I0307 17:07:39.210741 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 1.03624 (* 1 = 1.03624 loss)
I0307 17:07:39.210748 28053 sgd_solver.cpp:112] Iteration 600, lr = 0.1
I0307 17:07:47.325323 28053 solver.cpp:239] Iteration 700 (12.3239 iter/s, 8.11433s/100 iters), loss = 0.840299
I0307 17:07:47.325362 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.840299 (* 1 = 0.840299 loss)
I0307 17:07:47.325369 28053 sgd_solver.cpp:112] Iteration 700, lr = 0.1
I0307 17:07:53.577028 28062 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:07:55.443439 28053 solver.cpp:239] Iteration 800 (12.3186 iter/s, 8.11782s/100 iters), loss = 0.70265
I0307 17:07:55.443477 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.70265 (* 1 = 0.70265 loss)
I0307 17:07:55.443485 28053 sgd_solver.cpp:112] Iteration 800, lr = 0.1
I0307 17:08:03.564877 28053 solver.cpp:239] Iteration 900 (12.3135 iter/s, 8.12114s/100 iters), loss = 0.830889
I0307 17:08:03.564916 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.830889 (* 1 = 0.830889 loss)
I0307 17:08:03.564924 28053 sgd_solver.cpp:112] Iteration 900, lr = 0.1
I0307 17:08:11.616761 28053 solver.cpp:347] Iteration 1000, Testing net (#0)
I0307 17:08:12.735687 28053 solver.cpp:414]     Test net output #0: Accuracy1 = 0.4877
I0307 17:08:12.735721 28053 solver.cpp:414]     Test net output #1: SoftmaxWithLoss1 = 1.62281 (* 1 = 1.62281 loss)
I0307 17:08:12.810389 28053 solver.cpp:239] Iteration 1000 (10.8164 iter/s, 9.24518s/100 iters), loss = 0.764528
I0307 17:08:12.810428 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.764528 (* 1 = 0.764528 loss)
I0307 17:08:12.810437 28053 sgd_solver.cpp:112] Iteration 1000, lr = 0.1
I0307 17:08:20.941715 28053 solver.cpp:239] Iteration 1100 (12.2986 iter/s, 8.13103s/100 iters), loss = 0.848317
I0307 17:08:20.941754 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.848317 (* 1 = 0.848317 loss)
I0307 17:08:20.941761 28053 sgd_solver.cpp:112] Iteration 1100, lr = 0.1
I0307 17:08:26.390297 28062 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:08:29.062757 28053 solver.cpp:239] Iteration 1200 (12.3141 iter/s, 8.12074s/100 iters), loss = 0.649904
I0307 17:08:29.062798 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.649904 (* 1 = 0.649904 loss)
I0307 17:08:29.062804 28053 sgd_solver.cpp:112] Iteration 1200, lr = 0.1
I0307 17:08:37.186156 28053 solver.cpp:239] Iteration 1300 (12.3106 iter/s, 8.1231s/100 iters), loss = 0.805327
I0307 17:08:37.186194 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.805327 (* 1 = 0.805327 loss)
I0307 17:08:37.186201 28053 sgd_solver.cpp:112] Iteration 1300, lr = 0.1
I0307 17:08:45.313475 28053 solver.cpp:239] Iteration 1400 (12.3046 iter/s, 8.12702s/100 iters), loss = 0.641292
I0307 17:08:45.313514 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.641292 (* 1 = 0.641292 loss)
I0307 17:08:45.313522 28053 sgd_solver.cpp:112] Iteration 1400, lr = 0.1
I0307 17:08:53.357626 28053 solver.cpp:347] Iteration 1500, Testing net (#0)
I0307 17:08:54.473915 28053 solver.cpp:414]     Test net output #0: Accuracy1 = 0.4944
I0307 17:08:54.473950 28053 solver.cpp:414]     Test net output #1: SoftmaxWithLoss1 = 1.66966 (* 1 = 1.66966 loss)
I0307 17:08:54.546983 28053 solver.cpp:239] Iteration 1500 (10.8305 iter/s, 9.23317s/100 iters), loss = 0.619061
I0307 17:08:54.547024 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.619061 (* 1 = 0.619061 loss)
I0307 17:08:54.547034 28053 sgd_solver.cpp:112] Iteration 1500, lr = 0.1
I0307 17:08:59.266108 28062 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:09:02.679837 28053 solver.cpp:239] Iteration 1600 (12.2963 iter/s, 8.13255s/100 iters), loss = 0.764987
I0307 17:09:02.679874 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.764987 (* 1 = 0.764987 loss)
I0307 17:09:02.679881 28053 sgd_solver.cpp:112] Iteration 1600, lr = 0.1
I0307 17:09:10.812005 28053 solver.cpp:239] Iteration 1700 (12.2973 iter/s, 8.13187s/100 iters), loss = 0.57841
I0307 17:09:10.812041 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.57841 (* 1 = 0.57841 loss)
I0307 17:09:10.812048 28053 sgd_solver.cpp:112] Iteration 1700, lr = 0.1
I0307 17:09:18.942121 28053 solver.cpp:239] Iteration 1800 (12.3004 iter/s, 8.12982s/100 iters), loss = 0.572155
I0307 17:09:18.942159 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.572155 (* 1 = 0.572155 loss)
I0307 17:09:18.942167 28053 sgd_solver.cpp:112] Iteration 1800, lr = 0.1
I0307 17:09:27.071696 28053 solver.cpp:239] Iteration 1900 (12.3012 iter/s, 8.12928s/100 iters), loss = 0.624717
I0307 17:09:27.071733 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.624717 (* 1 = 0.624717 loss)
I0307 17:09:27.071740 28053 sgd_solver.cpp:112] Iteration 1900, lr = 0.1
I0307 17:09:31.060099 28062 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:09:35.126886 28053 solver.cpp:347] Iteration 2000, Testing net (#0)
I0307 17:09:36.199298 28064 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:09:36.242666 28053 solver.cpp:414]     Test net output #0: Accuracy1 = 0.5962
I0307 17:09:36.242694 28053 solver.cpp:414]     Test net output #1: SoftmaxWithLoss1 = 1.16955 (* 1 = 1.16955 loss)
I0307 17:09:36.323745 28053 solver.cpp:239] Iteration 2000 (10.8088 iter/s, 9.25172s/100 iters), loss = 0.620649
I0307 17:09:36.323783 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.620649 (* 1 = 0.620649 loss)
I0307 17:09:36.323791 28053 sgd_solver.cpp:112] Iteration 2000, lr = 0.1
I0307 17:09:44.458603 28053 solver.cpp:239] Iteration 2100 (12.2932 iter/s, 8.13456s/100 iters), loss = 0.414789
I0307 17:09:44.458639 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.414789 (* 1 = 0.414789 loss)
I0307 17:09:44.458647 28053 sgd_solver.cpp:112] Iteration 2100, lr = 0.1
I0307 17:09:52.592010 28053 solver.cpp:239] Iteration 2200 (12.2954 iter/s, 8.13311s/100 iters), loss = 0.517632
I0307 17:09:52.592046 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.517632 (* 1 = 0.517632 loss)
I0307 17:09:52.592053 28053 sgd_solver.cpp:112] Iteration 2200, lr = 0.1
I0307 17:10:00.724613 28053 solver.cpp:239] Iteration 2300 (12.2966 iter/s, 8.13231s/100 iters), loss = 0.5179
I0307 17:10:00.724651 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.5179 (* 1 = 0.5179 loss)
I0307 17:10:00.724658 28053 sgd_solver.cpp:112] Iteration 2300, lr = 0.1
I0307 17:10:03.902635 28062 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:10:08.864811 28053 solver.cpp:239] Iteration 2400 (12.2852 iter/s, 8.1399s/100 iters), loss = 0.435752
I0307 17:10:08.864848 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.435752 (* 1 = 0.435752 loss)
I0307 17:10:08.864856 28053 sgd_solver.cpp:112] Iteration 2400, lr = 0.1
I0307 17:10:16.918043 28053 solver.cpp:347] Iteration 2500, Testing net (#0)
I0307 17:10:18.035128 28053 solver.cpp:414]     Test net output #0: Accuracy1 = 0.6038
I0307 17:10:18.035162 28053 solver.cpp:414]     Test net output #1: SoftmaxWithLoss1 = 1.11787 (* 1 = 1.11787 loss)
I0307 17:10:18.115850 28053 solver.cpp:239] Iteration 2500 (10.81 iter/s, 9.25071s/100 iters), loss = 0.543677
I0307 17:10:18.115888 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.543677 (* 1 = 0.543677 loss)
I0307 17:10:18.115896 28053 sgd_solver.cpp:112] Iteration 2500, lr = 0.1
I0307 17:10:26.268126 28053 solver.cpp:239] Iteration 2600 (12.267 iter/s, 8.15198s/100 iters), loss = 0.625544
I0307 17:10:26.268163 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.625544 (* 1 = 0.625544 loss)
I0307 17:10:26.268170 28053 sgd_solver.cpp:112] Iteration 2600, lr = 0.1
I0307 17:10:34.419190 28053 solver.cpp:239] Iteration 2700 (12.2688 iter/s, 8.15077s/100 iters), loss = 0.45034
I0307 17:10:34.419315 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.45034 (* 1 = 0.45034 loss)
I0307 17:10:34.419325 28053 sgd_solver.cpp:112] Iteration 2700, lr = 0.1
I0307 17:10:36.869451 28062 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:10:42.573846 28053 solver.cpp:239] Iteration 2800 (12.2635 iter/s, 8.15427s/100 iters), loss = 0.43087
I0307 17:10:42.573882 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.43087 (* 1 = 0.43087 loss)
I0307 17:10:42.573889 28053 sgd_solver.cpp:112] Iteration 2800, lr = 0.1
I0307 17:10:50.724808 28053 solver.cpp:239] Iteration 2900 (12.2689 iter/s, 8.15066s/100 iters), loss = 0.538613
I0307 17:10:50.724845 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.538613 (* 1 = 0.538613 loss)
I0307 17:10:50.724853 28053 sgd_solver.cpp:112] Iteration 2900, lr = 0.1
I0307 17:10:58.794319 28053 solver.cpp:347] Iteration 3000, Testing net (#0)
I0307 17:10:59.911741 28053 solver.cpp:414]     Test net output #0: Accuracy1 = 0.5732
I0307 17:10:59.911773 28053 solver.cpp:414]     Test net output #1: SoftmaxWithLoss1 = 1.32462 (* 1 = 1.32462 loss)
I0307 17:10:59.992717 28053 solver.cpp:239] Iteration 3000 (10.7903 iter/s, 9.26758s/100 iters), loss = 0.502166
I0307 17:10:59.992753 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.502166 (* 1 = 0.502166 loss)
I0307 17:10:59.992763 28053 sgd_solver.cpp:112] Iteration 3000, lr = 0.1
I0307 17:11:08.142796 28053 solver.cpp:239] Iteration 3100 (12.2703 iter/s, 8.14978s/100 iters), loss = 0.492758
I0307 17:11:08.142877 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.492758 (* 1 = 0.492758 loss)
I0307 17:11:08.142886 28053 sgd_solver.cpp:112] Iteration 3100, lr = 0.1
I0307 17:11:09.777082 28062 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:11:16.290808 28053 solver.cpp:239] Iteration 3200 (12.2734 iter/s, 8.14767s/100 iters), loss = 0.388677
I0307 17:11:16.290845 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.388677 (* 1 = 0.388677 loss)
I0307 17:11:16.290853 28053 sgd_solver.cpp:112] Iteration 3200, lr = 0.1
I0307 17:11:24.434113 28053 solver.cpp:239] Iteration 3300 (12.2805 iter/s, 8.14301s/100 iters), loss = 0.332858
I0307 17:11:24.434150 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.332858 (* 1 = 0.332858 loss)
I0307 17:11:24.434157 28053 sgd_solver.cpp:112] Iteration 3300, lr = 0.1
I0307 17:11:32.579301 28053 solver.cpp:239] Iteration 3400 (12.2776 iter/s, 8.14489s/100 iters), loss = 0.406071
I0307 17:11:32.579339 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.406071 (* 1 = 0.406071 loss)
I0307 17:11:32.579346 28053 sgd_solver.cpp:112] Iteration 3400, lr = 0.1
I0307 17:11:40.641316 28053 solver.cpp:347] Iteration 3500, Testing net (#0)
I0307 17:11:41.756986 28053 solver.cpp:414]     Test net output #0: Accuracy1 = 0.7029
I0307 17:11:41.757020 28053 solver.cpp:414]     Test net output #1: SoftmaxWithLoss1 = 0.848863 (* 1 = 0.848863 loss)
I0307 17:11:41.837594 28053 solver.cpp:239] Iteration 3500 (10.8015 iter/s, 9.25796s/100 iters), loss = 0.380936
I0307 17:11:41.837631 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.380936 (* 1 = 0.380936 loss)
I0307 17:11:41.837641 28053 sgd_solver.cpp:112] Iteration 3500, lr = 0.1
I0307 17:11:42.740600 28062 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:11:49.984964 28053 solver.cpp:239] Iteration 3600 (12.2743 iter/s, 8.14707s/100 iters), loss = 0.3881
I0307 17:11:49.985002 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.3881 (* 1 = 0.3881 loss)
I0307 17:11:49.985008 28053 sgd_solver.cpp:112] Iteration 3600, lr = 0.1
I0307 17:11:58.125933 28053 solver.cpp:239] Iteration 3700 (12.284 iter/s, 8.14067s/100 iters), loss = 0.294601
I0307 17:11:58.125972 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.294601 (* 1 = 0.294601 loss)
I0307 17:11:58.125979 28053 sgd_solver.cpp:112] Iteration 3700, lr = 0.1
I0307 17:12:06.265580 28053 solver.cpp:239] Iteration 3800 (12.286 iter/s, 8.13935s/100 iters), loss = 0.476929
I0307 17:12:06.265619 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.476929 (* 1 = 0.476929 loss)
I0307 17:12:06.265627 28053 sgd_solver.cpp:112] Iteration 3800, lr = 0.1
I0307 17:12:14.405061 28053 solver.cpp:239] Iteration 3900 (12.2862 iter/s, 8.13918s/100 iters), loss = 0.357801
I0307 17:12:14.405179 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.357801 (* 1 = 0.357801 loss)
I0307 17:12:14.405189 28053 sgd_solver.cpp:112] Iteration 3900, lr = 0.1
I0307 17:12:14.571388 28062 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:12:22.465648 28053 solver.cpp:347] Iteration 4000, Testing net (#0)
I0307 17:12:23.584105 28053 solver.cpp:414]     Test net output #0: Accuracy1 = 0.712
I0307 17:12:23.584137 28053 solver.cpp:414]     Test net output #1: SoftmaxWithLoss1 = 0.834274 (* 1 = 0.834274 loss)
I0307 17:12:23.664552 28053 solver.cpp:239] Iteration 4000 (10.8002 iter/s, 9.25908s/100 iters), loss = 0.292343
I0307 17:12:23.664594 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.292343 (* 1 = 0.292343 loss)
I0307 17:12:23.664602 28053 sgd_solver.cpp:112] Iteration 4000, lr = 0.1
I0307 17:12:31.816361 28053 solver.cpp:239] Iteration 4100 (12.2677 iter/s, 8.15151s/100 iters), loss = 0.421368
I0307 17:12:31.816398 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.421368 (* 1 = 0.421368 loss)
I0307 17:12:31.816406 28053 sgd_solver.cpp:112] Iteration 4100, lr = 0.1
I0307 17:12:39.961925 28053 solver.cpp:239] Iteration 4200 (12.2771 iter/s, 8.14527s/100 iters), loss = 0.357167
I0307 17:12:39.961962 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.357167 (* 1 = 0.357167 loss)
I0307 17:12:39.961969 28053 sgd_solver.cpp:112] Iteration 4200, lr = 0.1
I0307 17:12:47.458350 28062 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:12:48.107051 28053 solver.cpp:239] Iteration 4300 (12.2777 iter/s, 8.14483s/100 iters), loss = 0.369025
I0307 17:12:48.107089 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.369025 (* 1 = 0.369025 loss)
I0307 17:12:48.107095 28053 sgd_solver.cpp:112] Iteration 4300, lr = 0.1
I0307 17:12:56.252007 28053 solver.cpp:239] Iteration 4400 (12.278 iter/s, 8.14466s/100 iters), loss = 0.45015
I0307 17:12:56.252045 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.45015 (* 1 = 0.45015 loss)
I0307 17:12:56.252053 28053 sgd_solver.cpp:112] Iteration 4400, lr = 0.1
I0307 17:13:04.319501 28053 solver.cpp:347] Iteration 4500, Testing net (#0)
I0307 17:13:05.398663 28064 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:13:05.442873 28053 solver.cpp:414]     Test net output #0: Accuracy1 = 0.7148
I0307 17:13:05.442904 28053 solver.cpp:414]     Test net output #1: SoftmaxWithLoss1 = 0.855017 (* 1 = 0.855017 loss)
I0307 17:13:05.524088 28053 solver.cpp:239] Iteration 4500 (10.7854 iter/s, 9.27175s/100 iters), loss = 0.268174
I0307 17:13:05.524127 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.268174 (* 1 = 0.268174 loss)
I0307 17:13:05.524135 28053 sgd_solver.cpp:112] Iteration 4500, lr = 0.1
I0307 17:13:13.678376 28053 solver.cpp:239] Iteration 4600 (12.2639 iter/s, 8.15399s/100 iters), loss = 0.277744
I0307 17:13:13.678413 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.277744 (* 1 = 0.277744 loss)
I0307 17:13:13.678421 28053 sgd_solver.cpp:112] Iteration 4600, lr = 0.1
I0307 17:13:20.447254 28062 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:13:21.831456 28053 solver.cpp:239] Iteration 4700 (12.2657 iter/s, 8.15278s/100 iters), loss = 0.311344
I0307 17:13:21.831498 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.311344 (* 1 = 0.311344 loss)
I0307 17:13:21.831506 28053 sgd_solver.cpp:112] Iteration 4700, lr = 0.1
I0307 17:13:29.986652 28053 solver.cpp:239] Iteration 4800 (12.2627 iter/s, 8.15484s/100 iters), loss = 0.362194
I0307 17:13:29.986690 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.362194 (* 1 = 0.362194 loss)
I0307 17:13:29.986696 28053 sgd_solver.cpp:112] Iteration 4800, lr = 0.1
I0307 17:13:38.135413 28053 solver.cpp:239] Iteration 4900 (12.2722 iter/s, 8.14847s/100 iters), loss = 0.340152
I0307 17:13:38.135450 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.340152 (* 1 = 0.340152 loss)
I0307 17:13:38.135457 28053 sgd_solver.cpp:112] Iteration 4900, lr = 0.1
I0307 17:13:46.204964 28053 solver.cpp:347] Iteration 5000, Testing net (#0)
I0307 17:13:47.323938 28053 solver.cpp:414]     Test net output #0: Accuracy1 = 0.7675
I0307 17:13:47.323971 28053 solver.cpp:414]     Test net output #1: SoftmaxWithLoss1 = 0.687872 (* 1 = 0.687872 loss)
I0307 17:13:47.404525 28053 solver.cpp:239] Iteration 5000 (10.7889 iter/s, 9.26878s/100 iters), loss = 0.297144
I0307 17:13:47.404564 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.297144 (* 1 = 0.297144 loss)
I0307 17:13:47.404572 28053 sgd_solver.cpp:112] Iteration 5000, lr = 0.1
I0307 17:13:53.440382 28062 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:13:55.557909 28053 solver.cpp:239] Iteration 5100 (12.2653 iter/s, 8.15309s/100 iters), loss = 0.249426
I0307 17:13:55.557947 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.249426 (* 1 = 0.249426 loss)
I0307 17:13:55.557955 28053 sgd_solver.cpp:112] Iteration 5100, lr = 0.1
I0307 17:14:03.706382 28053 solver.cpp:239] Iteration 5200 (12.2727 iter/s, 8.14818s/100 iters), loss = 0.537431
I0307 17:14:03.706420 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.537431 (* 1 = 0.537431 loss)
I0307 17:14:03.706427 28053 sgd_solver.cpp:112] Iteration 5200, lr = 0.1
I0307 17:14:11.853837 28053 solver.cpp:239] Iteration 5300 (12.2742 iter/s, 8.14716s/100 iters), loss = 0.475885
I0307 17:14:11.853876 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.475885 (* 1 = 0.475885 loss)
I0307 17:14:11.853883 28053 sgd_solver.cpp:112] Iteration 5300, lr = 0.1
I0307 17:14:20.002265 28053 solver.cpp:239] Iteration 5400 (12.2727 iter/s, 8.14813s/100 iters), loss = 0.23768
I0307 17:14:20.002303 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.237679 (* 1 = 0.237679 loss)
I0307 17:14:20.002310 28053 sgd_solver.cpp:112] Iteration 5400, lr = 0.1
I0307 17:14:25.223093 28062 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:14:28.074265 28053 solver.cpp:347] Iteration 5500, Testing net (#0)
I0307 17:14:29.191112 28053 solver.cpp:414]     Test net output #0: Accuracy1 = 0.6985
I0307 17:14:29.191144 28053 solver.cpp:414]     Test net output #1: SoftmaxWithLoss1 = 0.999857 (* 1 = 0.999857 loss)
I0307 17:14:29.271811 28053 solver.cpp:239] Iteration 5500 (10.7884 iter/s, 9.26922s/100 iters), loss = 0.329293
I0307 17:14:29.271847 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.329293 (* 1 = 0.329293 loss)
I0307 17:14:29.271857 28053 sgd_solver.cpp:112] Iteration 5500, lr = 0.1
I0307 17:14:37.418642 28053 solver.cpp:239] Iteration 5600 (12.2752 iter/s, 8.14654s/100 iters), loss = 0.299093
I0307 17:14:37.418679 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.299092 (* 1 = 0.299092 loss)
I0307 17:14:37.418687 28053 sgd_solver.cpp:112] Iteration 5600, lr = 0.1
I0307 17:14:45.566645 28053 solver.cpp:239] Iteration 5700 (12.2734 iter/s, 8.14771s/100 iters), loss = 0.319698
I0307 17:14:45.566684 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.319698 (* 1 = 0.319698 loss)
I0307 17:14:45.566690 28053 sgd_solver.cpp:112] Iteration 5700, lr = 0.1
I0307 17:14:53.706251 28053 solver.cpp:239] Iteration 5800 (12.2861 iter/s, 8.13931s/100 iters), loss = 0.290374
I0307 17:14:53.706292 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.290374 (* 1 = 0.290374 loss)
I0307 17:14:53.706300 28053 sgd_solver.cpp:112] Iteration 5800, lr = 0.1
I0307 17:14:58.190788 28062 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:15:01.855913 28053 solver.cpp:239] Iteration 5900 (12.2709 iter/s, 8.14937s/100 iters), loss = 0.289606
I0307 17:15:01.855952 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.289606 (* 1 = 0.289606 loss)
I0307 17:15:01.855958 28053 sgd_solver.cpp:112] Iteration 5900, lr = 0.1
I0307 17:15:09.920800 28053 solver.cpp:347] Iteration 6000, Testing net (#0)
I0307 17:15:11.036929 28053 solver.cpp:414]     Test net output #0: Accuracy1 = 0.7904
I0307 17:15:11.036962 28053 solver.cpp:414]     Test net output #1: SoftmaxWithLoss1 = 0.617195 (* 1 = 0.617195 loss)
I0307 17:15:11.117880 28053 solver.cpp:239] Iteration 6000 (10.7972 iter/s, 9.26164s/100 iters), loss = 0.362037
I0307 17:15:11.117919 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.362037 (* 1 = 0.362037 loss)
I0307 17:15:11.117928 28053 sgd_solver.cpp:112] Iteration 6000, lr = 0.1
I0307 17:15:19.265292 28053 solver.cpp:239] Iteration 6100 (12.2743 iter/s, 8.14712s/100 iters), loss = 0.431187
I0307 17:15:19.265331 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.431187 (* 1 = 0.431187 loss)
I0307 17:15:19.265338 28053 sgd_solver.cpp:112] Iteration 6100, lr = 0.1
I0307 17:15:27.409968 28053 solver.cpp:239] Iteration 6200 (12.2784 iter/s, 8.14438s/100 iters), loss = 0.276284
I0307 17:15:27.410006 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.276284 (* 1 = 0.276284 loss)
I0307 17:15:27.410013 28053 sgd_solver.cpp:112] Iteration 6200, lr = 0.1
I0307 17:15:31.080226 28062 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:15:35.556881 28053 solver.cpp:239] Iteration 6300 (12.275 iter/s, 8.14662s/100 iters), loss = 0.309863
I0307 17:15:35.556921 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.309863 (* 1 = 0.309863 loss)
I0307 17:15:35.556927 28053 sgd_solver.cpp:112] Iteration 6300, lr = 0.1
I0307 17:15:43.702721 28053 solver.cpp:239] Iteration 6400 (12.2766 iter/s, 8.14555s/100 iters), loss = 0.229753
I0307 17:15:43.702760 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.229753 (* 1 = 0.229753 loss)
I0307 17:15:43.702767 28053 sgd_solver.cpp:112] Iteration 6400, lr = 0.1
I0307 17:15:51.767072 28053 solver.cpp:347] Iteration 6500, Testing net (#0)
I0307 17:15:52.885814 28053 solver.cpp:414]     Test net output #0: Accuracy1 = 0.8221
I0307 17:15:52.885850 28053 solver.cpp:414]     Test net output #1: SoftmaxWithLoss1 = 0.50731 (* 1 = 0.50731 loss)
I0307 17:15:52.966112 28053 solver.cpp:239] Iteration 6500 (10.7956 iter/s, 9.26306s/100 iters), loss = 0.355428
I0307 17:15:52.966151 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.355428 (* 1 = 0.355428 loss)
I0307 17:15:52.966159 28053 sgd_solver.cpp:112] Iteration 6500, lr = 0.1
I0307 17:16:01.113406 28053 solver.cpp:239] Iteration 6600 (12.2745 iter/s, 8.147s/100 iters), loss = 0.347037
I0307 17:16:01.113487 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.347037 (* 1 = 0.347037 loss)
I0307 17:16:01.113497 28053 sgd_solver.cpp:112] Iteration 6600, lr = 0.1
I0307 17:16:04.049419 28062 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:16:09.252610 28053 solver.cpp:239] Iteration 6700 (12.2867 iter/s, 8.13887s/100 iters), loss = 0.243626
I0307 17:16:09.252648 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.243626 (* 1 = 0.243626 loss)
I0307 17:16:09.252655 28053 sgd_solver.cpp:112] Iteration 6700, lr = 0.1
I0307 17:16:17.399824 28053 solver.cpp:239] Iteration 6800 (12.2746 iter/s, 8.14692s/100 iters), loss = 0.359331
I0307 17:16:17.399863 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.359331 (* 1 = 0.359331 loss)
I0307 17:16:17.399870 28053 sgd_solver.cpp:112] Iteration 6800, lr = 0.1
I0307 17:16:25.544863 28053 solver.cpp:239] Iteration 6900 (12.2779 iter/s, 8.14475s/100 iters), loss = 0.340128
I0307 17:16:25.544900 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.340128 (* 1 = 0.340128 loss)
I0307 17:16:25.544909 28053 sgd_solver.cpp:112] Iteration 6900, lr = 0.1
I0307 17:16:33.609100 28053 solver.cpp:347] Iteration 7000, Testing net (#0)
I0307 17:16:34.683322 28064 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:16:34.727007 28053 solver.cpp:414]     Test net output #0: Accuracy1 = 0.7744
I0307 17:16:34.727043 28053 solver.cpp:414]     Test net output #1: SoftmaxWithLoss1 = 0.73806 (* 1 = 0.73806 loss)
I0307 17:16:34.808060 28053 solver.cpp:239] Iteration 7000 (10.7958 iter/s, 9.26287s/100 iters), loss = 0.315801
I0307 17:16:34.808100 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.315801 (* 1 = 0.315801 loss)
I0307 17:16:34.808109 28053 sgd_solver.cpp:112] Iteration 7000, lr = 0.1
I0307 17:16:37.011633 28062 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:16:42.954774 28053 solver.cpp:239] Iteration 7100 (12.2753 iter/s, 8.14642s/100 iters), loss = 0.250096
I0307 17:16:42.954810 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.250096 (* 1 = 0.250096 loss)
I0307 17:16:42.954818 28053 sgd_solver.cpp:112] Iteration 7100, lr = 0.1
I0307 17:16:51.096911 28053 solver.cpp:239] Iteration 7200 (12.2822 iter/s, 8.14185s/100 iters), loss = 0.306407
I0307 17:16:51.096948 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.306407 (* 1 = 0.306407 loss)
I0307 17:16:51.096956 28053 sgd_solver.cpp:112] Iteration 7200, lr = 0.1
I0307 17:16:59.241034 28053 solver.cpp:239] Iteration 7300 (12.2792 iter/s, 8.14383s/100 iters), loss = 0.221017
I0307 17:16:59.241071 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.221017 (* 1 = 0.221017 loss)
I0307 17:16:59.241078 28053 sgd_solver.cpp:112] Iteration 7300, lr = 0.1
I0307 17:17:07.390107 28053 solver.cpp:239] Iteration 7400 (12.2718 iter/s, 8.14878s/100 iters), loss = 0.297407
I0307 17:17:07.390189 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.297407 (* 1 = 0.297407 loss)
I0307 17:17:07.390199 28053 sgd_solver.cpp:112] Iteration 7400, lr = 0.1
I0307 17:17:08.781603 28062 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:17:15.461809 28053 solver.cpp:347] Iteration 7500, Testing net (#0)
I0307 17:17:16.581421 28053 solver.cpp:414]     Test net output #0: Accuracy1 = 0.7878
I0307 17:17:16.581451 28053 solver.cpp:414]     Test net output #1: SoftmaxWithLoss1 = 0.694859 (* 1 = 0.694859 loss)
I0307 17:17:16.662269 28053 solver.cpp:239] Iteration 7500 (10.7854 iter/s, 9.2718s/100 iters), loss = 0.256914
I0307 17:17:16.662305 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.256914 (* 1 = 0.256914 loss)
I0307 17:17:16.662313 28053 sgd_solver.cpp:112] Iteration 7500, lr = 0.1
I0307 17:17:24.808959 28053 solver.cpp:239] Iteration 7600 (12.2754 iter/s, 8.1464s/100 iters), loss = 0.325116
I0307 17:17:24.808995 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.325116 (* 1 = 0.325116 loss)
I0307 17:17:24.809003 28053 sgd_solver.cpp:112] Iteration 7600, lr = 0.1
I0307 17:17:32.958362 28053 solver.cpp:239] Iteration 7700 (12.2713 iter/s, 8.14911s/100 iters), loss = 0.217735
I0307 17:17:32.958398 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.217735 (* 1 = 0.217735 loss)
I0307 17:17:32.958405 28053 sgd_solver.cpp:112] Iteration 7700, lr = 0.1
I0307 17:17:41.104266 28053 solver.cpp:239] Iteration 7800 (12.2765 iter/s, 8.14561s/100 iters), loss = 0.202128
I0307 17:17:41.104344 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.202128 (* 1 = 0.202128 loss)
I0307 17:17:41.104353 28053 sgd_solver.cpp:112] Iteration 7800, lr = 0.1
I0307 17:17:41.760116 28062 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:17:49.250790 28053 solver.cpp:239] Iteration 7900 (12.2757 iter/s, 8.14619s/100 iters), loss = 0.309053
I0307 17:17:49.250828 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.309053 (* 1 = 0.309053 loss)
I0307 17:17:49.250835 28053 sgd_solver.cpp:112] Iteration 7900, lr = 0.1
I0307 17:17:57.314990 28053 solver.cpp:347] Iteration 8000, Testing net (#0)
I0307 17:17:58.435621 28053 solver.cpp:414]     Test net output #0: Accuracy1 = 0.8308
I0307 17:17:58.435652 28053 solver.cpp:414]     Test net output #1: SoftmaxWithLoss1 = 0.481411 (* 1 = 0.481411 loss)
I0307 17:17:58.516939 28053 solver.cpp:239] Iteration 8000 (10.7923 iter/s, 9.26582s/100 iters), loss = 0.205088
I0307 17:17:58.516973 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.205088 (* 1 = 0.205088 loss)
I0307 17:17:58.516981 28053 sgd_solver.cpp:112] Iteration 8000, lr = 0.1
I0307 17:18:06.664384 28053 solver.cpp:239] Iteration 8100 (12.2742 iter/s, 8.14716s/100 iters), loss = 0.25519
I0307 17:18:06.664422 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.25519 (* 1 = 0.25519 loss)
I0307 17:18:06.664429 28053 sgd_solver.cpp:112] Iteration 8100, lr = 0.1
I0307 17:18:14.732228 28062 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:18:14.812561 28053 solver.cpp:239] Iteration 8200 (12.2731 iter/s, 8.14788s/100 iters), loss = 0.320507
I0307 17:18:14.812598 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.320507 (* 1 = 0.320507 loss)
I0307 17:18:14.812606 28053 sgd_solver.cpp:112] Iteration 8200, lr = 0.1
I0307 17:18:22.960582 28053 solver.cpp:239] Iteration 8300 (12.2734 iter/s, 8.14773s/100 iters), loss = 0.203674
I0307 17:18:22.960619 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.203674 (* 1 = 0.203674 loss)
I0307 17:18:22.960626 28053 sgd_solver.cpp:112] Iteration 8300, lr = 0.1
I0307 17:18:31.108387 28053 solver.cpp:239] Iteration 8400 (12.2737 iter/s, 8.14751s/100 iters), loss = 0.441169
I0307 17:18:31.108424 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.441168 (* 1 = 0.441168 loss)
I0307 17:18:31.108431 28053 sgd_solver.cpp:112] Iteration 8400, lr = 0.1
I0307 17:18:39.168584 28053 solver.cpp:347] Iteration 8500, Testing net (#0)
I0307 17:18:40.286757 28053 solver.cpp:414]     Test net output #0: Accuracy1 = 0.7496
I0307 17:18:40.286792 28053 solver.cpp:414]     Test net output #1: SoftmaxWithLoss1 = 0.939935 (* 1 = 0.939935 loss)
I0307 17:18:40.367771 28053 solver.cpp:239] Iteration 8500 (10.8002 iter/s, 9.25906s/100 iters), loss = 0.279509
I0307 17:18:40.367808 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.279509 (* 1 = 0.279509 loss)
I0307 17:18:40.367816 28053 sgd_solver.cpp:112] Iteration 8500, lr = 0.1
I0307 17:18:47.622563 28062 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:18:48.514771 28053 solver.cpp:239] Iteration 8600 (12.2749 iter/s, 8.14671s/100 iters), loss = 0.258345
I0307 17:18:48.514808 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.258345 (* 1 = 0.258345 loss)
I0307 17:18:48.514816 28053 sgd_solver.cpp:112] Iteration 8600, lr = 0.1
I0307 17:18:56.657711 28053 solver.cpp:239] Iteration 8700 (12.281 iter/s, 8.14265s/100 iters), loss = 0.329602
I0307 17:18:56.657747 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.329602 (* 1 = 0.329602 loss)
I0307 17:18:56.657755 28053 sgd_solver.cpp:112] Iteration 8700, lr = 0.1
I0307 17:19:04.800310 28053 solver.cpp:239] Iteration 8800 (12.2815 iter/s, 8.14231s/100 iters), loss = 0.169029
I0307 17:19:04.800348 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.169028 (* 1 = 0.169028 loss)
I0307 17:19:04.800355 28053 sgd_solver.cpp:112] Iteration 8800, lr = 0.1
I0307 17:19:12.943944 28053 solver.cpp:239] Iteration 8900 (12.28 iter/s, 8.14334s/100 iters), loss = 0.258893
I0307 17:19:12.943982 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.258893 (* 1 = 0.258893 loss)
I0307 17:19:12.943989 28053 sgd_solver.cpp:112] Iteration 8900, lr = 0.1
I0307 17:19:19.460813 28062 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:19:21.007594 28053 solver.cpp:347] Iteration 9000, Testing net (#0)
I0307 17:19:22.126576 28053 solver.cpp:414]     Test net output #0: Accuracy1 = 0.7874
I0307 17:19:22.126606 28053 solver.cpp:414]     Test net output #1: SoftmaxWithLoss1 = 0.726504 (* 1 = 0.726504 loss)
I0307 17:19:22.207108 28053 solver.cpp:239] Iteration 9000 (10.7958 iter/s, 9.26284s/100 iters), loss = 0.168111
I0307 17:19:22.207146 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.168111 (* 1 = 0.168111 loss)
I0307 17:19:22.207154 28053 sgd_solver.cpp:112] Iteration 9000, lr = 0.1
I0307 17:19:30.351999 28053 solver.cpp:239] Iteration 9100 (12.2781 iter/s, 8.1446s/100 iters), loss = 0.301973
I0307 17:19:30.352036 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.301973 (* 1 = 0.301973 loss)
I0307 17:19:30.352043 28053 sgd_solver.cpp:112] Iteration 9100, lr = 0.1
I0307 17:19:38.494364 28053 solver.cpp:239] Iteration 9200 (12.2819 iter/s, 8.14208s/100 iters), loss = 0.17413
I0307 17:19:38.494400 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.17413 (* 1 = 0.17413 loss)
I0307 17:19:38.494406 28053 sgd_solver.cpp:112] Iteration 9200, lr = 0.1
I0307 17:19:46.639715 28053 solver.cpp:239] Iteration 9300 (12.2773 iter/s, 8.14508s/100 iters), loss = 0.167582
I0307 17:19:46.639752 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.167582 (* 1 = 0.167582 loss)
I0307 17:19:46.639760 28053 sgd_solver.cpp:112] Iteration 9300, lr = 0.1
I0307 17:19:52.347172 28062 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:19:54.788667 28053 solver.cpp:239] Iteration 9400 (12.2719 iter/s, 8.14868s/100 iters), loss = 0.137113
I0307 17:19:54.788704 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.137113 (* 1 = 0.137113 loss)
I0307 17:19:54.788712 28053 sgd_solver.cpp:112] Iteration 9400, lr = 0.1
I0307 17:20:02.854346 28053 solver.cpp:347] Iteration 9500, Testing net (#0)
I0307 17:20:03.928390 28064 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:20:03.972115 28053 solver.cpp:414]     Test net output #0: Accuracy1 = 0.773
I0307 17:20:03.972148 28053 solver.cpp:414]     Test net output #1: SoftmaxWithLoss1 = 0.845296 (* 1 = 0.845296 loss)
I0307 17:20:04.053103 28053 solver.cpp:239] Iteration 9500 (10.7943 iter/s, 9.26413s/100 iters), loss = 0.289262
I0307 17:20:04.053141 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.289262 (* 1 = 0.289262 loss)
I0307 17:20:04.053150 28053 sgd_solver.cpp:112] Iteration 9500, lr = 0.1
I0307 17:20:12.197533 28053 solver.cpp:239] Iteration 9600 (12.2787 iter/s, 8.14416s/100 iters), loss = 0.130753
I0307 17:20:12.197569 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.130753 (* 1 = 0.130753 loss)
I0307 17:20:12.197576 28053 sgd_solver.cpp:112] Iteration 9600, lr = 0.1
I0307 17:20:20.342511 28053 solver.cpp:239] Iteration 9700 (12.2779 iter/s, 8.14471s/100 iters), loss = 0.198571
I0307 17:20:20.342548 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.198571 (* 1 = 0.198571 loss)
I0307 17:20:20.342555 28053 sgd_solver.cpp:112] Iteration 9700, lr = 0.1
I0307 17:20:25.317078 28062 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:20:28.492681 28053 solver.cpp:239] Iteration 9800 (12.2701 iter/s, 8.1499s/100 iters), loss = 0.344363
I0307 17:20:28.492717 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.344363 (* 1 = 0.344363 loss)
I0307 17:20:28.492724 28053 sgd_solver.cpp:112] Iteration 9800, lr = 0.1
I0307 17:20:36.636569 28053 solver.cpp:239] Iteration 9900 (12.2796 iter/s, 8.14362s/100 iters), loss = 0.201609
I0307 17:20:36.636605 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.201609 (* 1 = 0.201609 loss)
I0307 17:20:36.636612 28053 sgd_solver.cpp:112] Iteration 9900, lr = 0.1
I0307 17:20:44.701658 28053 solver.cpp:464] Snapshotting to binary proto file examples/cifar10/cifar10_res20_iter_10000.caffemodel
I0307 17:20:44.714793 28053 sgd_solver.cpp:284] Snapshotting solver state to binary proto file examples/cifar10/cifar10_res20_iter_10000.solverstate
I0307 17:20:44.718201 28053 solver.cpp:347] Iteration 10000, Testing net (#0)
I0307 17:20:45.836889 28053 solver.cpp:414]     Test net output #0: Accuracy1 = 0.798
I0307 17:20:45.836921 28053 solver.cpp:414]     Test net output #1: SoftmaxWithLoss1 = 0.635435 (* 1 = 0.635435 loss)
I0307 17:20:45.917362 28053 solver.cpp:239] Iteration 10000 (10.7753 iter/s, 9.28049s/100 iters), loss = 0.210099
I0307 17:20:45.917404 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.210099 (* 1 = 0.210099 loss)
I0307 17:20:45.917413 28053 sgd_solver.cpp:112] Iteration 10000, lr = 0.1
I0307 17:20:54.061429 28053 solver.cpp:239] Iteration 10100 (12.2793 iter/s, 8.14379s/100 iters), loss = 0.316641
I0307 17:20:54.061467 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.316641 (* 1 = 0.316641 loss)
I0307 17:20:54.061475 28053 sgd_solver.cpp:112] Iteration 10100, lr = 0.1
I0307 17:20:58.301825 28062 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:21:02.214038 28053 solver.cpp:239] Iteration 10200 (12.2664 iter/s, 8.15233s/100 iters), loss = 0.177461
I0307 17:21:02.214076 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.177461 (* 1 = 0.177461 loss)
I0307 17:21:02.214084 28053 sgd_solver.cpp:112] Iteration 10200, lr = 0.1
I0307 17:21:10.366183 28053 solver.cpp:239] Iteration 10300 (12.2671 iter/s, 8.15187s/100 iters), loss = 0.265673
I0307 17:21:10.366223 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.265673 (* 1 = 0.265673 loss)
I0307 17:21:10.366230 28053 sgd_solver.cpp:112] Iteration 10300, lr = 0.1
I0307 17:21:18.516984 28053 solver.cpp:239] Iteration 10400 (12.2691 iter/s, 8.15053s/100 iters), loss = 0.147934
I0307 17:21:18.517022 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.147934 (* 1 = 0.147934 loss)
I0307 17:21:18.517030 28053 sgd_solver.cpp:112] Iteration 10400, lr = 0.1
I0307 17:21:26.587088 28053 solver.cpp:347] Iteration 10500, Testing net (#0)
I0307 17:21:27.705580 28053 solver.cpp:414]     Test net output #0: Accuracy1 = 0.8088
I0307 17:21:27.705615 28053 solver.cpp:414]     Test net output #1: SoftmaxWithLoss1 = 0.653178 (* 1 = 0.653178 loss)
I0307 17:21:27.786635 28053 solver.cpp:239] Iteration 10500 (10.7882 iter/s, 9.26934s/100 iters), loss = 0.152509
I0307 17:21:27.786674 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.152509 (* 1 = 0.152509 loss)
I0307 17:21:27.786684 28053 sgd_solver.cpp:112] Iteration 10500, lr = 0.1
I0307 17:21:31.215075 28062 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:21:35.935777 28053 solver.cpp:239] Iteration 10600 (12.2716 iter/s, 8.14887s/100 iters), loss = 0.248977
I0307 17:21:35.935817 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.248977 (* 1 = 0.248977 loss)
I0307 17:21:35.935824 28053 sgd_solver.cpp:112] Iteration 10600, lr = 0.1
I0307 17:21:44.081444 28053 solver.cpp:239] Iteration 10700 (12.2769 iter/s, 8.14539s/100 iters), loss = 0.238668
I0307 17:21:44.081482 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.238668 (* 1 = 0.238668 loss)
I0307 17:21:44.081490 28053 sgd_solver.cpp:112] Iteration 10700, lr = 0.1
I0307 17:21:52.228363 28053 solver.cpp:239] Iteration 10800 (12.275 iter/s, 8.14664s/100 iters), loss = 0.233173
I0307 17:21:52.228400 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.233173 (* 1 = 0.233173 loss)
I0307 17:21:52.228407 28053 sgd_solver.cpp:112] Iteration 10800, lr = 0.1
I0307 17:22:00.374817 28053 solver.cpp:239] Iteration 10900 (12.2757 iter/s, 8.14618s/100 iters), loss = 0.270741
I0307 17:22:00.374855 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.270741 (* 1 = 0.270741 loss)
I0307 17:22:00.374862 28053 sgd_solver.cpp:112] Iteration 10900, lr = 0.1
I0307 17:22:03.068300 28062 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:22:08.441993 28053 solver.cpp:347] Iteration 11000, Testing net (#0)
I0307 17:22:09.555562 28053 solver.cpp:414]     Test net output #0: Accuracy1 = 0.8516
I0307 17:22:09.555595 28053 solver.cpp:414]     Test net output #1: SoftmaxWithLoss1 = 0.454317 (* 1 = 0.454317 loss)
I0307 17:22:09.636802 28053 solver.cpp:239] Iteration 11000 (10.7972 iter/s, 9.26168s/100 iters), loss = 0.184956
I0307 17:22:09.636837 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.184956 (* 1 = 0.184956 loss)
I0307 17:22:09.636845 28053 sgd_solver.cpp:112] Iteration 11000, lr = 0.1
I0307 17:22:17.778267 28053 solver.cpp:239] Iteration 11100 (12.2832 iter/s, 8.14119s/100 iters), loss = 0.162337
I0307 17:22:17.778306 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.162337 (* 1 = 0.162337 loss)
I0307 17:22:17.778313 28053 sgd_solver.cpp:112] Iteration 11100, lr = 0.1
I0307 17:22:25.928211 28053 solver.cpp:239] Iteration 11200 (12.2704 iter/s, 8.14967s/100 iters), loss = 0.349521
I0307 17:22:25.928249 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.349521 (* 1 = 0.349521 loss)
I0307 17:22:25.928257 28053 sgd_solver.cpp:112] Iteration 11200, lr = 0.1
I0307 17:22:34.073913 28053 solver.cpp:239] Iteration 11300 (12.2768 iter/s, 8.14543s/100 iters), loss = 0.221659
I0307 17:22:34.073985 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.221659 (* 1 = 0.221659 loss)
I0307 17:22:34.073994 28053 sgd_solver.cpp:112] Iteration 11300, lr = 0.1
I0307 17:22:36.034430 28062 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:22:42.225080 28053 solver.cpp:239] Iteration 11400 (12.2687 iter/s, 8.15085s/100 iters), loss = 0.339769
I0307 17:22:42.225116 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.339769 (* 1 = 0.339769 loss)
I0307 17:22:42.225122 28053 sgd_solver.cpp:112] Iteration 11400, lr = 0.1
I0307 17:22:50.293818 28053 solver.cpp:347] Iteration 11500, Testing net (#0)
I0307 17:22:51.412595 28053 solver.cpp:414]     Test net output #0: Accuracy1 = 0.6601
I0307 17:22:51.412629 28053 solver.cpp:414]     Test net output #1: SoftmaxWithLoss1 = 1.46586 (* 1 = 1.46586 loss)
I0307 17:22:51.493532 28053 solver.cpp:239] Iteration 11500 (10.7896 iter/s, 9.26815s/100 iters), loss = 0.160911
I0307 17:22:51.493572 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.160911 (* 1 = 0.160911 loss)
I0307 17:22:51.493582 28053 sgd_solver.cpp:112] Iteration 11500, lr = 0.1
I0307 17:22:59.641666 28053 solver.cpp:239] Iteration 11600 (12.2732 iter/s, 8.14786s/100 iters), loss = 0.207034
I0307 17:22:59.641705 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.207034 (* 1 = 0.207034 loss)
I0307 17:22:59.641712 28053 sgd_solver.cpp:112] Iteration 11600, lr = 0.1
I0307 17:23:07.787189 28053 solver.cpp:239] Iteration 11700 (12.2771 iter/s, 8.14525s/100 iters), loss = 0.196726
I0307 17:23:07.787276 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.196726 (* 1 = 0.196726 loss)
I0307 17:23:07.787286 28053 sgd_solver.cpp:112] Iteration 11700, lr = 0.1
I0307 17:23:08.931464 28062 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:23:15.938858 28053 solver.cpp:239] Iteration 11800 (12.2679 iter/s, 8.15135s/100 iters), loss = 0.248855
I0307 17:23:15.938897 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.248855 (* 1 = 0.248855 loss)
I0307 17:23:15.938905 28053 sgd_solver.cpp:112] Iteration 11800, lr = 0.1
I0307 17:23:24.084190 28053 solver.cpp:239] Iteration 11900 (12.2774 iter/s, 8.14505s/100 iters), loss = 0.170662
I0307 17:23:24.084229 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.170662 (* 1 = 0.170662 loss)
I0307 17:23:24.084236 28053 sgd_solver.cpp:112] Iteration 11900, lr = 0.1
I0307 17:23:32.152621 28053 solver.cpp:347] Iteration 12000, Testing net (#0)
I0307 17:23:33.228984 28064 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:23:33.273717 28053 solver.cpp:414]     Test net output #0: Accuracy1 = 0.8486
I0307 17:23:33.273751 28053 solver.cpp:414]     Test net output #1: SoftmaxWithLoss1 = 0.452925 (* 1 = 0.452925 loss)
I0307 17:23:33.354504 28053 solver.cpp:239] Iteration 12000 (10.7875 iter/s, 9.27s/100 iters), loss = 0.262854
I0307 17:23:33.354547 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.262854 (* 1 = 0.262854 loss)
I0307 17:23:33.354555 28053 sgd_solver.cpp:112] Iteration 12000, lr = 0.1
I0307 17:23:41.503847 28053 solver.cpp:239] Iteration 12100 (12.2714 iter/s, 8.14906s/100 iters), loss = 0.17309
I0307 17:23:41.503968 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.17309 (* 1 = 0.17309 loss)
I0307 17:23:41.503978 28053 sgd_solver.cpp:112] Iteration 12100, lr = 0.1
I0307 17:23:41.915019 28062 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:23:49.650743 28053 solver.cpp:239] Iteration 12200 (12.2752 iter/s, 8.14654s/100 iters), loss = 0.251249
I0307 17:23:49.650779 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.251249 (* 1 = 0.251249 loss)
I0307 17:23:49.650787 28053 sgd_solver.cpp:112] Iteration 12200, lr = 0.1
I0307 17:23:57.796602 28053 solver.cpp:239] Iteration 12300 (12.2766 iter/s, 8.14558s/100 iters), loss = 0.267973
I0307 17:23:57.796640 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.267973 (* 1 = 0.267973 loss)
I0307 17:23:57.796648 28053 sgd_solver.cpp:112] Iteration 12300, lr = 0.1
I0307 17:24:05.929354 28053 solver.cpp:239] Iteration 12400 (12.2964 iter/s, 8.13247s/100 iters), loss = 0.175342
I0307 17:24:05.929394 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.175342 (* 1 = 0.175342 loss)
I0307 17:24:05.929401 28053 sgd_solver.cpp:112] Iteration 12400, lr = 0.1
I0307 17:24:13.672340 28062 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:24:13.996016 28053 solver.cpp:347] Iteration 12500, Testing net (#0)
I0307 17:24:15.114557 28053 solver.cpp:414]     Test net output #0: Accuracy1 = 0.8428
I0307 17:24:15.114593 28053 solver.cpp:414]     Test net output #1: SoftmaxWithLoss1 = 0.4872 (* 1 = 0.4872 loss)
I0307 17:24:15.195819 28053 solver.cpp:239] Iteration 12500 (10.792 iter/s, 9.26616s/100 iters), loss = 0.138519
I0307 17:24:15.195857 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.138519 (* 1 = 0.138519 loss)
I0307 17:24:15.195864 28053 sgd_solver.cpp:112] Iteration 12500, lr = 0.1
I0307 17:24:23.341624 28053 solver.cpp:239] Iteration 12600 (12.2767 iter/s, 8.14553s/100 iters), loss = 0.240651
I0307 17:24:23.341661 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.240651 (* 1 = 0.240651 loss)
I0307 17:24:23.341668 28053 sgd_solver.cpp:112] Iteration 12600, lr = 0.1
I0307 17:24:31.487293 28053 solver.cpp:239] Iteration 12700 (12.2769 iter/s, 8.14539s/100 iters), loss = 0.219797
I0307 17:24:31.487331 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.219797 (* 1 = 0.219797 loss)
I0307 17:24:31.487339 28053 sgd_solver.cpp:112] Iteration 12700, lr = 0.1
I0307 17:24:39.631772 28053 solver.cpp:239] Iteration 12800 (12.2787 iter/s, 8.1442s/100 iters), loss = 0.174588
I0307 17:24:39.631809 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.174588 (* 1 = 0.174588 loss)
I0307 17:24:39.631817 28053 sgd_solver.cpp:112] Iteration 12800, lr = 0.1
I0307 17:24:46.639205 28062 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:24:47.775590 28053 solver.cpp:239] Iteration 12900 (12.2797 iter/s, 8.14354s/100 iters), loss = 0.155781
I0307 17:24:47.775636 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.155781 (* 1 = 0.155781 loss)
I0307 17:24:47.775645 28053 sgd_solver.cpp:112] Iteration 12900, lr = 0.1
I0307 17:24:55.840821 28053 solver.cpp:347] Iteration 13000, Testing net (#0)
I0307 17:24:56.960067 28053 solver.cpp:414]     Test net output #0: Accuracy1 = 0.8258
I0307 17:24:56.960099 28053 solver.cpp:414]     Test net output #1: SoftmaxWithLoss1 = 0.573823 (* 1 = 0.573823 loss)
I0307 17:24:57.040802 28053 solver.cpp:239] Iteration 13000 (10.7934 iter/s, 9.2649s/100 iters), loss = 0.184002
I0307 17:24:57.040836 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.184002 (* 1 = 0.184002 loss)
I0307 17:24:57.040845 28053 sgd_solver.cpp:112] Iteration 13000, lr = 0.1
I0307 17:25:05.188758 28053 solver.cpp:239] Iteration 13100 (12.2734 iter/s, 8.14768s/100 iters), loss = 0.291255
I0307 17:25:05.188800 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.291255 (* 1 = 0.291255 loss)
I0307 17:25:05.188807 28053 sgd_solver.cpp:112] Iteration 13100, lr = 0.1
I0307 17:25:13.334347 28053 solver.cpp:239] Iteration 13200 (12.277 iter/s, 8.14531s/100 iters), loss = 0.27542
I0307 17:25:13.334385 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.27542 (* 1 = 0.27542 loss)
I0307 17:25:13.334393 28053 sgd_solver.cpp:112] Iteration 13200, lr = 0.1
I0307 17:25:19.606299 28062 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:25:21.477984 28053 solver.cpp:239] Iteration 13300 (12.2799 iter/s, 8.14336s/100 iters), loss = 0.13373
I0307 17:25:21.478024 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.13373 (* 1 = 0.13373 loss)
I0307 17:25:21.478031 28053 sgd_solver.cpp:112] Iteration 13300, lr = 0.1
I0307 17:25:29.619025 28053 solver.cpp:239] Iteration 13400 (12.2839 iter/s, 8.14076s/100 iters), loss = 0.189212
I0307 17:25:29.619065 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.189212 (* 1 = 0.189212 loss)
I0307 17:25:29.619072 28053 sgd_solver.cpp:112] Iteration 13400, lr = 0.1
I0307 17:25:37.681493 28053 solver.cpp:347] Iteration 13500, Testing net (#0)
I0307 17:25:38.799619 28053 solver.cpp:414]     Test net output #0: Accuracy1 = 0.8747
I0307 17:25:38.799655 28053 solver.cpp:414]     Test net output #1: SoftmaxWithLoss1 = 0.381773 (* 1 = 0.381773 loss)
I0307 17:25:38.880522 28053 solver.cpp:239] Iteration 13500 (10.7978 iter/s, 9.26118s/100 iters), loss = 0.131553
I0307 17:25:38.880563 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.131553 (* 1 = 0.131553 loss)
I0307 17:25:38.880571 28053 sgd_solver.cpp:112] Iteration 13500, lr = 0.1
I0307 17:25:47.028862 28053 solver.cpp:239] Iteration 13600 (12.2729 iter/s, 8.14806s/100 iters), loss = 0.152614
I0307 17:25:47.028901 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.152614 (* 1 = 0.152614 loss)
I0307 17:25:47.028908 28053 sgd_solver.cpp:112] Iteration 13600, lr = 0.1
I0307 17:25:52.492297 28062 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:25:55.179461 28053 solver.cpp:239] Iteration 13700 (12.2695 iter/s, 8.15032s/100 iters), loss = 0.196981
I0307 17:25:55.179498 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.196981 (* 1 = 0.196981 loss)
I0307 17:25:55.179507 28053 sgd_solver.cpp:112] Iteration 13700, lr = 0.1
I0307 17:26:03.325505 28053 solver.cpp:239] Iteration 13800 (12.2763 iter/s, 8.14576s/100 iters), loss = 0.165396
I0307 17:26:03.325542 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.165396 (* 1 = 0.165396 loss)
I0307 17:26:03.325549 28053 sgd_solver.cpp:112] Iteration 13800, lr = 0.1
I0307 17:26:11.471403 28053 solver.cpp:239] Iteration 13900 (12.2765 iter/s, 8.14562s/100 iters), loss = 0.22585
I0307 17:26:11.471441 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.22585 (* 1 = 0.22585 loss)
I0307 17:26:11.471447 28053 sgd_solver.cpp:112] Iteration 13900, lr = 0.1
I0307 17:26:19.537300 28053 solver.cpp:347] Iteration 14000, Testing net (#0)
I0307 17:26:20.654485 28053 solver.cpp:414]     Test net output #0: Accuracy1 = 0.8657
I0307 17:26:20.654520 28053 solver.cpp:414]     Test net output #1: SoftmaxWithLoss1 = 0.433922 (* 1 = 0.433922 loss)
I0307 17:26:20.735498 28053 solver.cpp:239] Iteration 14000 (10.7947 iter/s, 9.26378s/100 iters), loss = 0.220701
I0307 17:26:20.735536 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.220701 (* 1 = 0.220701 loss)
I0307 17:26:20.735545 28053 sgd_solver.cpp:112] Iteration 14000, lr = 0.1
I0307 17:26:25.465108 28062 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:26:28.885848 28053 solver.cpp:239] Iteration 14100 (12.2698 iter/s, 8.15007s/100 iters), loss = 0.256608
I0307 17:26:28.885886 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.256608 (* 1 = 0.256608 loss)
I0307 17:26:28.885893 28053 sgd_solver.cpp:112] Iteration 14100, lr = 0.1
I0307 17:26:37.032300 28053 solver.cpp:239] Iteration 14200 (12.2757 iter/s, 8.14617s/100 iters), loss = 0.145181
I0307 17:26:37.032337 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.145181 (* 1 = 0.145181 loss)
I0307 17:26:37.032344 28053 sgd_solver.cpp:112] Iteration 14200, lr = 0.1
I0307 17:26:45.180624 28053 solver.cpp:239] Iteration 14300 (12.2729 iter/s, 8.14804s/100 iters), loss = 0.193832
I0307 17:26:45.180661 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.193832 (* 1 = 0.193832 loss)
I0307 17:26:45.180668 28053 sgd_solver.cpp:112] Iteration 14300, lr = 0.1
I0307 17:26:53.324702 28053 solver.cpp:239] Iteration 14400 (12.2793 iter/s, 8.1438s/100 iters), loss = 0.165439
I0307 17:26:53.324738 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.165439 (* 1 = 0.165439 loss)
I0307 17:26:53.324746 28053 sgd_solver.cpp:112] Iteration 14400, lr = 0.1
I0307 17:26:57.320197 28062 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:27:01.395108 28053 solver.cpp:347] Iteration 14500, Testing net (#0)
I0307 17:27:02.471082 28064 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:27:02.514537 28053 solver.cpp:414]     Test net output #0: Accuracy1 = 0.856
I0307 17:27:02.514564 28053 solver.cpp:414]     Test net output #1: SoftmaxWithLoss1 = 0.483141 (* 1 = 0.483141 loss)
I0307 17:27:02.595764 28053 solver.cpp:239] Iteration 14500 (10.7866 iter/s, 9.27075s/100 iters), loss = 0.206802
I0307 17:27:02.595798 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.206802 (* 1 = 0.206802 loss)
I0307 17:27:02.595808 28053 sgd_solver.cpp:112] Iteration 14500, lr = 0.1
I0307 17:27:10.743456 28053 solver.cpp:239] Iteration 14600 (12.2738 iter/s, 8.14742s/100 iters), loss = 0.135242
I0307 17:27:10.743494 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.135242 (* 1 = 0.135242 loss)
I0307 17:27:10.743501 28053 sgd_solver.cpp:112] Iteration 14600, lr = 0.1
I0307 17:27:18.890575 28053 solver.cpp:239] Iteration 14700 (12.2747 iter/s, 8.14684s/100 iters), loss = 0.183735
I0307 17:27:18.890611 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.183735 (* 1 = 0.183735 loss)
I0307 17:27:18.890619 28053 sgd_solver.cpp:112] Iteration 14700, lr = 0.1
I0307 17:27:27.036700 28053 solver.cpp:239] Iteration 14800 (12.2762 iter/s, 8.14585s/100 iters), loss = 0.163149
I0307 17:27:27.036737 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.163149 (* 1 = 0.163149 loss)
I0307 17:27:27.036744 28053 sgd_solver.cpp:112] Iteration 14800, lr = 0.1
I0307 17:27:30.220024 28062 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:27:35.187865 28053 solver.cpp:239] Iteration 14900 (12.2686 iter/s, 8.15088s/100 iters), loss = 0.123065
I0307 17:27:35.187902 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.123065 (* 1 = 0.123065 loss)
I0307 17:27:35.187909 28053 sgd_solver.cpp:112] Iteration 14900, lr = 0.1
I0307 17:27:43.254827 28053 solver.cpp:347] Iteration 15000, Testing net (#0)
I0307 17:27:44.372717 28053 solver.cpp:414]     Test net output #0: Accuracy1 = 0.86
I0307 17:27:44.372750 28053 solver.cpp:414]     Test net output #1: SoftmaxWithLoss1 = 0.440238 (* 1 = 0.440238 loss)
I0307 17:27:44.453200 28053 solver.cpp:239] Iteration 15000 (10.7933 iter/s, 9.26502s/100 iters), loss = 0.272326
I0307 17:27:44.453239 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.272326 (* 1 = 0.272326 loss)
I0307 17:27:44.453248 28053 sgd_solver.cpp:112] Iteration 15000, lr = 0.1
I0307 17:27:52.604400 28053 solver.cpp:239] Iteration 15100 (12.2686 iter/s, 8.15092s/100 iters), loss = 0.254655
I0307 17:27:52.604437 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.254655 (* 1 = 0.254655 loss)
I0307 17:27:52.604444 28053 sgd_solver.cpp:112] Iteration 15100, lr = 0.1
I0307 17:28:00.752882 28053 solver.cpp:239] Iteration 15200 (12.2726 iter/s, 8.1482s/100 iters), loss = 0.212641
I0307 17:28:00.752988 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.212641 (* 1 = 0.212641 loss)
I0307 17:28:00.752998 28053 sgd_solver.cpp:112] Iteration 15200, lr = 0.1
I0307 17:28:03.201623 28062 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:28:08.902981 28053 solver.cpp:239] Iteration 15300 (12.2703 iter/s, 8.14975s/100 iters), loss = 0.14224
I0307 17:28:08.903017 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.14224 (* 1 = 0.14224 loss)
I0307 17:28:08.903024 28053 sgd_solver.cpp:112] Iteration 15300, lr = 0.1
I0307 17:28:17.039654 28053 solver.cpp:239] Iteration 15400 (12.2905 iter/s, 8.13639s/100 iters), loss = 0.194312
I0307 17:28:17.039690 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.194312 (* 1 = 0.194312 loss)
I0307 17:28:17.039698 28053 sgd_solver.cpp:112] Iteration 15400, lr = 0.1
I0307 17:28:25.103838 28053 solver.cpp:347] Iteration 15500, Testing net (#0)
I0307 17:28:26.221114 28053 solver.cpp:414]     Test net output #0: Accuracy1 = 0.8056
I0307 17:28:26.221146 28053 solver.cpp:414]     Test net output #1: SoftmaxWithLoss1 = 0.714226 (* 1 = 0.714226 loss)
I0307 17:28:26.302248 28053 solver.cpp:239] Iteration 15500 (10.7965 iter/s, 9.26228s/100 iters), loss = 0.273514
I0307 17:28:26.302286 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.273513 (* 1 = 0.273513 loss)
I0307 17:28:26.302294 28053 sgd_solver.cpp:112] Iteration 15500, lr = 0.1
I0307 17:28:34.449234 28053 solver.cpp:239] Iteration 15600 (12.2749 iter/s, 8.1467s/100 iters), loss = 0.217504
I0307 17:28:34.449316 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.217504 (* 1 = 0.217504 loss)
I0307 17:28:34.449326 28053 sgd_solver.cpp:112] Iteration 15600, lr = 0.1
I0307 17:28:36.083235 28062 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:28:42.597472 28053 solver.cpp:239] Iteration 15700 (12.2731 iter/s, 8.14791s/100 iters), loss = 0.102334
I0307 17:28:42.597512 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.102334 (* 1 = 0.102334 loss)
I0307 17:28:42.597518 28053 sgd_solver.cpp:112] Iteration 15700, lr = 0.1
I0307 17:28:50.740643 28053 solver.cpp:239] Iteration 15800 (12.2807 iter/s, 8.14289s/100 iters), loss = 0.164937
I0307 17:28:50.740679 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.164937 (* 1 = 0.164937 loss)
I0307 17:28:50.740686 28053 sgd_solver.cpp:112] Iteration 15800, lr = 0.1
I0307 17:28:58.883853 28053 solver.cpp:239] Iteration 15900 (12.2806 iter/s, 8.14293s/100 iters), loss = 0.163418
I0307 17:28:58.883890 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.163418 (* 1 = 0.163418 loss)
I0307 17:28:58.883898 28053 sgd_solver.cpp:112] Iteration 15900, lr = 0.1
I0307 17:29:06.949309 28053 solver.cpp:347] Iteration 16000, Testing net (#0)
I0307 17:29:08.067834 28053 solver.cpp:414]     Test net output #0: Accuracy1 = 0.8631
I0307 17:29:08.067869 28053 solver.cpp:414]     Test net output #1: SoftmaxWithLoss1 = 0.423406 (* 1 = 0.423406 loss)
I0307 17:29:08.148667 28053 solver.cpp:239] Iteration 16000 (10.7939 iter/s, 9.2645s/100 iters), loss = 0.188938
I0307 17:29:08.148707 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.188938 (* 1 = 0.188938 loss)
I0307 17:29:08.148715 28053 sgd_solver.cpp:112] Iteration 16000, lr = 0.1
I0307 17:29:09.050990 28062 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:29:16.296448 28053 solver.cpp:239] Iteration 16100 (12.2737 iter/s, 8.1475s/100 iters), loss = 0.138701
I0307 17:29:16.296485 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.138701 (* 1 = 0.138701 loss)
I0307 17:29:16.296492 28053 sgd_solver.cpp:112] Iteration 16100, lr = 0.1
I0307 17:29:24.436456 28053 solver.cpp:239] Iteration 16200 (12.2854 iter/s, 8.13973s/100 iters), loss = 0.160401
I0307 17:29:24.436493 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.1604 (* 1 = 0.1604 loss)
I0307 17:29:24.436501 28053 sgd_solver.cpp:112] Iteration 16200, lr = 0.1
I0307 17:29:32.579439 28053 solver.cpp:239] Iteration 16300 (12.2809 iter/s, 8.1427s/100 iters), loss = 0.231634
I0307 17:29:32.579476 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.231634 (* 1 = 0.231634 loss)
I0307 17:29:32.579483 28053 sgd_solver.cpp:112] Iteration 16300, lr = 0.1
I0307 17:29:40.722399 28053 solver.cpp:239] Iteration 16400 (12.281 iter/s, 8.14268s/100 iters), loss = 0.162727
I0307 17:29:40.722515 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.162727 (* 1 = 0.162727 loss)
I0307 17:29:40.722525 28053 sgd_solver.cpp:112] Iteration 16400, lr = 0.1
I0307 17:29:40.889389 28062 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:29:48.787400 28053 solver.cpp:347] Iteration 16500, Testing net (#0)
I0307 17:29:49.905814 28053 solver.cpp:414]     Test net output #0: Accuracy1 = 0.8647
I0307 17:29:49.905848 28053 solver.cpp:414]     Test net output #1: SoftmaxWithLoss1 = 0.458171 (* 1 = 0.458171 loss)
I0307 17:29:49.986878 28053 solver.cpp:239] Iteration 16500 (10.7944 iter/s, 9.26409s/100 iters), loss = 0.226245
I0307 17:29:49.986918 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.226245 (* 1 = 0.226245 loss)
I0307 17:29:49.986927 28053 sgd_solver.cpp:112] Iteration 16500, lr = 0.1
I0307 17:29:58.132846 28053 solver.cpp:239] Iteration 16600 (12.2764 iter/s, 8.14568s/100 iters), loss = 0.210613
I0307 17:29:58.132882 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.210613 (* 1 = 0.210613 loss)
I0307 17:29:58.132890 28053 sgd_solver.cpp:112] Iteration 16600, lr = 0.1
I0307 17:30:06.276747 28053 solver.cpp:239] Iteration 16700 (12.2796 iter/s, 8.14362s/100 iters), loss = 0.185913
I0307 17:30:06.276788 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.185913 (* 1 = 0.185913 loss)
I0307 17:30:06.276795 28053 sgd_solver.cpp:112] Iteration 16700, lr = 0.1
I0307 17:30:13.774749 28062 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:30:14.422936 28053 solver.cpp:239] Iteration 16800 (12.2761 iter/s, 8.1459s/100 iters), loss = 0.0746698
I0307 17:30:14.422973 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.0746697 (* 1 = 0.0746697 loss)
I0307 17:30:14.422981 28053 sgd_solver.cpp:112] Iteration 16800, lr = 0.1
I0307 17:30:22.568851 28053 solver.cpp:239] Iteration 16900 (12.2765 iter/s, 8.14563s/100 iters), loss = 0.279329
I0307 17:30:22.568889 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.279329 (* 1 = 0.279329 loss)
I0307 17:30:22.568897 28053 sgd_solver.cpp:112] Iteration 16900, lr = 0.1
I0307 17:30:30.633373 28053 solver.cpp:347] Iteration 17000, Testing net (#0)
I0307 17:30:31.710973 28064 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:30:31.754447 28053 solver.cpp:414]     Test net output #0: Accuracy1 = 0.8778
I0307 17:30:31.754482 28053 solver.cpp:414]     Test net output #1: SoftmaxWithLoss1 = 0.372648 (* 1 = 0.372648 loss)
I0307 17:30:31.835669 28053 solver.cpp:239] Iteration 17000 (10.7916 iter/s, 9.2665s/100 iters), loss = 0.137223
I0307 17:30:31.835708 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.137223 (* 1 = 0.137223 loss)
I0307 17:30:31.835716 28053 sgd_solver.cpp:112] Iteration 17000, lr = 0.1
I0307 17:30:39.982987 28053 solver.cpp:239] Iteration 17100 (12.2744 iter/s, 8.14704s/100 iters), loss = 0.165972
I0307 17:30:39.983026 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.165972 (* 1 = 0.165972 loss)
I0307 17:30:39.983032 28053 sgd_solver.cpp:112] Iteration 17100, lr = 0.1
I0307 17:30:46.746430 28062 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:30:48.128541 28053 solver.cpp:239] Iteration 17200 (12.2771 iter/s, 8.14527s/100 iters), loss = 0.104294
I0307 17:30:48.128587 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.104294 (* 1 = 0.104294 loss)
I0307 17:30:48.128594 28053 sgd_solver.cpp:112] Iteration 17200, lr = 0.1
I0307 17:30:56.277256 28053 solver.cpp:239] Iteration 17300 (12.2724 iter/s, 8.14835s/100 iters), loss = 0.194648
I0307 17:30:56.277293 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.194648 (* 1 = 0.194648 loss)
I0307 17:30:56.277300 28053 sgd_solver.cpp:112] Iteration 17300, lr = 0.1
I0307 17:31:04.423436 28053 solver.cpp:239] Iteration 17400 (12.2761 iter/s, 8.1459s/100 iters), loss = 0.204263
I0307 17:31:04.423475 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.204263 (* 1 = 0.204263 loss)
I0307 17:31:04.423481 28053 sgd_solver.cpp:112] Iteration 17400, lr = 0.1
I0307 17:31:12.475463 28053 solver.cpp:347] Iteration 17500, Testing net (#0)
I0307 17:31:13.593158 28053 solver.cpp:414]     Test net output #0: Accuracy1 = 0.8508
I0307 17:31:13.593191 28053 solver.cpp:414]     Test net output #1: SoftmaxWithLoss1 = 0.450955 (* 1 = 0.450955 loss)
I0307 17:31:13.674302 28053 solver.cpp:239] Iteration 17500 (10.8102 iter/s, 9.25055s/100 iters), loss = 0.157774
I0307 17:31:13.674340 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.157774 (* 1 = 0.157774 loss)
I0307 17:31:13.674348 28053 sgd_solver.cpp:112] Iteration 17500, lr = 0.1
I0307 17:31:19.710172 28062 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:31:21.826571 28053 solver.cpp:239] Iteration 17600 (12.2669 iter/s, 8.15199s/100 iters), loss = 0.112136
I0307 17:31:21.826611 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.112136 (* 1 = 0.112136 loss)
I0307 17:31:21.826618 28053 sgd_solver.cpp:112] Iteration 17600, lr = 0.1
I0307 17:31:29.973667 28053 solver.cpp:239] Iteration 17700 (12.2747 iter/s, 8.14681s/100 iters), loss = 0.108473
I0307 17:31:29.973704 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.108473 (* 1 = 0.108473 loss)
I0307 17:31:29.973712 28053 sgd_solver.cpp:112] Iteration 17700, lr = 0.1
I0307 17:31:38.121824 28053 solver.cpp:239] Iteration 17800 (12.2731 iter/s, 8.14787s/100 iters), loss = 0.148568
I0307 17:31:38.121861 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.148568 (* 1 = 0.148568 loss)
I0307 17:31:38.121868 28053 sgd_solver.cpp:112] Iteration 17800, lr = 0.1
I0307 17:31:46.267117 28053 solver.cpp:239] Iteration 17900 (12.2775 iter/s, 8.14501s/100 iters), loss = 0.103739
I0307 17:31:46.267155 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.103739 (* 1 = 0.103739 loss)
I0307 17:31:46.267163 28053 sgd_solver.cpp:112] Iteration 17900, lr = 0.1
I0307 17:31:51.486788 28062 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:31:54.339357 28053 solver.cpp:347] Iteration 18000, Testing net (#0)
I0307 17:31:55.456573 28053 solver.cpp:414]     Test net output #0: Accuracy1 = 0.8601
I0307 17:31:55.456604 28053 solver.cpp:414]     Test net output #1: SoftmaxWithLoss1 = 0.43181 (* 1 = 0.43181 loss)
I0307 17:31:55.537490 28053 solver.cpp:239] Iteration 18000 (10.7874 iter/s, 9.27006s/100 iters), loss = 0.10521
I0307 17:31:55.537524 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.10521 (* 1 = 0.10521 loss)
I0307 17:31:55.537533 28053 sgd_solver.cpp:112] Iteration 18000, lr = 0.1
I0307 17:32:03.684707 28053 solver.cpp:239] Iteration 18100 (12.2745 iter/s, 8.14694s/100 iters), loss = 0.172407
I0307 17:32:03.684746 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.172407 (* 1 = 0.172407 loss)
I0307 17:32:03.684753 28053 sgd_solver.cpp:112] Iteration 18100, lr = 0.1
I0307 17:32:11.831699 28053 solver.cpp:239] Iteration 18200 (12.2749 iter/s, 8.14671s/100 iters), loss = 0.23644
I0307 17:32:11.831737 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.23644 (* 1 = 0.23644 loss)
I0307 17:32:11.831744 28053 sgd_solver.cpp:112] Iteration 18200, lr = 0.1
I0307 17:32:19.976838 28053 solver.cpp:239] Iteration 18300 (12.2777 iter/s, 8.14485s/100 iters), loss = 0.112976
I0307 17:32:19.976876 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.112976 (* 1 = 0.112976 loss)
I0307 17:32:19.976883 28053 sgd_solver.cpp:112] Iteration 18300, lr = 0.1
I0307 17:32:24.462393 28062 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:32:28.127497 28053 solver.cpp:239] Iteration 18400 (12.2694 iter/s, 8.15038s/100 iters), loss = 0.10367
I0307 17:32:28.127535 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.10367 (* 1 = 0.10367 loss)
I0307 17:32:28.127542 28053 sgd_solver.cpp:112] Iteration 18400, lr = 0.1
I0307 17:32:36.188961 28053 solver.cpp:347] Iteration 18500, Testing net (#0)
I0307 17:32:37.307415 28053 solver.cpp:414]     Test net output #0: Accuracy1 = 0.8489
I0307 17:32:37.307452 28053 solver.cpp:414]     Test net output #1: SoftmaxWithLoss1 = 0.494617 (* 1 = 0.494617 loss)
I0307 17:32:37.388064 28053 solver.cpp:239] Iteration 18500 (10.7988 iter/s, 9.26025s/100 iters), loss = 0.1935
I0307 17:32:37.388103 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.1935 (* 1 = 0.1935 loss)
I0307 17:32:37.388110 28053 sgd_solver.cpp:112] Iteration 18500, lr = 0.1
I0307 17:32:45.534961 28053 solver.cpp:239] Iteration 18600 (12.275 iter/s, 8.14661s/100 iters), loss = 0.200989
I0307 17:32:45.535001 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.200989 (* 1 = 0.200989 loss)
I0307 17:32:45.535007 28053 sgd_solver.cpp:112] Iteration 18600, lr = 0.1
I0307 17:32:53.682147 28053 solver.cpp:239] Iteration 18700 (12.2746 iter/s, 8.1469s/100 iters), loss = 0.124025
I0307 17:32:53.682186 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.124025 (* 1 = 0.124025 loss)
I0307 17:32:53.682194 28053 sgd_solver.cpp:112] Iteration 18700, lr = 0.1
I0307 17:32:57.355312 28062 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:33:01.833441 28053 solver.cpp:239] Iteration 18800 (12.2684 iter/s, 8.15101s/100 iters), loss = 0.198439
I0307 17:33:01.833478 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.198439 (* 1 = 0.198439 loss)
I0307 17:33:01.833485 28053 sgd_solver.cpp:112] Iteration 18800, lr = 0.1
I0307 17:33:09.981794 28053 solver.cpp:239] Iteration 18900 (12.2728 iter/s, 8.14807s/100 iters), loss = 0.150901
I0307 17:33:09.981833 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.150901 (* 1 = 0.150901 loss)
I0307 17:33:09.981840 28053 sgd_solver.cpp:112] Iteration 18900, lr = 0.1
I0307 17:33:18.045765 28053 solver.cpp:347] Iteration 19000, Testing net (#0)
I0307 17:33:19.164279 28053 solver.cpp:414]     Test net output #0: Accuracy1 = 0.86
I0307 17:33:19.164315 28053 solver.cpp:414]     Test net output #1: SoftmaxWithLoss1 = 0.430611 (* 1 = 0.430611 loss)
I0307 17:33:19.244870 28053 solver.cpp:239] Iteration 19000 (10.7959 iter/s, 9.26276s/100 iters), loss = 0.156292
I0307 17:33:19.244910 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.156292 (* 1 = 0.156292 loss)
I0307 17:33:19.244918 28053 sgd_solver.cpp:112] Iteration 19000, lr = 0.1
I0307 17:33:27.389015 28053 solver.cpp:239] Iteration 19100 (12.2792 iter/s, 8.14386s/100 iters), loss = 0.283647
I0307 17:33:27.389083 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.283647 (* 1 = 0.283647 loss)
I0307 17:33:27.389091 28053 sgd_solver.cpp:112] Iteration 19100, lr = 0.1
I0307 17:33:30.325253 28062 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:33:35.535754 28053 solver.cpp:239] Iteration 19200 (12.2753 iter/s, 8.14643s/100 iters), loss = 0.163088
I0307 17:33:35.535794 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.163088 (* 1 = 0.163088 loss)
I0307 17:33:35.535800 28053 sgd_solver.cpp:112] Iteration 19200, lr = 0.1
I0307 17:33:43.678684 28053 solver.cpp:239] Iteration 19300 (12.281 iter/s, 8.14264s/100 iters), loss = 0.266922
I0307 17:33:43.678725 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.266922 (* 1 = 0.266922 loss)
I0307 17:33:43.678732 28053 sgd_solver.cpp:112] Iteration 19300, lr = 0.1
I0307 17:33:51.821516 28053 solver.cpp:239] Iteration 19400 (12.2812 iter/s, 8.14255s/100 iters), loss = 0.257255
I0307 17:33:51.821557 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.257255 (* 1 = 0.257255 loss)
I0307 17:33:51.821563 28053 sgd_solver.cpp:112] Iteration 19400, lr = 0.1
I0307 17:33:59.882436 28053 solver.cpp:347] Iteration 19500, Testing net (#0)
I0307 17:34:00.960068 28064 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:34:01.003463 28053 solver.cpp:414]     Test net output #0: Accuracy1 = 0.8876
I0307 17:34:01.003499 28053 solver.cpp:414]     Test net output #1: SoftmaxWithLoss1 = 0.334413 (* 1 = 0.334413 loss)
I0307 17:34:01.084679 28053 solver.cpp:239] Iteration 19500 (10.7958 iter/s, 9.26285s/100 iters), loss = 0.131684
I0307 17:34:01.084719 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.131684 (* 1 = 0.131684 loss)
I0307 17:34:01.084728 28053 sgd_solver.cpp:112] Iteration 19500, lr = 0.1
I0307 17:34:03.290331 28062 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:34:09.236555 28053 solver.cpp:239] Iteration 19600 (12.2675 iter/s, 8.15159s/100 iters), loss = 0.153104
I0307 17:34:09.236593 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.153104 (* 1 = 0.153104 loss)
I0307 17:34:09.236600 28053 sgd_solver.cpp:112] Iteration 19600, lr = 0.1
I0307 17:34:17.385586 28053 solver.cpp:239] Iteration 19700 (12.2718 iter/s, 8.14875s/100 iters), loss = 0.178359
I0307 17:34:17.385625 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.178359 (* 1 = 0.178359 loss)
I0307 17:34:17.385632 28053 sgd_solver.cpp:112] Iteration 19700, lr = 0.1
I0307 17:34:25.540650 28053 solver.cpp:239] Iteration 19800 (12.2627 iter/s, 8.15478s/100 iters), loss = 0.14662
I0307 17:34:25.540689 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.14662 (* 1 = 0.14662 loss)
I0307 17:34:25.540696 28053 sgd_solver.cpp:112] Iteration 19800, lr = 0.1
I0307 17:34:33.693799 28053 solver.cpp:239] Iteration 19900 (12.2656 iter/s, 8.15286s/100 iters), loss = 0.0752056
I0307 17:34:33.693884 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.0752055 (* 1 = 0.0752055 loss)
I0307 17:34:33.693892 28053 sgd_solver.cpp:112] Iteration 19900, lr = 0.1
I0307 17:34:35.085975 28062 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:34:41.772831 28053 solver.cpp:464] Snapshotting to binary proto file examples/cifar10/cifar10_res20_iter_20000.caffemodel
I0307 17:34:41.780388 28053 sgd_solver.cpp:284] Snapshotting solver state to binary proto file examples/cifar10/cifar10_res20_iter_20000.solverstate
I0307 17:34:41.783759 28053 solver.cpp:347] Iteration 20000, Testing net (#0)
I0307 17:34:42.905076 28053 solver.cpp:414]     Test net output #0: Accuracy1 = 0.8716
I0307 17:34:42.905107 28053 solver.cpp:414]     Test net output #1: SoftmaxWithLoss1 = 0.393327 (* 1 = 0.393327 loss)
I0307 17:34:42.985957 28053 solver.cpp:239] Iteration 20000 (10.7622 iter/s, 9.2918s/100 iters), loss = 0.161328
I0307 17:34:42.985992 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.161328 (* 1 = 0.161328 loss)
I0307 17:34:42.986001 28053 sgd_solver.cpp:50] MultiStep Status: Iteration 20000, step = 1
I0307 17:34:42.986004 28053 sgd_solver.cpp:112] Iteration 20000, lr = 0.01
I0307 17:34:51.134047 28053 solver.cpp:239] Iteration 20100 (12.2732 iter/s, 8.14781s/100 iters), loss = 0.126683
I0307 17:34:51.134083 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.126683 (* 1 = 0.126683 loss)
I0307 17:34:51.134090 28053 sgd_solver.cpp:112] Iteration 20100, lr = 0.01
I0307 17:34:59.282408 28053 solver.cpp:239] Iteration 20200 (12.2728 iter/s, 8.14808s/100 iters), loss = 0.0543512
I0307 17:34:59.282446 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.0543511 (* 1 = 0.0543511 loss)
I0307 17:34:59.282454 28053 sgd_solver.cpp:112] Iteration 20200, lr = 0.01
I0307 17:35:07.432140 28053 solver.cpp:239] Iteration 20300 (12.2708 iter/s, 8.14945s/100 iters), loss = 0.0583055
I0307 17:35:07.432238 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.0583055 (* 1 = 0.0583055 loss)
I0307 17:35:07.432248 28053 sgd_solver.cpp:112] Iteration 20300, lr = 0.01
I0307 17:35:08.088439 28062 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:35:15.581535 28053 solver.cpp:239] Iteration 20400 (12.2714 iter/s, 8.14906s/100 iters), loss = 0.051741
I0307 17:35:15.581574 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.051741 (* 1 = 0.051741 loss)
I0307 17:35:15.581583 28053 sgd_solver.cpp:112] Iteration 20400, lr = 0.01
I0307 17:35:23.651223 28053 solver.cpp:347] Iteration 20500, Testing net (#0)
I0307 17:35:24.766614 28053 solver.cpp:414]     Test net output #0: Accuracy1 = 0.9642
I0307 17:35:24.766649 28053 solver.cpp:414]     Test net output #1: SoftmaxWithLoss1 = 0.106028 (* 1 = 0.106028 loss)
I0307 17:35:24.847499 28053 solver.cpp:239] Iteration 20500 (10.7926 iter/s, 9.26565s/100 iters), loss = 0.0681712
I0307 17:35:24.847540 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.0681712 (* 1 = 0.0681712 loss)
I0307 17:35:24.847548 28053 sgd_solver.cpp:112] Iteration 20500, lr = 0.01
I0307 17:35:32.998427 28053 solver.cpp:239] Iteration 20600 (12.269 iter/s, 8.15064s/100 iters), loss = 0.081209
I0307 17:35:32.998466 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.081209 (* 1 = 0.081209 loss)
I0307 17:35:32.998472 28053 sgd_solver.cpp:112] Iteration 20600, lr = 0.01
I0307 17:35:41.053898 28062 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:35:41.133944 28053 solver.cpp:239] Iteration 20700 (12.2922 iter/s, 8.13523s/100 iters), loss = 0.033722
I0307 17:35:41.133982 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.033722 (* 1 = 0.033722 loss)
I0307 17:35:41.133991 28053 sgd_solver.cpp:112] Iteration 20700, lr = 0.01
I0307 17:35:49.282389 28053 solver.cpp:239] Iteration 20800 (12.2727 iter/s, 8.14816s/100 iters), loss = 0.100512
I0307 17:35:49.282428 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.100512 (* 1 = 0.100512 loss)
I0307 17:35:49.282434 28053 sgd_solver.cpp:112] Iteration 20800, lr = 0.01
I0307 17:35:57.417089 28053 solver.cpp:239] Iteration 20900 (12.2934 iter/s, 8.13442s/100 iters), loss = 0.0859822
I0307 17:35:57.417129 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.0859822 (* 1 = 0.0859822 loss)
I0307 17:35:57.417136 28053 sgd_solver.cpp:112] Iteration 20900, lr = 0.01
I0307 17:36:05.475798 28053 solver.cpp:347] Iteration 21000, Testing net (#0)
I0307 17:36:06.593771 28053 solver.cpp:414]     Test net output #0: Accuracy1 = 0.9831
I0307 17:36:06.593807 28053 solver.cpp:414]     Test net output #1: SoftmaxWithLoss1 = 0.0632253 (* 1 = 0.0632253 loss)
I0307 17:36:06.674618 28053 solver.cpp:239] Iteration 21000 (10.8024 iter/s, 9.25721s/100 iters), loss = 0.0508009
I0307 17:36:06.674657 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.0508008 (* 1 = 0.0508008 loss)
I0307 17:36:06.674666 28053 sgd_solver.cpp:112] Iteration 21000, lr = 0.01
I0307 17:36:13.931176 28062 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:36:14.824223 28053 solver.cpp:239] Iteration 21100 (12.271 iter/s, 8.14932s/100 iters), loss = 0.0367434
I0307 17:36:14.824262 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.0367433 (* 1 = 0.0367433 loss)
I0307 17:36:14.824270 28053 sgd_solver.cpp:112] Iteration 21100, lr = 0.01
I0307 17:36:22.974509 28053 solver.cpp:239] Iteration 21200 (12.2699 iter/s, 8.15s/100 iters), loss = 0.0705364
I0307 17:36:22.974546 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.0705364 (* 1 = 0.0705364 loss)
I0307 17:36:22.974553 28053 sgd_solver.cpp:112] Iteration 21200, lr = 0.01
I0307 17:36:31.123281 28053 solver.cpp:239] Iteration 21300 (12.2722 iter/s, 8.14849s/100 iters), loss = 0.0316494
I0307 17:36:31.123322 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.0316493 (* 1 = 0.0316493 loss)
I0307 17:36:31.123329 28053 sgd_solver.cpp:112] Iteration 21300, lr = 0.01
I0307 17:36:39.277061 28053 solver.cpp:239] Iteration 21400 (12.2647 iter/s, 8.15349s/100 iters), loss = 0.0276234
I0307 17:36:39.277099 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.0276233 (* 1 = 0.0276233 loss)
I0307 17:36:39.277107 28053 sgd_solver.cpp:112] Iteration 21400, lr = 0.01
I0307 17:36:45.799988 28062 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:36:47.360574 28053 solver.cpp:347] Iteration 21500, Testing net (#0)
I0307 17:36:48.518635 28053 solver.cpp:414]     Test net output #0: Accuracy1 = 0.9886
I0307 17:36:48.518666 28053 solver.cpp:414]     Test net output #1: SoftmaxWithLoss1 = 0.0483657 (* 1 = 0.0483657 loss)
I0307 17:36:48.601999 28053 solver.cpp:239] Iteration 21500 (10.7243 iter/s, 9.32462s/100 iters), loss = 0.0242822
I0307 17:36:48.602032 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.0242822 (* 1 = 0.0242822 loss)
I0307 17:36:48.602041 28053 sgd_solver.cpp:112] Iteration 21500, lr = 0.01
I0307 17:36:56.989176 28053 solver.cpp:239] Iteration 21600 (11.9234 iter/s, 8.38689s/100 iters), loss = 0.0642963
I0307 17:36:56.989212 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.0642963 (* 1 = 0.0642963 loss)
I0307 17:36:56.989219 28053 sgd_solver.cpp:112] Iteration 21600, lr = 0.01
I0307 17:37:05.193634 28053 solver.cpp:239] Iteration 21700 (12.1889 iter/s, 8.20417s/100 iters), loss = 0.0208784
I0307 17:37:05.193672 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.0208784 (* 1 = 0.0208784 loss)
I0307 17:37:05.193680 28053 sgd_solver.cpp:112] Iteration 21700, lr = 0.01
I0307 17:37:13.347112 28053 solver.cpp:239] Iteration 21800 (12.2651 iter/s, 8.15319s/100 iters), loss = 0.0344917
I0307 17:37:13.347151 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.0344917 (* 1 = 0.0344917 loss)
I0307 17:37:13.347158 28053 sgd_solver.cpp:112] Iteration 21800, lr = 0.01
I0307 17:37:19.058950 28062 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:37:21.501302 28053 solver.cpp:239] Iteration 21900 (12.2641 iter/s, 8.1539s/100 iters), loss = 0.0215571
I0307 17:37:21.501339 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.0215571 (* 1 = 0.0215571 loss)
I0307 17:37:21.501346 28053 sgd_solver.cpp:112] Iteration 21900, lr = 0.01
I0307 17:37:29.577805 28053 solver.cpp:347] Iteration 22000, Testing net (#0)
I0307 17:37:30.653414 28064 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:37:30.697775 28053 solver.cpp:414]     Test net output #0: Accuracy1 = 0.9934
I0307 17:37:30.697809 28053 solver.cpp:414]     Test net output #1: SoftmaxWithLoss1 = 0.0349909 (* 1 = 0.0349909 loss)
I0307 17:37:30.778434 28053 solver.cpp:239] Iteration 22000 (10.7796 iter/s, 9.27681s/100 iters), loss = 0.0298908
I0307 17:37:30.778472 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.0298908 (* 1 = 0.0298908 loss)
I0307 17:37:30.778481 28053 sgd_solver.cpp:112] Iteration 22000, lr = 0.01
I0307 17:37:38.931149 28053 solver.cpp:239] Iteration 22100 (12.2663 iter/s, 8.15243s/100 iters), loss = 0.0116193
I0307 17:37:38.931188 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.0116193 (* 1 = 0.0116193 loss)
I0307 17:37:38.931195 28053 sgd_solver.cpp:112] Iteration 22100, lr = 0.01
I0307 17:37:47.085148 28053 solver.cpp:239] Iteration 22200 (12.2644 iter/s, 8.15371s/100 iters), loss = 0.0172726
I0307 17:37:47.085186 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.0172725 (* 1 = 0.0172725 loss)
I0307 17:37:47.085193 28053 sgd_solver.cpp:112] Iteration 22200, lr = 0.01
I0307 17:37:52.065407 28062 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:37:55.241370 28053 solver.cpp:239] Iteration 22300 (12.261 iter/s, 8.15594s/100 iters), loss = 0.0181818
I0307 17:37:55.241410 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.0181818 (* 1 = 0.0181818 loss)
I0307 17:37:55.241416 28053 sgd_solver.cpp:112] Iteration 22300, lr = 0.01
I0307 17:38:03.396903 28053 solver.cpp:239] Iteration 22400 (12.262 iter/s, 8.15525s/100 iters), loss = 0.0167989
I0307 17:38:03.396941 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.0167989 (* 1 = 0.0167989 loss)
I0307 17:38:03.396950 28053 sgd_solver.cpp:112] Iteration 22400, lr = 0.01
I0307 17:38:11.470376 28053 solver.cpp:347] Iteration 22500, Testing net (#0)
I0307 17:38:12.588730 28053 solver.cpp:414]     Test net output #0: Accuracy1 = 0.9926
I0307 17:38:12.588764 28053 solver.cpp:414]     Test net output #1: SoftmaxWithLoss1 = 0.0348123 (* 1 = 0.0348123 loss)
I0307 17:38:12.669026 28053 solver.cpp:239] Iteration 22500 (10.7854 iter/s, 9.27181s/100 iters), loss = 0.0229201
I0307 17:38:12.669064 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.0229201 (* 1 = 0.0229201 loss)
I0307 17:38:12.669071 28053 sgd_solver.cpp:112] Iteration 22500, lr = 0.01
I0307 17:38:20.826061 28053 solver.cpp:239] Iteration 22600 (12.2598 iter/s, 8.15675s/100 iters), loss = 0.0170328
I0307 17:38:20.826097 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.0170328 (* 1 = 0.0170328 loss)
I0307 17:38:20.826104 28053 sgd_solver.cpp:112] Iteration 22600, lr = 0.01
I0307 17:38:25.069854 28062 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:38:28.984616 28053 solver.cpp:239] Iteration 22700 (12.2575 iter/s, 8.15827s/100 iters), loss = 0.00848746
I0307 17:38:28.984655 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.00848742 (* 1 = 0.00848742 loss)
I0307 17:38:28.984663 28053 sgd_solver.cpp:112] Iteration 22700, lr = 0.01
I0307 17:38:37.136435 28053 solver.cpp:239] Iteration 22800 (12.2676 iter/s, 8.15153s/100 iters), loss = 0.0337309
I0307 17:38:37.136471 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.0337309 (* 1 = 0.0337309 loss)
I0307 17:38:37.136478 28053 sgd_solver.cpp:112] Iteration 22800, lr = 0.01
I0307 17:38:45.292824 28053 solver.cpp:239] Iteration 22900 (12.2608 iter/s, 8.15611s/100 iters), loss = 0.0058788
I0307 17:38:45.292865 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.00587877 (* 1 = 0.00587877 loss)
I0307 17:38:45.292871 28053 sgd_solver.cpp:112] Iteration 22900, lr = 0.01
I0307 17:38:53.367173 28053 solver.cpp:347] Iteration 23000, Testing net (#0)
I0307 17:38:54.486500 28053 solver.cpp:414]     Test net output #0: Accuracy1 = 0.9897
I0307 17:38:54.486531 28053 solver.cpp:414]     Test net output #1: SoftmaxWithLoss1 = 0.0407407 (* 1 = 0.0407407 loss)
I0307 17:38:54.567236 28053 solver.cpp:239] Iteration 23000 (10.7827 iter/s, 9.27409s/100 iters), loss = 0.0136064
I0307 17:38:54.567275 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.0136064 (* 1 = 0.0136064 loss)
I0307 17:38:54.567283 28053 sgd_solver.cpp:112] Iteration 23000, lr = 0.01
I0307 17:38:57.997220 28062 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:39:02.726724 28053 solver.cpp:239] Iteration 23100 (12.2561 iter/s, 8.1592s/100 iters), loss = 0.0177692
I0307 17:39:02.726763 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.0177691 (* 1 = 0.0177691 loss)
I0307 17:39:02.726769 28053 sgd_solver.cpp:112] Iteration 23100, lr = 0.01
I0307 17:39:10.880362 28053 solver.cpp:239] Iteration 23200 (12.2649 iter/s, 8.15335s/100 iters), loss = 0.00902001
I0307 17:39:10.880399 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.00901997 (* 1 = 0.00901997 loss)
I0307 17:39:10.880408 28053 sgd_solver.cpp:112] Iteration 23200, lr = 0.01
I0307 17:39:19.023862 28053 solver.cpp:239] Iteration 23300 (12.2802 iter/s, 8.14322s/100 iters), loss = 0.0095067
I0307 17:39:19.023905 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.00950666 (* 1 = 0.00950666 loss)
I0307 17:39:19.023916 28053 sgd_solver.cpp:112] Iteration 23300, lr = 0.01
I0307 17:39:27.172652 28053 solver.cpp:239] Iteration 23400 (12.2722 iter/s, 8.1485s/100 iters), loss = 0.0183982
I0307 17:39:27.172695 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.0183982 (* 1 = 0.0183982 loss)
I0307 17:39:27.172706 28053 sgd_solver.cpp:112] Iteration 23400, lr = 0.01
I0307 17:39:29.865125 28062 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:39:35.241971 28053 solver.cpp:347] Iteration 23500, Testing net (#0)
I0307 17:39:36.357867 28053 solver.cpp:414]     Test net output #0: Accuracy1 = 0.9928
I0307 17:39:36.357900 28053 solver.cpp:414]     Test net output #1: SoftmaxWithLoss1 = 0.0306288 (* 1 = 0.0306288 loss)
I0307 17:39:36.438494 28053 solver.cpp:239] Iteration 23500 (10.7927 iter/s, 9.26552s/100 iters), loss = 0.00850025
I0307 17:39:36.438534 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.0085002 (* 1 = 0.0085002 loss)
I0307 17:39:36.438545 28053 sgd_solver.cpp:112] Iteration 23500, lr = 0.01
I0307 17:39:44.588744 28053 solver.cpp:239] Iteration 23600 (12.27 iter/s, 8.14996s/100 iters), loss = 0.00522374
I0307 17:39:44.588791 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.00522369 (* 1 = 0.00522369 loss)
I0307 17:39:44.588804 28053 sgd_solver.cpp:112] Iteration 23600, lr = 0.01
I0307 17:39:52.741602 28053 solver.cpp:239] Iteration 23700 (12.2661 iter/s, 8.15257s/100 iters), loss = 0.0145287
I0307 17:39:52.741645 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.0145287 (* 1 = 0.0145287 loss)
I0307 17:39:52.741657 28053 sgd_solver.cpp:112] Iteration 23700, lr = 0.01
I0307 17:40:00.897907 28053 solver.cpp:239] Iteration 23800 (12.2609 iter/s, 8.15602s/100 iters), loss = 0.0157248
I0307 17:40:00.897997 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.0157247 (* 1 = 0.0157247 loss)
I0307 17:40:00.898010 28053 sgd_solver.cpp:112] Iteration 23800, lr = 0.01
I0307 17:40:02.858395 28062 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:40:09.050987 28053 solver.cpp:239] Iteration 23900 (12.2658 iter/s, 8.15275s/100 iters), loss = 0.0314987
I0307 17:40:09.051026 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.0314987 (* 1 = 0.0314987 loss)
I0307 17:40:09.051033 28053 sgd_solver.cpp:112] Iteration 23900, lr = 0.01
I0307 17:40:17.125916 28053 solver.cpp:347] Iteration 24000, Testing net (#0)
I0307 17:40:18.244169 28053 solver.cpp:414]     Test net output #0: Accuracy1 = 0.9946
I0307 17:40:18.244210 28053 solver.cpp:414]     Test net output #1: SoftmaxWithLoss1 = 0.0266027 (* 1 = 0.0266027 loss)
I0307 17:40:18.325397 28053 solver.cpp:239] Iteration 24000 (10.7827 iter/s, 9.27409s/100 iters), loss = 0.0180016
I0307 17:40:18.325434 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.0180016 (* 1 = 0.0180016 loss)
I0307 17:40:18.325443 28053 sgd_solver.cpp:112] Iteration 24000, lr = 0.01
I0307 17:40:26.475965 28053 solver.cpp:239] Iteration 24100 (12.2695 iter/s, 8.15028s/100 iters), loss = 0.00984657
I0307 17:40:26.476004 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.00984652 (* 1 = 0.00984652 loss)
I0307 17:40:26.476011 28053 sgd_solver.cpp:112] Iteration 24100, lr = 0.01
I0307 17:40:34.629389 28053 solver.cpp:239] Iteration 24200 (12.2652 iter/s, 8.15314s/100 iters), loss = 0.0127075
I0307 17:40:34.629472 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.0127075 (* 1 = 0.0127075 loss)
I0307 17:40:34.629480 28053 sgd_solver.cpp:112] Iteration 24200, lr = 0.01
I0307 17:40:35.774686 28062 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:40:42.785581 28053 solver.cpp:239] Iteration 24300 (12.2611 iter/s, 8.15587s/100 iters), loss = 0.0118236
I0307 17:40:42.785620 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.0118235 (* 1 = 0.0118235 loss)
I0307 17:40:42.785627 28053 sgd_solver.cpp:112] Iteration 24300, lr = 0.01
I0307 17:40:50.934448 28053 solver.cpp:239] Iteration 24400 (12.2721 iter/s, 8.14858s/100 iters), loss = 0.0294426
I0307 17:40:50.934487 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.0294425 (* 1 = 0.0294425 loss)
I0307 17:40:50.934495 28053 sgd_solver.cpp:112] Iteration 24400, lr = 0.01
I0307 17:40:59.007648 28053 solver.cpp:347] Iteration 24500, Testing net (#0)
I0307 17:41:00.086819 28064 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:41:00.131265 28053 solver.cpp:414]     Test net output #0: Accuracy1 = 0.9973
I0307 17:41:00.131296 28053 solver.cpp:414]     Test net output #1: SoftmaxWithLoss1 = 0.0195128 (* 1 = 0.0195128 loss)
I0307 17:41:00.211716 28053 solver.cpp:239] Iteration 24500 (10.7794 iter/s, 9.27695s/100 iters), loss = 0.00802913
I0307 17:41:00.211755 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.00802909 (* 1 = 0.00802909 loss)
I0307 17:41:00.211763 28053 sgd_solver.cpp:112] Iteration 24500, lr = 0.01
I0307 17:41:08.364980 28053 solver.cpp:239] Iteration 24600 (12.2655 iter/s, 8.15298s/100 iters), loss = 0.00276877
I0307 17:41:08.365097 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.00276873 (* 1 = 0.00276873 loss)
I0307 17:41:08.365108 28053 sgd_solver.cpp:112] Iteration 24600, lr = 0.01
I0307 17:41:08.775543 28062 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:41:16.519281 28053 solver.cpp:239] Iteration 24700 (12.264 iter/s, 8.15394s/100 iters), loss = 0.0193814
I0307 17:41:16.519320 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.0193813 (* 1 = 0.0193813 loss)
I0307 17:41:16.519328 28053 sgd_solver.cpp:112] Iteration 24700, lr = 0.01
I0307 17:41:24.671725 28053 solver.cpp:239] Iteration 24800 (12.2667 iter/s, 8.15216s/100 iters), loss = 0.0148588
I0307 17:41:24.671766 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.0148588 (* 1 = 0.0148588 loss)
I0307 17:41:24.671772 28053 sgd_solver.cpp:112] Iteration 24800, lr = 0.01
I0307 17:41:32.820598 28053 solver.cpp:239] Iteration 24900 (12.2721 iter/s, 8.14859s/100 iters), loss = 0.0155012
I0307 17:41:32.820637 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.0155011 (* 1 = 0.0155011 loss)
I0307 17:41:32.820644 28053 sgd_solver.cpp:112] Iteration 24900, lr = 0.01
I0307 17:41:40.572142 28062 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:41:40.895579 28053 solver.cpp:347] Iteration 25000, Testing net (#0)
I0307 17:41:42.011189 28053 solver.cpp:414]     Test net output #0: Accuracy1 = 0.9971
I0307 17:41:42.011224 28053 solver.cpp:414]     Test net output #1: SoftmaxWithLoss1 = 0.0171639 (* 1 = 0.0171639 loss)
I0307 17:41:42.091346 28053 solver.cpp:239] Iteration 25000 (10.787 iter/s, 9.27043s/100 iters), loss = 0.00491223
I0307 17:41:42.091388 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.0049122 (* 1 = 0.0049122 loss)
I0307 17:41:42.091395 28053 sgd_solver.cpp:50] MultiStep Status: Iteration 25000, step = 2
I0307 17:41:42.091400 28053 sgd_solver.cpp:112] Iteration 25000, lr = 0.001
I0307 17:41:50.246639 28053 solver.cpp:239] Iteration 25100 (12.2624 iter/s, 8.155s/100 iters), loss = 0.00507517
I0307 17:41:50.246676 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.00507513 (* 1 = 0.00507513 loss)
I0307 17:41:50.246683 28053 sgd_solver.cpp:112] Iteration 25100, lr = 0.001
I0307 17:41:58.399411 28053 solver.cpp:239] Iteration 25200 (12.2662 iter/s, 8.15249s/100 iters), loss = 0.0124119
I0307 17:41:58.399451 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.0124119 (* 1 = 0.0124119 loss)
I0307 17:41:58.399457 28053 sgd_solver.cpp:112] Iteration 25200, lr = 0.001
I0307 17:42:06.554543 28053 solver.cpp:239] Iteration 25300 (12.2626 iter/s, 8.15484s/100 iters), loss = 0.005895
I0307 17:42:06.554581 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.00589496 (* 1 = 0.00589496 loss)
I0307 17:42:06.554589 28053 sgd_solver.cpp:112] Iteration 25300, lr = 0.001
I0307 17:42:13.553126 28062 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:42:14.691584 28053 solver.cpp:239] Iteration 25400 (12.2899 iter/s, 8.13675s/100 iters), loss = 0.00561891
I0307 17:42:14.691632 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.00561887 (* 1 = 0.00561887 loss)
I0307 17:42:14.691640 28053 sgd_solver.cpp:112] Iteration 25400, lr = 0.001
I0307 17:42:22.769546 28053 solver.cpp:347] Iteration 25500, Testing net (#0)
I0307 17:42:23.889716 28053 solver.cpp:414]     Test net output #0: Accuracy1 = 0.9984
I0307 17:42:23.889746 28053 solver.cpp:414]     Test net output #1: SoftmaxWithLoss1 = 0.0147132 (* 1 = 0.0147132 loss)
I0307 17:42:23.970643 28053 solver.cpp:239] Iteration 25500 (10.7773 iter/s, 9.27873s/100 iters), loss = 0.00388709
I0307 17:42:23.970677 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.00388705 (* 1 = 0.00388705 loss)
I0307 17:42:23.970685 28053 sgd_solver.cpp:112] Iteration 25500, lr = 0.001
I0307 17:42:32.130188 28053 solver.cpp:239] Iteration 25600 (12.256 iter/s, 8.15926s/100 iters), loss = 0.00633608
I0307 17:42:32.130226 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.00633604 (* 1 = 0.00633604 loss)
I0307 17:42:32.130232 28053 sgd_solver.cpp:112] Iteration 25600, lr = 0.001
I0307 17:42:40.285295 28053 solver.cpp:239] Iteration 25700 (12.2627 iter/s, 8.15482s/100 iters), loss = 0.00820935
I0307 17:42:40.285336 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.00820931 (* 1 = 0.00820931 loss)
I0307 17:42:40.285342 28053 sgd_solver.cpp:112] Iteration 25700, lr = 0.001
I0307 17:42:46.657167 28062 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:42:48.538928 28053 solver.cpp:239] Iteration 25800 (12.1163 iter/s, 8.25334s/100 iters), loss = 0.00699731
I0307 17:42:48.538965 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.00699727 (* 1 = 0.00699727 loss)
I0307 17:42:48.538974 28053 sgd_solver.cpp:112] Iteration 25800, lr = 0.001
I0307 17:42:56.685211 28053 solver.cpp:239] Iteration 25900 (12.276 iter/s, 8.146s/100 iters), loss = 0.012381
I0307 17:42:56.685250 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.012381 (* 1 = 0.012381 loss)
I0307 17:42:56.685257 28053 sgd_solver.cpp:112] Iteration 25900, lr = 0.001
I0307 17:43:04.747647 28053 solver.cpp:347] Iteration 26000, Testing net (#0)
I0307 17:43:05.864248 28053 solver.cpp:414]     Test net output #0: Accuracy1 = 0.9993
I0307 17:43:05.864284 28053 solver.cpp:414]     Test net output #1: SoftmaxWithLoss1 = 0.0104602 (* 1 = 0.0104602 loss)
I0307 17:43:05.945361 28053 solver.cpp:239] Iteration 26000 (10.7993 iter/s, 9.25983s/100 iters), loss = 0.00759512
I0307 17:43:05.945401 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.00759508 (* 1 = 0.00759508 loss)
I0307 17:43:05.945410 28053 sgd_solver.cpp:112] Iteration 26000, lr = 0.001
I0307 17:43:14.090467 28053 solver.cpp:239] Iteration 26100 (12.2777 iter/s, 8.14482s/100 iters), loss = 0.00793772
I0307 17:43:14.090507 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.00793768 (* 1 = 0.00793768 loss)
I0307 17:43:14.090513 28053 sgd_solver.cpp:112] Iteration 26100, lr = 0.001
I0307 17:43:19.551580 28062 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:43:22.237176 28053 solver.cpp:239] Iteration 26200 (12.2753 iter/s, 8.14642s/100 iters), loss = 0.0044616
I0307 17:43:22.237217 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.00446156 (* 1 = 0.00446156 loss)
I0307 17:43:22.237224 28053 sgd_solver.cpp:112] Iteration 26200, lr = 0.001
I0307 17:43:30.380648 28053 solver.cpp:239] Iteration 26300 (12.2802 iter/s, 8.14318s/100 iters), loss = 0.0134312
I0307 17:43:30.380686 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.0134311 (* 1 = 0.0134311 loss)
I0307 17:43:30.380692 28053 sgd_solver.cpp:112] Iteration 26300, lr = 0.001
I0307 17:43:38.524950 28053 solver.cpp:239] Iteration 26400 (12.279 iter/s, 8.14402s/100 iters), loss = 0.0083311
I0307 17:43:38.524986 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.00833106 (* 1 = 0.00833106 loss)
I0307 17:43:38.524994 28053 sgd_solver.cpp:112] Iteration 26400, lr = 0.001
I0307 17:43:46.587751 28053 solver.cpp:347] Iteration 26500, Testing net (#0)
I0307 17:43:47.706959 28053 solver.cpp:414]     Test net output #0: Accuracy1 = 0.999
I0307 17:43:47.706995 28053 solver.cpp:414]     Test net output #1: SoftmaxWithLoss1 = 0.00895855 (* 1 = 0.00895855 loss)
I0307 17:43:47.787951 28053 solver.cpp:239] Iteration 26500 (10.796 iter/s, 9.26268s/100 iters), loss = 0.00934959
I0307 17:43:47.787993 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.00934954 (* 1 = 0.00934954 loss)
I0307 17:43:47.788002 28053 sgd_solver.cpp:112] Iteration 26500, lr = 0.001
I0307 17:43:52.514912 28062 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:43:55.935557 28053 solver.cpp:239] Iteration 26600 (12.274 iter/s, 8.14732s/100 iters), loss = 0.00617459
I0307 17:43:55.935595 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.00617454 (* 1 = 0.00617454 loss)
I0307 17:43:55.935602 28053 sgd_solver.cpp:112] Iteration 26600, lr = 0.001
I0307 17:44:04.079603 28053 solver.cpp:239] Iteration 26700 (12.2793 iter/s, 8.14376s/100 iters), loss = 0.00719665
I0307 17:44:04.079643 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.0071966 (* 1 = 0.0071966 loss)
I0307 17:44:04.079651 28053 sgd_solver.cpp:112] Iteration 26700, lr = 0.001
I0307 17:44:12.224714 28053 solver.cpp:239] Iteration 26800 (12.2777 iter/s, 8.14482s/100 iters), loss = 0.00813051
I0307 17:44:12.224752 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.00813046 (* 1 = 0.00813046 loss)
I0307 17:44:12.224761 28053 sgd_solver.cpp:112] Iteration 26800, lr = 0.001
I0307 17:44:20.369596 28053 solver.cpp:239] Iteration 26900 (12.2781 iter/s, 8.1446s/100 iters), loss = 0.0104618
I0307 17:44:20.369634 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.0104618 (* 1 = 0.0104618 loss)
I0307 17:44:20.369642 28053 sgd_solver.cpp:112] Iteration 26900, lr = 0.001
I0307 17:44:24.366777 28062 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:44:28.439448 28053 solver.cpp:347] Iteration 27000, Testing net (#0)
I0307 17:44:29.516309 28064 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:44:29.559916 28053 solver.cpp:414]     Test net output #0: Accuracy1 = 1
I0307 17:44:29.559942 28053 solver.cpp:414]     Test net output #1: SoftmaxWithLoss1 = 0.00663302 (* 1 = 0.00663302 loss)
I0307 17:44:29.641083 28053 solver.cpp:239] Iteration 27000 (10.7861 iter/s, 9.27117s/100 iters), loss = 0.0071216
I0307 17:44:29.641119 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.00712155 (* 1 = 0.00712155 loss)
I0307 17:44:29.641127 28053 sgd_solver.cpp:112] Iteration 27000, lr = 0.001
I0307 17:44:37.789783 28053 solver.cpp:239] Iteration 27100 (12.2723 iter/s, 8.14842s/100 iters), loss = 0.0104474
I0307 17:44:37.789821 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.0104474 (* 1 = 0.0104474 loss)
I0307 17:44:37.789829 28053 sgd_solver.cpp:112] Iteration 27100, lr = 0.001
I0307 17:44:45.934769 28053 solver.cpp:239] Iteration 27200 (12.2779 iter/s, 8.1447s/100 iters), loss = 0.0138817
I0307 17:44:45.934808 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.0138816 (* 1 = 0.0138816 loss)
I0307 17:44:45.934815 28053 sgd_solver.cpp:112] Iteration 27200, lr = 0.001
I0307 17:44:54.080394 28053 solver.cpp:239] Iteration 27300 (12.277 iter/s, 8.14534s/100 iters), loss = 0.00439354
I0307 17:44:54.080433 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.0043935 (* 1 = 0.0043935 loss)
I0307 17:44:54.080440 28053 sgd_solver.cpp:112] Iteration 27300, lr = 0.001
I0307 17:44:57.263072 28062 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:45:02.228893 28053 solver.cpp:239] Iteration 27400 (12.2726 iter/s, 8.14821s/100 iters), loss = 0.00374928
I0307 17:45:02.228930 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.00374924 (* 1 = 0.00374924 loss)
I0307 17:45:02.228937 28053 sgd_solver.cpp:112] Iteration 27400, lr = 0.001
I0307 17:45:10.296716 28053 solver.cpp:347] Iteration 27500, Testing net (#0)
I0307 17:45:11.416514 28053 solver.cpp:414]     Test net output #0: Accuracy1 = 1
I0307 17:45:11.416548 28053 solver.cpp:414]     Test net output #1: SoftmaxWithLoss1 = 0.00642438 (* 1 = 0.00642438 loss)
I0307 17:45:11.497539 28053 solver.cpp:239] Iteration 27500 (10.7894 iter/s, 9.26833s/100 iters), loss = 0.0103929
I0307 17:45:11.497575 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.0103928 (* 1 = 0.0103928 loss)
I0307 17:45:11.497583 28053 sgd_solver.cpp:112] Iteration 27500, lr = 0.001
I0307 17:45:19.641237 28053 solver.cpp:239] Iteration 27600 (12.2799 iter/s, 8.14341s/100 iters), loss = 0.0140633
I0307 17:45:19.641274 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.0140633 (* 1 = 0.0140633 loss)
I0307 17:45:19.641283 28053 sgd_solver.cpp:112] Iteration 27600, lr = 0.001
I0307 17:45:27.787108 28053 solver.cpp:239] Iteration 27700 (12.2766 iter/s, 8.14559s/100 iters), loss = 0.00767905
I0307 17:45:27.787228 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.00767901 (* 1 = 0.00767901 loss)
I0307 17:45:27.787238 28053 sgd_solver.cpp:112] Iteration 27700, lr = 0.001
I0307 17:45:30.236403 28062 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:45:35.936295 28053 solver.cpp:239] Iteration 27800 (12.2717 iter/s, 8.14882s/100 iters), loss = 0.00566066
I0307 17:45:35.936333 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.00566062 (* 1 = 0.00566062 loss)
I0307 17:45:35.936341 28053 sgd_solver.cpp:112] Iteration 27800, lr = 0.001
I0307 17:45:44.079643 28053 solver.cpp:239] Iteration 27900 (12.2804 iter/s, 8.14306s/100 iters), loss = 0.00671511
I0307 17:45:44.079680 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.00671507 (* 1 = 0.00671507 loss)
I0307 17:45:44.079689 28053 sgd_solver.cpp:112] Iteration 27900, lr = 0.001
I0307 17:45:52.145752 28053 solver.cpp:347] Iteration 28000, Testing net (#0)
I0307 17:45:53.262584 28053 solver.cpp:414]     Test net output #0: Accuracy1 = 0.9993
I0307 17:45:53.262617 28053 solver.cpp:414]     Test net output #1: SoftmaxWithLoss1 = 0.00938534 (* 1 = 0.00938534 loss)
I0307 17:45:53.343708 28053 solver.cpp:239] Iteration 28000 (10.7948 iter/s, 9.26375s/100 iters), loss = 0.00487208
I0307 17:45:53.343746 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.00487204 (* 1 = 0.00487204 loss)
I0307 17:45:53.343755 28053 sgd_solver.cpp:112] Iteration 28000, lr = 0.001
I0307 17:46:01.492943 28053 solver.cpp:239] Iteration 28100 (12.2715 iter/s, 8.14895s/100 iters), loss = 0.00514142
I0307 17:46:01.493027 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.00514138 (* 1 = 0.00514138 loss)
I0307 17:46:01.493036 28053 sgd_solver.cpp:112] Iteration 28100, lr = 0.001
I0307 17:46:03.127892 28062 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:46:09.642684 28053 solver.cpp:239] Iteration 28200 (12.2708 iter/s, 8.14941s/100 iters), loss = 0.00942879
I0307 17:46:09.642724 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.00942875 (* 1 = 0.00942875 loss)
I0307 17:46:09.642730 28053 sgd_solver.cpp:112] Iteration 28200, lr = 0.001
I0307 17:46:17.790477 28053 solver.cpp:239] Iteration 28300 (12.2737 iter/s, 8.14751s/100 iters), loss = 0.022002
I0307 17:46:17.790516 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.022002 (* 1 = 0.022002 loss)
I0307 17:46:17.790524 28053 sgd_solver.cpp:112] Iteration 28300, lr = 0.001
I0307 17:46:25.938814 28053 solver.cpp:239] Iteration 28400 (12.2729 iter/s, 8.14805s/100 iters), loss = 0.00987394
I0307 17:46:25.938853 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.0098739 (* 1 = 0.0098739 loss)
I0307 17:46:25.938860 28053 sgd_solver.cpp:112] Iteration 28400, lr = 0.001
I0307 17:46:34.006947 28053 solver.cpp:347] Iteration 28500, Testing net (#0)
I0307 17:46:35.125108 28053 solver.cpp:414]     Test net output #0: Accuracy1 = 0.9998
I0307 17:46:35.125142 28053 solver.cpp:414]     Test net output #1: SoftmaxWithLoss1 = 0.00788707 (* 1 = 0.00788707 loss)
I0307 17:46:35.206367 28053 solver.cpp:239] Iteration 28500 (10.7907 iter/s, 9.26723s/100 iters), loss = 0.00568525
I0307 17:46:35.206403 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.00568521 (* 1 = 0.00568521 loss)
I0307 17:46:35.206411 28053 sgd_solver.cpp:112] Iteration 28500, lr = 0.001
I0307 17:46:36.109341 28062 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:46:43.355706 28053 solver.cpp:239] Iteration 28600 (12.2714 iter/s, 8.14906s/100 iters), loss = 0.00645617
I0307 17:46:43.355746 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.00645613 (* 1 = 0.00645613 loss)
I0307 17:46:43.355753 28053 sgd_solver.cpp:112] Iteration 28600, lr = 0.001
I0307 17:46:51.501394 28053 solver.cpp:239] Iteration 28700 (12.2769 iter/s, 8.1454s/100 iters), loss = 0.00415104
I0307 17:46:51.501432 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.004151 (* 1 = 0.004151 loss)
I0307 17:46:51.501440 28053 sgd_solver.cpp:112] Iteration 28700, lr = 0.001
I0307 17:46:59.647495 28053 solver.cpp:239] Iteration 28800 (12.2762 iter/s, 8.14582s/100 iters), loss = 0.0162124
I0307 17:46:59.647533 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.0162124 (* 1 = 0.0162124 loss)
I0307 17:46:59.647541 28053 sgd_solver.cpp:112] Iteration 28800, lr = 0.001
I0307 17:47:07.792186 28053 solver.cpp:239] Iteration 28900 (12.2784 iter/s, 8.14441s/100 iters), loss = 0.0025885
I0307 17:47:07.792304 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.00258846 (* 1 = 0.00258846 loss)
I0307 17:47:07.792313 28053 sgd_solver.cpp:112] Iteration 28900, lr = 0.001
I0307 17:47:07.958511 28062 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:47:15.858340 28053 solver.cpp:347] Iteration 29000, Testing net (#0)
I0307 17:47:16.979497 28053 solver.cpp:414]     Test net output #0: Accuracy1 = 0.9994
I0307 17:47:16.979530 28053 solver.cpp:414]     Test net output #1: SoftmaxWithLoss1 = 0.00746279 (* 1 = 0.00746279 loss)
I0307 17:47:17.060395 28053 solver.cpp:239] Iteration 29000 (10.79 iter/s, 9.26781s/100 iters), loss = 0.00845176
I0307 17:47:17.060431 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.00845172 (* 1 = 0.00845172 loss)
I0307 17:47:17.060439 28053 sgd_solver.cpp:112] Iteration 29000, lr = 0.001
I0307 17:47:25.206794 28053 solver.cpp:239] Iteration 29100 (12.2758 iter/s, 8.14611s/100 iters), loss = 0.00788424
I0307 17:47:25.206831 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.0078842 (* 1 = 0.0078842 loss)
I0307 17:47:25.206838 28053 sgd_solver.cpp:112] Iteration 29100, lr = 0.001
I0307 17:47:33.352423 28053 solver.cpp:239] Iteration 29200 (12.277 iter/s, 8.14534s/100 iters), loss = 0.0093124
I0307 17:47:33.352463 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.00931236 (* 1 = 0.00931236 loss)
I0307 17:47:33.352469 28053 sgd_solver.cpp:112] Iteration 29200, lr = 0.001
I0307 17:47:40.850914 28062 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:47:41.498723 28053 solver.cpp:239] Iteration 29300 (12.2759 iter/s, 8.14601s/100 iters), loss = 0.00515456
I0307 17:47:41.498760 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.00515452 (* 1 = 0.00515452 loss)
I0307 17:47:41.498769 28053 sgd_solver.cpp:112] Iteration 29300, lr = 0.001
I0307 17:47:49.645469 28053 solver.cpp:239] Iteration 29400 (12.2753 iter/s, 8.14646s/100 iters), loss = 0.0137547
I0307 17:47:49.645507 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.0137547 (* 1 = 0.0137547 loss)
I0307 17:47:49.645514 28053 sgd_solver.cpp:112] Iteration 29400, lr = 0.001
I0307 17:47:57.709373 28053 solver.cpp:347] Iteration 29500, Testing net (#0)
I0307 17:47:58.785656 28064 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:47:58.829218 28053 solver.cpp:414]     Test net output #0: Accuracy1 = 0.9999
I0307 17:47:58.829252 28053 solver.cpp:414]     Test net output #1: SoftmaxWithLoss1 = 0.00597498 (* 1 = 0.00597498 loss)
I0307 17:47:58.910516 28053 solver.cpp:239] Iteration 29500 (10.7936 iter/s, 9.26473s/100 iters), loss = 0.003987
I0307 17:47:58.910553 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.00398697 (* 1 = 0.00398697 loss)
I0307 17:47:58.910562 28053 sgd_solver.cpp:112] Iteration 29500, lr = 0.001
I0307 17:48:07.059798 28053 solver.cpp:239] Iteration 29600 (12.2715 iter/s, 8.149s/100 iters), loss = 0.00789706
I0307 17:48:07.059835 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.00789703 (* 1 = 0.00789703 loss)
I0307 17:48:07.059842 28053 sgd_solver.cpp:112] Iteration 29600, lr = 0.001
I0307 17:48:13.814476 28062 data_layer.cpp:73] Restarting data prefetching from start.
I0307 17:48:15.197880 28053 solver.cpp:239] Iteration 29700 (12.2883 iter/s, 8.13779s/100 iters), loss = 0.00634108
I0307 17:48:15.197921 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.00634104 (* 1 = 0.00634104 loss)
I0307 17:48:15.197929 28053 sgd_solver.cpp:112] Iteration 29700, lr = 0.001
I0307 17:48:23.346300 28053 solver.cpp:239] Iteration 29800 (12.2728 iter/s, 8.14813s/100 iters), loss = 0.0120193
I0307 17:48:23.346338 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.0120193 (* 1 = 0.0120193 loss)
I0307 17:48:23.346345 28053 sgd_solver.cpp:112] Iteration 29800, lr = 0.001
I0307 17:48:31.491257 28053 solver.cpp:239] Iteration 29900 (12.278 iter/s, 8.14467s/100 iters), loss = 0.00934458
I0307 17:48:31.491295 28053 solver.cpp:258]     Train net output #0: SoftmaxWithLoss1 = 0.00934454 (* 1 = 0.00934454 loss)
I0307 17:48:31.491302 28053 sgd_solver.cpp:112] Iteration 29900, lr = 0.001
I0307 17:48:39.744381 28053 solver.cpp:464] Snapshotting to binary proto file examples/cifar10/cifar10_res20_iter_30000.caffemodel
I0307 17:48:39.751703 28053 sgd_solver.cpp:284] Snapshotting solver state to binary proto file examples/cifar10/cifar10_res20_iter_30000.solverstate
I0307 17:48:39.770910 28053 solver.cpp:327] Iteration 30000, loss = 0.00584112
I0307 17:48:39.770934 28053 solver.cpp:347] Iteration 30000, Testing net (#0)
I0307 17:48:40.955268 28053 solver.cpp:414]     Test net output #0: Accuracy1 = 1
I0307 17:48:40.955301 28053 solver.cpp:414]     Test net output #1: SoftmaxWithLoss1 = 0.00586703 (* 1 = 0.00586703 loss)
I0307 17:48:40.955305 28053 solver.cpp:332] Optimization Done.
I0307 17:48:40.955309 28053 caffe.cpp:250] Optimization Done.
